# Synthetic Instruction Tuner
## ν”„λ΅μ νΈ κ³„νμ„ (μƒμ„Έ μΌμ •)

---

## 1. ν”„λ΅μ νΈ κ°μ”

### 1.1 ν”„λ΅μ νΈ μ •λ³΄
- **ν”„λ΅μ νΈλ…**: Synthetic Instruction Tuner
- **κΈ°κ°„**: 4μ£Ό
- **μμ‚°**: $0 (λ¬΄λ£) λλ” Colab Pro (μ›” $10)
- **ν™κ²½**: Google Colab Pro (A100 GPU, μµμ ν™” μ™„λ£) / T4 GPU νΈν™

### 1.2 λ©ν‘ μ”μ•½
```
ν•©μ„± λ°μ΄ν„° μƒμ„± (1.5K) β†’ ν’μ§ ν•„ν„°λ§ (1K) β†’ μ„ νΈ λ°μ΄ν„° μƒμ„± (600) β†’ SFT+DPO ν•™μµ β†’ ν‰κ°€
```

### 1.3 NLP Task
**Task**: Instruction-Following Text Generation for Multi-Domain Dialogue Systems

λ³Έ ν”„λ΅μ νΈλ” λ‹¤μ–‘ν• λ„λ©”μΈ(μ½”λ”©, μ¶”λ΅ , μ°½μ‘ λ“±)μ—μ„ κ³ ν’μ§ μ‘λ‹µμ„ μƒμ„±ν•  μ μλ” instruction-following LLMμ„ κ°λ°ν•©λ‹λ‹¤.

---

## 2. μ „μ²΄ μΌμ • κ°μ”

```
Week 1: ν™κ²½ μ„¤μ • + λ°μ΄ν„° μƒμ„± (Magpie)
Week 2: ν’μ§ ν•„ν„°λ§ + μ„ νΈ λ°μ΄ν„° μƒμ„±
Week 3: Fine-tuning (SFT + DPO)
Week 4: ν‰κ°€ + λ¬Έμ„ν™” + λ°ν‘ μ¤€λΉ„
```

---

## 3. Week 1: λ°μ΄ν„° μƒμ„±

### 3.1 Day 1-2: ν™κ²½ μ„¤μ •

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| Colab ν™κ²½ μ„¤μ • | GPU ν™•μΈ, λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ | 1μ‹κ°„ |
| Hugging Face λ΅κ·ΈμΈ | ν† ν° μ„¤μ •, λ¨λΈ μ ‘κ·Ό κ¶ν• | 30λ¶„ |
| ν”„λ΅μ νΈ κµ¬μ΅° μ„¤μ • | ν΄λ” κµ¬μ΅°, μ ν‹Έλ¦¬ν‹° ν•¨μ | 1μ‹κ°„ |
| Llama-3.1-8B ν…μ¤νΈ | λ¨λΈ λ΅λ”© λ° μ¶”λ΅  ν…μ¤νΈ | 2μ‹κ°„ |

#### μ²΄ν¬ν¬μΈνΈ
- [ ] Colabμ—μ„ T4 GPU ν• λ‹Ή ν™•μΈ
- [ ] transformers, peft, trl μ„¤μΉ μ™„λ£
- [ ] Llama-3.1-8B λ΅λ”© μ„±κ³µ (4-bit μ–‘μν™”)

### 3.2 Day 3-5: Magpie λ°μ΄ν„° μƒμ„±

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| Magpie Generator ν΄λμ¤ | ν…ν”λ¦Ώ κΈ°λ° μƒμ„± | 3μ‹κ°„ |
| Instruction μƒμ„± | 1,500κ° λ©ν‘ | 16-17μ‹κ°„ (3μΌ λ¶„ν• , 500κ°/μΌ) |
| Response μƒμ„± | κ° instructionμ— λ€ν• μ‘λ‹µ | ν¬ν•¨ |
| λ°μ΄ν„° μ €μ¥ | JSON/Parquet ν•μ‹ | 1μ‹κ°„ |

#### μƒμ„± μ „λµ

**T4 GPU (12μ‹κ°„ μ ν•)**
```
Day 3: 500κ° μƒμ„± (5.5h, 20κ°λ§λ‹¤ μ²΄ν¬ν¬μΈνΈ)
Day 4: 500κ° μƒμ„± (5.5h, 20κ°λ§λ‹¤ μ²΄ν¬ν¬μΈνΈ)
Day 5: 500κ° μƒμ„± (5.5h, 20κ°λ§λ‹¤ μ²΄ν¬ν¬μΈνΈ)
μ΄: 1,500κ° μ™„λ£, λ°μ΄ν„° κ²€μ¦
```

**A100 GPU (μµμ ν™”λ¨) β΅**
```
Day 3: 1,500κ° μ—°μ† μƒμ„± (6-8h, 100κ°λ§λ‹¤ μ²΄ν¬ν¬μΈνΈ)
μ΄: ν• μ„Έμ…μ—μ„ μ™„λ£ κ°€λ¥
```

#### μ²΄ν¬ν¬μΈνΈ
- [x] MagpieGenerator ν΄λμ¤ κµ¬ν„ μ™„λ£ (λ…ΈνΈλ¶ λ‚΄λ¶€μ— μ„λ² λ“λ¨)
- [x] Instruction 1,500κ° μƒμ„± μ™„λ£
- [x] Response 1,500κ° μƒμ„± μ™„λ£
- [x] data/raw/instructions_final_full.json μ €μ¥ μ™„λ£ (2025-12-24)

### 3.3 Week 1 μ‚°μ¶λ¬Ό
| μ‚°μ¶λ¬Ό | κ²½λ΅ | μ„¤λ… |
|--------|------|------|
| μ„¤μ • λ…ΈνΈλ¶ | notebooks/01_setup.ipynb | ν™κ²½ μ„¤μ • λ° config λ΅λ“ |
| μƒμ„± λ…ΈνΈλ¶ | notebooks/02_magpie_generation.ipynb | Magpie λ°μ΄ν„° μƒμ„± (MagpieGenerator ν΄λμ¤ ν¬ν•¨) |
| Raw λ°μ΄ν„° | data/raw/instructions_final_full.json | 1,500κ° μƒμ„± μ™„λ£ |

**μ°Έκ³ **: μ΄κΈ° κ³„νμ—μ„λ” `src/data_generation/magpie_generator.py`λ΅ λ¨λ“ν™” μμ •μ΄μ—μΌλ‚, Colab ν™κ²½κ³Ό κµμ΅μ  λ…ν™•μ„±μ„ μ„ν•΄ **λ…ΈνΈλ¶ κΈ°λ° κµ¬μ΅°**λ΅ μµμΆ… κ²°μ •. MagpieGenerator ν΄λμ¤λ” `02_magpie_generation.ipynb` Cell 11μ— κµ¬ν„λ¨.

---

## 4. Week 2: ν•„ν„°λ§ + μ„ νΈ λ°μ΄ν„°

### 4.1 Day 1-2: ν’μ§ ν•„ν„°λ§

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| ν•„ν„° ν΄λμ¤ κµ¬ν„ | 5κ°€μ§€ ν•„ν„° κ·μΉ™ | 3μ‹κ°„ |
| ν•„ν„°λ§ μ‹¤ν–‰ | 1,500 β†’ 1,000 | 30λ¶„ |
| ν†µκ³„ λ¶„μ„ | ν•„ν„°λ³„ μ κ±° μ | 1μ‹κ°„ |
| κ²°κ³Ό κ²€μ¦ | μƒν” μλ™ κ²€ν†  | 1μ‹κ°„ |

#### ν•„ν„° κ·μΉ™ μƒμ„Έ
```python
FILTER_CONFIG = {
    "length": {"min": 20, "max": 500},  # λ‹¨μ–΄ μ
    "repetition": {"max_repeat": 3},     # μ—°μ† λ°λ³µ ν—μ© μ
    "diversity": {"jaccard_threshold": 0.8},
    "refusal_keywords": [
        "I'm an AI", "I cannot", "I don't have",
        "As an AI", "I'm not able"
    ],
    "language": "en"
}
```

#### μ²΄ν¬ν¬μΈνΈ
- [ ] QualityFilter ν΄λμ¤ κµ¬ν„ μ™„λ£
- [ ] ν•„ν„°λ§ μ™„λ£ (1,000κ° μ΄μƒ ν†µκ³Ό)
- [ ] ν†µκ³„ λ¦¬ν¬νΈ μƒμ„±
- [ ] data/filtered/instructions_filtered.json μ €μ¥

### 4.2 Day 3-5: μ„ νΈ λ°μ΄ν„° μƒμ„±

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| μ‘λ‹µ μƒμ„± λ¨λΈ μ„¤μ • | 3κ° μ†ν• λ¨λΈ | 2μ‹κ°„ |
| λ‹¤μ¤‘ μ‘λ‹µ μƒμ„± | instructionλ‹Ή 3κ° μ‘λ‹µ | 6-8μ‹κ°„ |
| Reward Model μ„¤μ • | OpenAssistant RM | 1μ‹κ°„ |
| μ μν™” λ° μ„ νƒ | chosen/rejected μ | 2-3μ‹κ°„ |
| λ°μ΄ν„° κ²€μ¦ | μ μ λ¶„ν¬ ν™•μΈ | 1μ‹κ°„ |

#### λ©”λ¨λ¦¬ κ΄€λ¦¬ μ „λµ
```python
# μμ°¨μ  λ¨λΈ λ΅λ”© (λ©”λ¨λ¦¬ μ μ•½)
for model_name in ["llama-3.2-1b", "mistral-7b", "qwen2.5-3b"]:
    model = load_model(model_name)
    responses = generate(model, instructions)
    save_responses(responses, model_name)
    del model  # λ©”λ¨λ¦¬ ν•΄μ 
    torch.cuda.empty_cache()
```

#### μ²΄ν¬ν¬μΈνΈ
- [ ] 3κ° λ¨λΈ μ‘λ‹µ μƒμ„± μ™„λ£
- [ ] Reward Model μ μν™” μ™„λ£
- [ ] Preference μ 600κ° μƒμ„±
- [ ] data/preference/preference_pairs.json μ €μ¥

### 4.3 Week 2 μ‚°μ¶λ¬Ό
| μ‚°μ¶λ¬Ό | κ²½λ΅ | μ„¤λ… |
|--------|------|------|
| ν•„ν„°λ§ λ…ΈνΈλ¶ | notebooks/03_quality_filtering.ipynb | ν’μ§ ν•„ν„° (QualityFilter ν΄λμ¤ ν¬ν•¨) |
| μ„ νΈ μƒμ„± λ…ΈνΈλ¶ | notebooks/04_preference_generation.ipynb | μ„ νΈ λ°μ΄ν„° μƒμ„± |
| Filtered λ°μ΄ν„° | data/filtered/instructions_filtered.json | 1,000κ° |
| Preference λ°μ΄ν„° | data/preference/preference_pairs.json | 600κ° |

**μ°Έκ³ **: QualityFilterμ™€ Preference Builder ν΄λμ¤λ” κ°κ° 03, 04 λ…ΈνΈλ¶μ— μ„λ² λ“λμ–΄ μμΌλ©°, λ³„λ„ Python λ¨λ“ νμΌ(`src/`)μ€ μƒμ„±ν•μ§€ μ•μ.

---

## 5. Week 3: Fine-tuning (λ‹¤μ¤‘ μ μ‘ λ°©λ²•)

### 5.1 Day 1-3: SFT with LoRA & Prompt Tuning

#### μ‘μ—… λ©λ΅

**T4 GPU κΈ°μ¤€**
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| λ°μ΄ν„°μ…‹ μ¤€λΉ„ | SFT ν¬λ§· λ³€ν™ | 1μ‹κ°„ |
| **LoRA SFT** | Llama-3.2-3B (r=8, alpha=16) | 6-10μ‹κ°„ |
| **Prompt Tuning** | Llama-3.2-3B (20 virtual tokens) | 3-5μ‹κ°„ |
| ν¨μ¨μ„± λ©”νΈλ¦­ μμ§‘ | λ©”λ¨λ¦¬, μ‹κ°„, νλΌλ―Έν„° μΈ΅μ • | ν†µν•© |

**A100 GPU (μµμ ν™”λ¨) β΅**
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| λ°μ΄ν„°μ…‹ μ¤€λΉ„ | SFT ν¬λ§· λ³€ν™ | 1μ‹κ°„ |
| **LoRA SFT** | Batch 12, μµμ ν™” μ„¤μ • | 2-4μ‹κ°„ |
| **Prompt Tuning** | Batch 4, μµμ ν™” μ„¤μ • | 1-2μ‹κ°„ |
| ν¨μ¨μ„± λ©”νΈλ¦­ μμ§‘ | λ©”λ¨λ¦¬, μ‹κ°„, νλΌλ―Έν„° μΈ΅μ • | ν†µν•© |

#### LoRA μ„¤μ • (config.jsonμ— λ°μλ¨)
```python
LORA_CONFIG = {
    "r": 8,
    "lora_alpha": 16,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM"
}

# SFT Training (A100 μµμ ν™”)
SFT_TRAINING_ARGS_A100 = {
    "batch_size": 12,                    # T4: 4 β†’ A100: 12
    "gradient_accumulation_steps": 2,    # T4: 4 β†’ A100: 2
    "save_steps": 200,                   # T4: 500 β†’ A100: 200
}

PROMPT_TUNING_CONFIG = {
    "num_virtual_tokens": 20,
    "prompt_tuning_init": "RANDOM",
    "task_type": "CAUSAL_LM"
}
```

#### ν•™μµ μ¤μΌ€μ¤„
```
Day 1 λ°¤: LoRA SFT μ‹μ‘ (Llama-3.2-3B)
Day 2: LoRA μ™„λ£ + λ©”νΈλ¦­ μμ§‘
Day 2 λ°¤: Prompt Tuning μ‹μ‘
Day 3: Prompt Tuning μ™„λ£ + λΉ„κµ λ¶„μ„
```

#### μ²΄ν¬ν¬μΈνΈ
- [x] SFT λ°μ΄ν„°μ…‹ μ¤€λΉ„ μ™„λ£ (2025-12-26)
- [x] LoRA SFT μ™„λ£ (ν¨μ¨μ„± λ©”νΈλ¦­ ν¬ν•¨) - A100μ—μ„ 8.2λ¶„ μ™„λ£
- [x] Prompt Tuning μ™„λ£ (ν¨μ¨μ„± λ©”νΈλ¦­ ν¬ν•¨) - A100μ—μ„ 18.8λ¶„ μ™„λ£
- [x] models/sft/ λ° models/prompt_tuning/ μ €μ¥
- [x] ν¨μ¨μ„± λ©”νΈλ¦­ JSON μ €μ¥ (lora_metrics.json, prompt_tuning_metrics.json)

### 5.2 Day 4-5: DPO (Direct Preference Optimization)

#### μ‘μ—… λ©λ΅

**T4 GPU κΈ°μ¤€**
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| DPO λ°μ΄ν„°μ…‹ μ¤€λΉ„ | ν¬λ§· λ³€ν™ | 1μ‹κ°„ |
| **DPO on LoRA** | LoRA μ²΄ν¬ν¬μΈνΈμ—μ„ μ‹μ‘ | 4-6μ‹κ°„ |
| ν¨μ¨μ„± λ©”νΈλ¦­ μμ§‘ | DPO λ©”λ¨λ¦¬, μ‹κ°„ μΈ΅μ • | ν†µν•© |

**A100 GPU (μµμ ν™”λ¨) β΅**
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| DPO λ°μ΄ν„°μ…‹ μ¤€λΉ„ | ν¬λ§· λ³€ν™ | 1μ‹κ°„ |
| **DPO on LoRA** | Batch 8, μµμ ν™” μ„¤μ • | 1-2μ‹κ°„ |
| ν¨μ¨μ„± λ©”νΈλ¦­ μμ§‘ | DPO λ©”λ¨λ¦¬, μ‹κ°„ μΈ΅μ • | ν†µν•© |

#### DPO μ„¤μ • (config.jsonμ— λ°μλ¨)
```python
# T4 κΈ°λ³Έ
DPO_CONFIG_T4 = {
    "beta": 0.1,
    "learning_rate": 5e-5,
    "num_train_epochs": 1,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8
}

# A100 μµμ ν™”
DPO_CONFIG_A100 = {
    "beta": 0.1,
    "learning_rate": 5e-5,
    "num_train_epochs": 1,
    "per_device_train_batch_size": 8,    # T4: 2 β†’ A100: 8 (4λ°°)
    "gradient_accumulation_steps": 2,    # T4: 8 β†’ A100: 2
    "save_steps": 100                     # T4: 200 β†’ A100: 100
}
# λ©”λ¨λ¦¬: ~30-35GB (policy + reference λ¨λΈ λ™μ‹ λ΅λ“)
```

#### μ²΄ν¬ν¬μΈνΈ
- [ ] DPO λ°μ΄ν„°μ…‹ μ¤€λΉ„ μ™„λ£
- [ ] DPO ν•™μµ μ™„λ£ (ν¨μ¨μ„± λ©”νΈλ¦­ ν¬ν•¨)
- [ ] models/dpo/final/ μ €μ¥
- [ ] DPO ν¨μ¨μ„± λ©”νΈλ¦­ JSON μ €μ¥

### 5.3 Week 3 μ‚°μ¶λ¬Ό
| μ‚°μ¶λ¬Ό | κ²½λ΅ | μ„¤λ… | μƒνƒ |
|--------|------|------|------|
| LoRA SFT λ…ΈνΈλ¶ | notebooks/05_sft_training.ipynb | LoRA ν•™μµ + λ©”νΈλ¦­ | β… μ™„λ£ (2025-12-26) |
| Prompt Tuning λ…ΈνΈλ¶ | notebooks/05b_prompt_tuning.ipynb | PT ν•™μµ + λ©”νΈλ¦­ | β… μ™„λ£ (2025-12-26) |
| DPO λ…ΈνΈλ¶ | notebooks/06_dpo_training.ipynb | DPO ν•™μµ + λ©”νΈλ¦­ | β³ μ„ νƒμ‚¬ν•­ |
| LoRA λ¨λΈ | models/sft/final/ | LoRA adapters (~50MB) | β… μ €μ¥ μ™„λ£ |
| Prompt Tuning λ¨λΈ | models/prompt_tuning/final/ | Soft prompts (~1MB) | β… μ €μ¥ μ™„λ£ |
| DPO λ¨λΈ | models/dpo/final/ | DPO adapters | β³ μ„ νƒμ‚¬ν•­ |
| ν¨μ¨μ„± λ©”νΈλ¦­ | evaluation/metrics/*.json | lora_metrics.json, prompt_tuning_metrics.json | β… μ™„λ£ |

### 5.4 μ‹¤μ  ν•™μµ κ²°κ³Ό μ”μ•½

#### LoRA SFT (Notebook 05)
- **λ°νƒ€μ„**: 8.2λ¶„ (A100)
- **λΉ„μ©**: 0.73 compute units
- **Train Loss**: 0.748
- **Eval Loss**: 0.541
- **Trainable Params**: 12,156,928 (0.67%)
- **Peak Memory**: 5.31 GB
- **Inference Speed**: 7.70 tok/s
- **Model Size**: ~50 MB

#### Prompt Tuning (Notebook 05b)
- **λ°νƒ€μ„**: 18.8λ¶„ (A100)
- **λΉ„μ©**: 1.68 compute units
- **Train Loss**: 5.223
- **Eval Loss**: 2.979
- **Trainable Params**: 61,440 (0.003%)
- **Peak Memory**: 5.94 GB
- **Inference Speed**: 8.44 tok/s
- **Model Size**: ~1 MB

#### λΉ„κµ λ¶„μ„
| Metric | LoRA | Prompt Tuning | Winner |
|--------|------|---------------|--------|
| Trainable Params | 12.16M | 61K | π† PT (197x fewer) |
| Training Time | 8.2 min | 18.8 min | π† LoRA (2.3x faster) |
| Eval Loss | 0.541 | 2.979 | π† LoRA (5.5x better) |
| Model Size | ~50 MB | ~1 MB | π† PT (50x smaller) |
| Inference Speed | 7.70 tok/s | 8.44 tok/s | π† PT (9.6% faster) |

---

## 6. Week 4: ν‰κ°€ + λ¬Έμ„ν™”

### 6.1 Day 1-2: λ²¤μΉλ§ν¬ ν‰κ°€

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| lm-eval μ„¤μ • | ν™κ²½ κµ¬μ„± | 1μ‹κ°„ |
| Base λ¨λΈ ν‰κ°€ | 3κ° λ¨λΈ Γ— 3κ° λ²¤μΉλ§ν¬ | 3-4μ‹κ°„ |
| SFT λ¨λΈ ν‰κ°€ | 3κ° λ¨λΈ Γ— 3κ° λ²¤μΉλ§ν¬ | 3-4μ‹κ°„ |
| DPO λ¨λΈ ν‰κ°€ | 3κ° λ¨λΈ Γ— 3κ° λ²¤μΉλ§ν¬ | 3-4μ‹κ°„ |
| κ²°κ³Ό λ¶„μ„ | λΉ„κµ ν…μ΄λΈ”, μ°¨νΈ | 2μ‹κ°„ |

#### ν‰κ°€ λ…λ Ήμ–΄
```bash
# IFEval ν‰κ°€
lm_eval --model hf \
    --model_args pretrained=./models/dpo/llama \
    --tasks ifeval \
    --batch_size 4 \
    --output_path ./evaluation/results/llama_dpo_ifeval.json

# MT-Bench, MMLUλ„ λ™μΌ λ°©μ‹
```

#### μ²΄ν¬ν¬μΈνΈ
- [ ] Base λ¨λΈ 3κ° ν‰κ°€ μ™„λ£
- [ ] SFT λ¨λΈ 3κ° ν‰κ°€ μ™„λ£
- [ ] DPO λ¨λΈ 3κ° ν‰κ°€ μ™„λ£
- [ ] λΉ„κµ ν…μ΄λΈ” μƒμ„± μ™„λ£

### 6.2 Day 3: Agent ν‰κ°€ (μ¶”κ°€)

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| AgentBench μ„¤μ • | ν™κ²½ κµ¬μ„± | 1μ‹κ°„ |
| DPO λ¨λΈ ν‰κ°€ | 3κ° λ¨λΈ ν‰κ°€ | 2-3μ‹κ°„ |
| κ²°κ³Ό λ¶„μ„ | κΈ°μ΅΄ κ²°κ³Όμ™€ ν†µν•© | 1μ‹κ°„ |

#### μ²΄ν¬ν¬μΈνΈ
- [ ] AgentBench μ„¤μ • μ™„λ£
- [ ] 3κ° DPO λ¨λΈ Agent ν‰κ°€ μ™„λ£
- [ ] κ²°κ³Ό ν†µν•© μ™„λ£

### 6.3 Day 4-5: λ¬Έμ„ν™” + λ°ν‘ μ¤€λΉ„

#### μ‘μ—… λ©λ΅
| μ‘μ—… | μ„¤λ… | μμƒ μ‹κ°„ |
|------|------|----------|
| κΈ°μ  λ³΄κ³ μ„ μ‘μ„± | μ „μ²΄ κ³Όμ • μ„¤λ… | 4-6μ‹κ°„ |
| λ°ν‘ μλ£ μ μ‘ | PPT/Slides | 3-4μ‹κ°„ |
| μ½”λ“ μ •λ¦¬ | μ£Όμ„, README | 2μ‹κ°„ |
| Hugging Face μ—…λ΅λ“ | λ¨λΈ + λ°μ΄ν„°μ…‹ | 2μ‹κ°„ |

#### λ³΄κ³ μ„ κµ¬μ΅°
```markdown
1. μ„λ΅  - ν”„λ΅μ νΈ λ©μ , λ°°κ²½
2. κ΄€λ ¨ μ—°κµ¬ - Magpie, DPO, LoRA
3. λ°©λ²•λ΅  - νμ΄ν”„λΌμΈ μ„¤λ…
4. μ‹¤ν— μ„¤μ • - λ¨λΈ, λ°μ΄ν„°, ν•μ΄νΌνλΌλ―Έν„°
5. κ²°κ³Ό - λ²¤μΉλ§ν¬ κ²°κ³Ό, λ¶„μ„
6. λ…Όμ - μ„±κ³µ μ”μΈ, ν•κ³„μ 
7. κ²°λ΅ 
```

#### μ²΄ν¬ν¬μΈνΈ
- [ ] κΈ°μ  λ³΄κ³ μ„ μ™„μ„±
- [ ] λ°ν‘ μλ£ μ™„μ„±
- [ ] README.md μ‘μ„± μ™„λ£
- [ ] Hugging Face Hub μ—…λ΅λ“ μ™„λ£

### 6.4 Week 4 μ‚°μ¶λ¬Ό
| μ‚°μ¶λ¬Ό | κ²½λ΅ | μ„¤λ… |
|--------|------|------|
| ν‰κ°€ λ…ΈνΈλ¶ | notebooks/07_benchmark_evaluation.ipynb | λ²¤μΉλ§ν¬ |
| Agent λ…ΈνΈλ¶ | notebooks/08_agent_evaluation.ipynb | AgentBench |
| **λΉ„κµ λ¶„μ„ λ…ΈνΈλ¶** | notebooks/09_comparative_analysis.ipynb | μΆ…ν•© λΉ„κµ |
| ν‰κ°€ κ²°κ³Ό | evaluation/results/ | JSON νμΌλ“¤ |
| λΉ„κµ ν…μ΄λΈ” | evaluation/metrics/comparison_summary.csv | κ²°κ³Ό μ •λ¦¬ |
| μ‹κ°ν™” | evaluation/figures/ | λΉ„κµ μ°¨νΈ μ΄λ―Έμ§€ |
| Written Report | docs/report_template.md | μµμΆ… λ³΄κ³ μ„ |
| Presentation | docs/presentation_template.md | λ°ν‘ μλ£ |

---

## 7. λ§μΌμ¤ν†¤

| λ§μΌμ¤ν†¤ | μ™„λ£ κΈ°μ¤€ | λ©ν‘μΌ | μƒνƒ |
|----------|----------|--------|------|
| M1: ν™κ²½ μ¤€λΉ„ | Colab + λ¨λΈ λ΅λ”© μ„±κ³µ | Week 1 Day 2 | β… μ™„λ£ |
| M2: λ°μ΄ν„° μƒμ„± | 1,500κ° raw λ°μ΄ν„° | Week 1 Day 5 | β… μ™„λ£ (2025-12-24) |
| M3: λ°μ΄ν„° μ •μ  | 1,000κ° filtered (preference λ―Έμ™„) | Week 2 Day 5 | β… μ™„λ£ (ν•„ν„°λ§λ§) |
| M4: SFT μ™„λ£ | LoRA + Prompt Tuning μ™„λ£ | Week 3 Day 3 | β… μ™„λ£ (2025-12-26) |
| M5: DPO μ™„λ£ | DPO ν•™μµ λ° μ²΄ν¬ν¬μΈνΈ | Week 3 Day 5 | β³ λ‹¤μ λ‹¨κ³„ |
| M6: ν‰κ°€ μ™„λ£ | λ¨λ“  λ²¤μΉλ§ν¬ κ²°κ³Ό | Week 4 Day 3 | β³ λ€κΈ° |
| M7: ν”„λ΅μ νΈ μ™„λ£ | λ³΄κ³ μ„ + λ°ν‘ μλ£ | Week 4 Day 5 | π”„ μ§„ν–‰ μ¤‘ (λ³΄κ³ μ„ μ™„λ£) |

---

## 8. λ¦¬μ¤ν¬ λ€μ‘ κ³„ν

### 8.1 μ‹κ°„ μ§€μ—° μ‹
| μƒν™© | λ€μ‘ |
|------|------|
| λ°μ΄ν„° μƒμ„± μ§€μ—° | 1,000κ°λ΅ μ¶•μ† (μ΄λ―Έ μµμ† κ·λ¨) |
| λ¨λΈ ν•™μµ μ§€μ—° | Mistral-7B μ μ™Έ (3B 2κ°λ§) |
| ν‰κ°€ μ§€μ—° | IFEvalλ§ ν•„μ, λ‚λ¨Έμ§€ μ„ νƒ |

### 8.2 κΈ°μ μ  λ¬Έμ  μ‹
| μƒν™© | λ€μ‘ |
|------|------|
| GPU OOM | λ°°μΉ ν¬κΈ° 2λ΅ μ¶•μ†, gradient accumulation μ¦κ°€ |
| λ¨λΈ μλ ΄ μ‹¤ν¨ | learning rate μ΅°μ •, epoch μ¦κ°€ |
| Colab λκΉ€ | μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬μ‹μ‘ |

---

## 9. μ „μ²΄ λ…ΈνΈλ¶ μƒνƒ μ”μ•½

### 9.1 μ™„λ£λ λ…ΈνΈλ¶ β…

| Notebook | Status | Runtime | Cost | Output |
|----------|--------|---------|------|--------|
| 01_setup | β… μ™„λ£ | ~5λ¶„ | Free | ν™κ²½ μ„¤μ • |
| 02_magpie_generation | β… μ™„λ£ | ~3.5μ‹κ°„ | Free (T4) | 1,500 raw samples |
| 03_quality_filtering | β… μ™„λ£ | ~15λ¶„ | Free | 1,000 filtered samples |
| 04_preference_generation_STABLE_OPTIMIZED | β… μ™„λ£ | 2025-12-26 | Free | 600 preference pairs |
| 05_sft_training | β… μ™„λ£ | 8.2λ¶„ | 0.73 units | LoRA model (~50MB) |
| 05b_prompt_tuning | β… μ™„λ£ | 18.8λ¶„ | 1.68 units | PT model (~1MB) |

**μ΄ λΉ„μ©**: ~2.41 compute units (100 units μ¤‘ 2.41% μ‚¬μ©)

### 9.2 μ„ νƒμ‚¬ν•­ λ…ΈνΈλ¶ (Optional)

| Notebook | Status | Dependency | λΉ„κ³  |
|----------|--------|------------|------|
| 06_dpo_training | β³ μ¤€λΉ„λ¨ | 04, 05 μ™„λ£ | DPO ν•™μµ (μ„ νƒμ‚¬ν•­) |
| 07_benchmark_evaluation | β³ μ¤€λΉ„λ¨ | 05, 05b μ™„λ£ | λ²¤μΉλ§ν¬ ν‰κ°€ (μ„ νƒμ‚¬ν•­) |
| 08_agent_evaluation | β³ μ¤€λΉ„λ¨ | 05 λλ” 06 μ™„λ£ | Agent λ¥λ ¥ ν‰κ°€ (μ„ νƒμ‚¬ν•­) |
| 09_comparative_analysis | β… μ‹¤ν–‰κ°€λ¥ | 05, 05b μ™„λ£ | LoRA vs PT λΉ„κµ |

### 9.3 ν”„λ΅μ νΈ μ™„λ£λ„

**ν•µμ‹¬ νμ΄ν”„λΌμΈ**: β… **100% μ™„λ£**
- λ°μ΄ν„° μƒμ„± β†’ ν•„ν„°λ§ β†’ SFT (LoRA + PT) β†’ λΉ„κµ λ¶„μ„

**μ„ νƒμ  ν™•μ¥**: β³ **μ¤€λΉ„ μ™„λ£**
- DPO, λ²¤μΉλ§ν¬ ν‰κ°€, Agent ν‰κ°€ (ν•„μ”μ‹ μ‹¤ν–‰ κ°€λ¥)

---

## 10. λ³€κ²½ μ΄λ ¥

| λ²„μ „ | λ‚ μ§ | λ³€κ²½ λ‚΄μ© | μ‘μ„±μ |
|------|------|----------|--------|
| 1.0 | 2025-12-23 | μ΄κΈ° μ‘μ„± | - |
| 1.1 | 2025-12-26 | Week 3 μ™„λ£ μƒνƒ μ—…λ°μ΄νΈ (LoRA, Prompt Tuning) | Claude |
| 1.2 | 2025-12-26 | μ‹¤μ  ν•™μµ κ²°κ³Ό λ°μ (A100 μµμ ν™”, 2.41 compute units) | Claude |
| 1.3 | 2025-12-26 | λ…ΈνΈλ¶ μƒνƒ μ”μ•½ μ„Ήμ… μ¶”κ°€, Dragon LLM μ°Έμ΅° μ κ±° | Claude |

---

*λ³Έ κ³„νμ„λ” ν”„λ΅μ νΈ μ§„ν–‰ μ¤‘ μƒν™©μ— λ”°λΌ μμ •λ  μ μμµλ‹λ‹¤.*

# 05_sft_training.ipynb - Complete Guide

## âœ… COMPLETED on A100 GPU (2025-12-26)

**ì‹¤ì œ í•™ìŠµ ê²°ê³¼:**
- **í•™ìŠµ ì‹œê°„**: 8.2ë¶„ (A100)
- **Eval Loss**: 0.541
- **Train Loss**: 0.748
- **ì´ Steps**: 114
- **ë¹„ìš©**: ~0.73 compute units (ì˜ˆìƒ 10-21 ëŒ€ë¹„ **15ë°° ì ˆê°**)
- **Trainable Parameters**: 12,156,928 (0.67% of model)

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [A100 ìµœì í™” ì„¤ì •](#a100-ìµœì í™”-ì„¤ì •)
3. [Fixed Issues](#fixed-issues)
4. [ì‹¤í–‰ ê°€ì´ë“œ](#ì‹¤í–‰-ê°€ì´ë“œ)
5. [ì„±ê³µ ê¸°ì¤€](#ì„±ê³µ-ê¸°ì¤€)
6. [ì—ëŸ¬ í•´ê²°](#ì—ëŸ¬-í•´ê²°)
7. [ì¶œë ¥ íŒŒì¼](#ì¶œë ¥-íŒŒì¼)
8. [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë…¸íŠ¸ë¶ì€ **LoRA (Low-Rank Adaptation)**ë¥¼ ì‚¬ìš©í•˜ì—¬ Llama-3.2-3B ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **Parameter-Efficient**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 0.67%ë§Œ í•™ìŠµ (12M / 1.8B)
- **A100 ìµœì í™”**: BF16 + TF32 ì§€ì›ìœ¼ë¡œ 2-3ë°° ë¹ ë¥¸ í•™ìŠµ
- **ì•ˆì •ì  í•™ìŠµ**: ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ì¤‘ë‹¨ ë³µêµ¬ ê°€ëŠ¥
- **ë¹„ìš© íš¨ìœ¨**: ì˜ˆìƒ ëŒ€ë¹„ 15ë°° ì €ë ´ (0.73 units vs 10-21 units)

---

## A100 ìµœì í™” ì„¤ì •

### ğŸš€ ì£¼ìš” ë³€ê²½ ì‚¬í•­

#### 1. **Batch Size ì¦ê°€**
- **T4**: 4 â†’ **A100**: 12 (3ë°° ì¦ê°€)
- **Gradient Accumulation**: 4 â†’ 2 (ì ˆë°˜ìœ¼ë¡œ)
- **Effective Batch Size**: 16 â†’ 24 (50% ì¦ê°€)

#### 2. **BF16 í™œì„±í™”**
- **FP16**: False (T4 ì „ìš©)
- **BF16**: True (A100 ë„¤ì´í‹°ë¸Œ ì§€ì›)
- **TF32**: True (ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ)

#### 3. **Checkpoint ê°„ê²© ì¡°ì •**
- **T4**: 50 steps â†’ **A100**: 100 steps
- ë” ë¹ ë¥¸ í•™ìŠµìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¹ˆë„ ê°ì†Œ
- Total steps: ~114 (900 samples Ã— 3 epochs Ã· 24)

#### 4. **Quantization dtype ë³€ê²½**
- **T4**: `torch.float16`
- **A100**: `torch.bfloat16`

### ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | T4 (Free) | A100 (Pro) | ì‹¤ì œ A100 ê²°ê³¼ | ê°œì„  |
|-----|-----------|-----------|---------------|------|
| **Batch Size** | 4 | 12 | 12 | 3x |
| **Effective Batch** | 16 | 24 | 24 | 1.5x |
| **í•™ìŠµ ì‹œê°„** | 6-10ì‹œê°„ | 2-4ì‹œê°„ | **8.2ë¶„** | **40-70ë°° ë¹ ë¦„!** |
| **Precision** | FP16 | BF16 | BF16 | ë” ì•ˆì •ì  |
| **ì´ Steps** | ~168 | ~112 | 114 | - |
| **ë¹„ìš©** | Free | 10-21 units | **0.73 units** | **15ë°° ì ˆê°** |
| **Eval Loss** | - | - | **0.541** | ìš°ìˆ˜ |

### ğŸ’° ì‹¤ì œ ë¹„ìš© ë¶„ì„

#### **SFT Training (Notebook 05)**
- **ì˜ˆìƒ**: 10-21 units
- **ì‹¤ì œ**: **0.73 units** (8.2ë¶„ Ã— 5.37 units/hr Ã· 60)
- **ì ˆê°**: **93-96%**

#### **ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤ì œ ë¹„ìš©**
- **SFT (05)**: 0.73 units
- **Prompt Tuning (05b)**: 1.68 units
- **ì´í•© (05+05b)**: **2.41 units** (ì˜ˆìƒ 31-58 ëŒ€ë¹„ **20ë°° ì ˆê°**)

---

## Fixed Issues

### ğŸ”§ API Compatibility âœ…

#### 1. **TRL API Updates**
- âŒ `evaluation_strategy` â†’ âœ… `eval_strategy`
- âŒ `dataset_text_field` â†’ âœ… `formatting_func`
- âŒ `max_seq_length` â†’ âœ… Removed (handled by trainer)
- âŒ `tokenizer` parameter â†’ âœ… Removed from SFTTrainer
- âŒ `packing` parameter â†’ âœ… Removed (not supported)

#### 2. **GPU Optimization** âœ…
- **Batch size**: 12 (A100 40GB VRAM)
- **Gradient accumulation**: 2
- **FP16**: False
- **BF16**: True (A100 native support)
- **TF32**: True
- **Checkpoint interval**: 100 steps

#### 3. **Documentation Updates** âœ…
- Cell 0: A100 ìµœì í™” ì„¤ëª… ì¶”ê°€
- Cell 25: ìµœì í™” ìƒì„¸ ì„¤ëª…
- Cell 4: Config ìë™ ì ìš©
- Cell 7: BF16/TF32 í™œì„±í™”

---

## ì‹¤í–‰ ê°€ì´ë“œ

### ğŸ¯ A100 ì‹¤í–‰ ë°©ë²•

#### **1. ëŸ°íƒ€ì„ ë³€ê²½**
```
ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU ìœ í˜•: A100
```

#### **2. ì„¸ì…˜ ì‹œì‘**
```
ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘
```

#### **3. Cell ìˆœì„œëŒ€ë¡œ ì‹¤í–‰**

**Cell 1-2**: Mount Drive
```
âœ… Expected: "Mounted at /content/drive"
```

**Cell 3**: Load config
```
âœ… Expected: "Configuration loaded!"
```

**Cell 4**: Verify config (A100 settings auto-applied)
```
âœ… Expected:
A100 GPU settings applied!
  Base model: meta-llama/Llama-3.2-3B
  Batch size: 12
  Gradient accumulation: 2
  Effective batch size: 24
  Expected A100 training time: 2-4 hours
```

**Cell 5**: Install libraries
```
âœ… Expected: "Libraries installed successfully!"
```

**Cell 6**: Check GPU
```
âœ… Expected:
GPU: NVIDIA A100-SXM4-40GB
GPU Memory: 40.00 GB
```

**Cell 7**: BF16/TF32 Setup
```
âœ… Expected:
==================================================
A100 GPU Performance Settings:
  BF16 enabled: True
  TF32 enabled: True
  Optimal for A100 40GB VRAM
==================================================
```

**Cell 27**: Training arguments
```
âœ… Expected:
Training arguments (A100 optimized):
  Epochs: 3
  Batch size: 12
  Gradient accumulation: 2
  Effective batch size: 24
  Learning rate: 0.0002
  BF16: True, TF32: True
  Total steps: ~112
  Save/eval every: 100 steps
  Expected training time: 2-4 hours on A100
```

**Cell 31**: START TRAINING
```
âœ… Expected:
Starting SFT training...
Start time: 2025-12-26 XX:XX:XX
==================================================
{'loss': 1.XXXX, 'learning_rate': 0.000XX, 'epoch': 0.XX}
...
```

### ğŸ“Š Training Configuration

**Expected Behavior:**
```
Training samples: 900
Validation samples: 100
Total steps: ~114 (3 epochs Ã— 900 / 24)

Checkpoints saved at:
  - Step 100 (~88%, ~7ë¶„)
  - Step 114 (100%, final)

Actual training time: 8.2 minutes on A100 âœ…
```

**Memory Usage:**
```
Model loading: ~2.2 GB
After LoRA: ~3-4 GB
During training: ~8-10 GB peak
Safe margin: 30+ GB free (on 40GB A100)
```

---

## ì„±ê³µ ê¸°ì¤€

### âœ… í•™ìŠµ ì„±ê³µ ì¡°ê±´

1. âœ… GPU í™•ì¸: `GPU: NVIDIA A100-SXM4-40GB`
2. âœ… BF16 í™œì„±í™”: `BF16: True`
3. âœ… Batch size: 12
4. âœ… Effective batch: 24
5. âœ… Cell 31 ì‹¤í–‰ ì‹œ ì—ëŸ¬ ì—†ìŒ
6. âœ… Lossê°€ ì‹œê°„ì— ë”°ë¼ ê°ì†Œ
7. âœ… Checkpoint ì €ì¥ í™•ì¸ (step 100, 114)
8. âœ… Final model saved to `models/sft/final/`
9. âœ… Training curves plotted (Cell 42)
10. âœ… **ì‹¤ì œ ì™„ë£Œ ì‹œê°„: 8.2ë¶„** (ì˜ˆìƒ 2-4ì‹œê°„ ëŒ€ë¹„ í›¨ì”¬ ë¹ ë¦„!)

### ğŸ“ˆ ì‹¤ì œ í•™ìŠµ ê²°ê³¼

```json
{
  "train_loss": 0.748,
  "eval_loss": 0.541,
  "total_steps": 114,
  "training_time_minutes": 8.2,
  "trainable_params": 12156928,
  "trainable_ratio_percent": 0.67,
  "peak_memory_gb": 5.31,
  "inference_speed_tok_per_sec": 7.70
}
```

---

## ì—ëŸ¬ í•´ê²°

### âš ï¸ Known Warnings (Safe to Ignore)

```python
UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
```
- **Reason**: Normal behavior with 4-bit quantization
- **Impact**: None on training quality

```python
UserWarning: Already found a `peft_config` attribute in the model.
```
- **Reason**: LoRA already applied in Cell 20
- **Impact**: None (expected behavior)

### ğŸ”´ Error Scenarios

#### Error 1: `NotImplementedError: BFloat16`
**Solution**: Make sure Cell 27 has `bf16=True` (A100) or `bf16=False` (T4)
```python
# A100
fp16=False,
bf16=True,   # A100 supports BF16
tf32=True,

# T4
fp16=True,
bf16=False,  # T4 doesn't support BF16
```

#### Error 2: `CUDA out of memory`
**Solution**: Reduce batch size
```python
# In Cell 4 or config.json
config['training']['sft_batch_size'] = 8  # Reduce from 12 to 8
```

#### Error 3: `TypeError: unexpected keyword argument`
**Solution**: This notebook is already fixed for latest TRL API
- If still occurs, check TRL version: `!pip show trl`
- Should be `>=0.7.4`

#### Error 4: Wrong GPU Type
**Solution**:
1. ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ A100 ì„ íƒ
2. ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘
3. Cell 6ì—ì„œ GPU í™•ì¸

---

## ì¶œë ¥ íŒŒì¼

### ğŸ“ í•™ìŠµ ì™„ë£Œ í›„ ìƒì„±ë˜ëŠ” íŒŒì¼

```
/content/drive/MyDrive/synthetic-instruction-tuner/
â”œâ”€â”€ models/sft/
â”‚   â”œâ”€â”€ sft-checkpoint/
â”‚   â”‚   â”œâ”€â”€ checkpoint-100/              # ~88% progress
â”‚   â”‚   â”œâ”€â”€ checkpoint-114/              # Final checkpoint
â”‚   â”‚   â””â”€â”€ logs/
â”‚   â””â”€â”€ final/
â”‚       â”œâ”€â”€ adapter_config.json          # LoRA configuration
â”‚       â”œâ”€â”€ adapter_model.safetensors    # ~50 MB (LoRA weights)
â”‚       â”œâ”€â”€ training_config.json         # Training metadata âœ…
â”‚       â””â”€â”€ tokenizer files
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â””â”€â”€ sft_training_curves.png      # Loss curves âœ…
â”‚   â””â”€â”€ metrics/
â”‚       â””â”€â”€ lora_metrics.json            # Efficiency metrics âœ…
```

### ğŸ“Š training_config.json ë‚´ìš©

```json
{
  "base_model": "meta-llama/Llama-3.2-3B",
  "training_data_size": 900,
  "validation_data_size": 100,
  "lora_config": {
    "r": 8,
    "alpha": 16,
    "dropout": 0.05,
    "target_modules": [
      "down_proj", "q_proj", "gate_proj", "up_proj",
      "v_proj", "k_proj", "o_proj"
    ]
  },
  "training_args": {
    "epochs": 3,
    "batch_size": 12,
    "gradient_accumulation_steps": 2,
    "learning_rate": 0.0002
  },
  "results": {
    "train_loss": 0.748087699998889,
    "eval_loss": 0.540707528591156,
    "total_steps": 114
  },
  "timestamp": "2025-12-26 13:44:13"
}
```

---

## ë‹¤ìŒ ë‹¨ê³„

### ğŸ¯ SFT ì™„ë£Œ í›„

#### **Option 1: Prompt Tuning ë¹„êµ** (ê¶Œì¥)
```
âœ… Notebook 05b: Prompt Tuning (A100, 18.8ë¶„, 1.68 units)
â†’ LoRA vs Prompt Tuning ë¹„êµ ë¶„ì„ ê°€ëŠ¥
```

#### **Option 2: DPO Training**
```
âœ… Notebook 06: DPO Training (A100, ì˜ˆìƒ 1-2ì‹œê°„)
â†’ Preference alignment ì ìš©
```

#### **Option 3: Evaluation**
```
âœ… Notebook 07: Benchmark Evaluation
âœ… Notebook 08: Agent Evaluation
âœ… Notebook 09: Comparative Analysis
```

### ğŸ“Š ì „ì²´ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™©

| Stage | Notebook | Status | ì‹œê°„ | ë¹„ìš© |
|-------|----------|--------|------|------|
| ë°ì´í„° ìƒì„± | 01-04 | âœ… ì™„ë£Œ | - | Free (T4) |
| LoRA SFT | 05 | âœ… ì™„ë£Œ | 8.2ë¶„ | 0.73 units |
| Prompt Tuning | 05b | âœ… ì™„ë£Œ | 18.8ë¶„ | 1.68 units |
| DPO | 06 | â³ ë‹¤ìŒ | ì˜ˆìƒ 1-2ì‹œê°„ | ì˜ˆìƒ 5-10 units |
| Benchmark | 07 | â³ ëŒ€ê¸° | - | - |
| Agent Eval | 08 | â³ ëŒ€ê¸° | - | - |
| Analysis | 09 | â³ ëŒ€ê¸° | - | - |

---

## ğŸ’¡ Tips & Best Practices

### **ë¹„ìš© ì ˆê° ì „ëµ**
1. âœ… ë°ì´í„° ìƒì„±ì€ ë¬´ë£Œ T4 ì‚¬ìš©
2. âœ… Fine-tuningë§Œ A100 ì‚¬ìš© (ì‹¤ì œë¡œ 10ë¶„ ë‚´ì™¸ë©´ ì¶©ë¶„)
3. âœ… ë¶ˆí•„ìš”í•œ ì…€ ì¬ì‹¤í–‰ ë°©ì§€
4. âœ… í•™ìŠµ ì™„ë£Œ í›„ ì¦‰ì‹œ ëŸ°íƒ€ì„ ì¢…ë£Œ

### **ì„±ëŠ¥ ìµœì í™”**
1. âœ… A100ì—ì„œëŠ” BF16 í•„ìˆ˜ ì‚¬ìš©
2. âœ… Batch sizeë¥¼ 16ê¹Œì§€ ì˜¬ë¦´ ìˆ˜ ìˆìŒ (ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ)
3. âœ… Gradient accumulationì„ 1ë¡œ ì¤„ì´ë©´ ë” ë¹ ë¦„
4. âš ï¸ ë‹¨, effective batch size ìœ ì§€ í•„ìš”

### **ì•ˆì •ì„± í™•ë³´**
1. âœ… BF16ì€ FP16ë³´ë‹¤ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì 
2. âœ… Checkpoint ì‹œìŠ¤í…œìœ¼ë¡œ ì¤‘ë‹¨ ë³µêµ¬ ê°€ëŠ¥
3. âœ… Gradient explosion ë°œìƒ ì‹œ learning rate ì¤„ì´ê¸°
4. âœ… ì •ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

### **ì‹¤ì „ ê²½í—˜ ê³µìœ **

**ì˜ˆìƒí–ˆë˜ ê²ƒ:**
- í•™ìŠµ ì‹œê°„: 2-4ì‹œê°„
- ë¹„ìš©: 10-21 units
- ì²´í¬í¬ì¸íŠ¸: 2-3íšŒ

**ì‹¤ì œ ê²°ê³¼:**
- í•™ìŠµ ì‹œê°„: **8.2ë¶„** (ì˜ˆìƒ ëŒ€ë¹„ 15-30ë°° ë¹ ë¦„!)
- ë¹„ìš©: **0.73 units** (ì˜ˆìƒ ëŒ€ë¹„ 15ë°° ì €ë ´!)
- ì²´í¬í¬ì¸íŠ¸: 2íšŒ (step 100, 114)

**êµí›ˆ:**
1. A100 ì„±ëŠ¥ì´ ì˜ˆìƒë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜
2. ë°ì´í„°ì…‹ í¬ê¸°(900 samples)ê°€ ì‘ì•„ì„œ ë¹ ë¥´ê²Œ ì™„ë£Œ
3. BF16 + TF32 ìµœì í™” íš¨ê³¼ê°€ í¼
4. 100 units êµ¬ë§¤ëŠ” ì¶©ë¶„íˆ ì—¬ìœ  ìˆìŒ

---

## ğŸ“ LoRA vs Full Fine-Tuning ë¹„êµ

| í•­ëª© | Full Fine-Tuning | LoRA (r=8) | ë¹„ìœ¨ |
|------|------------------|------------|------|
| Trainable Params | 1.8B | 12.16M | **0.67%** |
| Memory | ~24GB | ~6GB | **25%** |
| Training Time | 10-20ì‹œê°„ | 8.2ë¶„ | **1%** |
| Model Size | ~7GB | ~50MB | **0.7%** |
| Quality | 100% | ~95-98% | ìš°ìˆ˜ |

**ê²°ë¡ **: LoRAëŠ” í’ˆì§ˆì„ ê±°ì˜ ìœ ì§€í•˜ë©´ì„œ ë¹„ìš©/ì‹œê°„ì„ 1% ìˆ˜ì¤€ìœ¼ë¡œ ì ˆê°!

---

## ğŸ“š ì°¸ê³  ìë£Œ

### LoRA ë…¼ë¬¸
- **LoRA: Low-Rank Adaptation of Large Language Models** (2021)
- https://arxiv.org/abs/2106.09685

### ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Transformers**: 4.41.0+
- **PEFT**: 0.7.0+ (LoRA êµ¬í˜„)
- **TRL**: 0.7.4+ (SFTTrainer)
- **BitsAndBytes**: 0.41.3+ (4-bit quantization)

---

**Status**: âœ… **COMPLETED on A100 GPU (2025-12-26)**
**Training Time**: 8.2 minutes
**Cost**: 0.73 compute units
**Eval Loss**: 0.541 (Excellent!)
**Next**: Notebook 05b (Prompt Tuning) or 06 (DPO)

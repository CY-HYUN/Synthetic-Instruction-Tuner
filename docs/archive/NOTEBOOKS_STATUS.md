# Notebooks Status & Review

**Last Updated**: 2025-12-26
**Current Progress**: Notebooks 01-05b completed

---

## âœ… Completed Notebooks

### 01_setup.ipynb
- **Status**: âœ… Completed
- **Runtime**: ~5 minutes
- **Notes**: Environment setup and configuration loading

### 02_magpie_generation.ipynb
- **Status**: âœ… Completed (2025-12-24)
- **Output**: 1,500 raw instruction-response pairs
- **Runtime**: ~3.5 hours on T4 (free tier)
- **Files**: `data/raw/magpie_data.json`

### 03_quality_filtering.ipynb
- **Status**: âœ… Completed
- **Output**: 1,000 filtered samples (900 train, 100 val)
- **Runtime**: ~15 minutes
- **Files**: `data/filtered/sft_train.json`, `data/filtered/sft_val.json`

### 04_preference_generation_STABLE_OPTIMIZED.ipynb
- **Status**: âœ… **COMPLETED** (2025-12-26)
- **Output**: ì„ í˜¸ ë°ì´í„° ìƒì„± ì™„ë£Œ
- **Files**:
  - `data/preference/dpo_train.json` (1.2MB)
  - `data/preference/dpo_val.json` (128KB)
  - `data/preference/preference_data.json` (1.4MB)
- **Impact**: âœ… Notebook 06 (DPO Training) ì‹¤í–‰ ê°€ëŠ¥!

### 05_sft_training.ipynb
- **Status**: âœ… Completed (2025-12-26)
- **Method**: LoRA (r=8, alpha=16)
- **Runtime**: 8.2ë¶„ on A100
- **Cost**: 0.73 compute units
- **Results**:
  - Train Loss: 0.748
  - Eval Loss: 0.541
  - Trainable Params: 12,156,928 (0.67%)
- **Files**:
  - `models/sft/final/` (~50MB)
  - `evaluation/metrics/lora_metrics.json`
  - `evaluation/figures/sft_training_curves.png`

### 05b_prompt_tuning.ipynb
- **Status**: âœ… Completed (2025-12-26)
- **Method**: Prompt Tuning (20 virtual tokens)
- **Runtime**: 18.8ë¶„ on A100
- **Cost**: 1.68 compute units
- **Results**:
  - Train Loss: 5.223
  - Eval Loss: 2.979
  - Trainable Params: 61,440 (0.003%)
- **Files**:
  - `models/prompt_tuning/final/` (~1MB)
  - `evaluation/metrics/prompt_tuning_metrics.json`

---

## â³ Pending Notebooks

### 06_dpo_training.ipynb
- **Status**: âœ… **READY TO RUN**
- **Dependencies**: Requires `04_preference_generation.ipynb` output âœ…
- **Required Files**:
  - `data/preference/dpo_train.json` âœ… (1.2MB)
  - `data/preference/dpo_val.json` âœ… (128KB)
- **Estimated Runtime**: 1-2 hours on A100
- **Estimated Cost**: 5-10 compute units

#### ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
âœ… **ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥!**

ì„ í˜¸ ë°ì´í„°ê°€ ëª¨ë‘ ì¤€ë¹„ë˜ì–´ ìˆì–´ì„œ DPO í•™ìŠµì„ ë°”ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ë…¸íŠ¸ë¶ ìƒíƒœ

í˜„ì¬ notebookì€ ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ì‘ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. **ìˆ˜ì • ì—†ì´ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥**í•©ë‹ˆë‹¤.

**ì˜ˆìƒ ê²°ê³¼**:
- DPOë¡œ SFT ëª¨ë¸ì„ ì¶”ê°€ ì •ë ¬
- ì„ í˜¸ë„ ê¸°ë°˜ ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ
- `models/dpo/final/` ìƒì„± (~50MB)

---

### 07_benchmark_evaluation.ipynb
- **Status**: âœ… **READY TO RUN**
- **Dependencies**:
  - Base model âœ…
  - SFT model âœ…
  - DPO model âœ… (DPO ì™„ë£Œ í›„)
- **Estimated Runtime**: 2-3 hours on A100
- **Estimated Cost**: Minimal (evaluation only)

#### ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
âœ… **DPO ì™„ë£Œ í›„ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥**

í˜„ì¬ ë…¸íŠ¸ë¶ì€ Base, SFT, DPO 3ê°€ì§€ ëª¨ë¸ì„ ë¹„êµí•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

#### ë…¸íŠ¸ë¶ ìƒíƒœ
**ìˆ˜ì • í•„ìš” ì—†ìŒ** - Notebook 06 ì™„ë£Œ í›„ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë§Œì•½ Prompt Tuningë„ í•¨ê»˜ ë¹„êµí•˜ê³  ì‹¶ë‹¤ë©´ Cell ì¶”ê°€ í•„ìš”:
```python
# Cell 12-2: PT ëª¨ë¸ ì¶”ê°€ (ì„ íƒì‚¬í•­)
PT_MODEL_PATH = f"{config['paths']['models_prompt_tuning']}/final"
pt_tokenizer = AutoTokenizer.from_pretrained(PT_MODEL_PATH)
pt_model = PeftModel.from_pretrained(pt_base, PT_MODEL_PATH)
```

**í‰ê°€ ëŒ€ìƒ**:
```
Base â†’ SFT (LoRA) â†’ DPO
(+ Prompt Tuning ì„ íƒ ì¶”ê°€ ê°€ëŠ¥)
```

---

### 08_agent_evaluation.ipynb
- **Status**: âœ… **READY TO RUN**
- **Dependencies**:
  - DPO model âœ… (Notebook 06 ì™„ë£Œ í›„)
- **Estimated Runtime**: 1-2 hours on A100

#### ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
âœ… **DPO ì™„ë£Œ í›„ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥**

í˜„ì¬ ë…¸íŠ¸ë¶ì€ DPO ëª¨ë¸ì˜ agent ëŠ¥ë ¥ì„ í‰ê°€í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

#### ë…¸íŠ¸ë¶ ìƒíƒœ
**ìˆ˜ì • í•„ìš” ì—†ìŒ** - Notebook 06 ì™„ë£Œ í›„ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**í‰ê°€ ë‚´ìš©**:
- Multi-step planning
- Reasoning and problem solving
- Context maintenance
- Adapting to feedback
- Tool use simulation

---

### 09_comparative_analysis.ipynb
- **Status**: âœ… **READY TO RUN**
- **Dependencies**:
  - `evaluation/metrics/lora_metrics.json` âœ…
  - `evaluation/metrics/prompt_tuning_metrics.json` âœ…
  - `evaluation/metrics/dpo_metrics.json` âœ… (Notebook 06 ì™„ë£Œ í›„)

#### ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
âœ… **ì§€ê¸ˆ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥** (í˜„ì¬ LoRA + PT ë¹„êµ)
âœ… **DPO ì™„ë£Œ í›„ ì¬ì‹¤í–‰** (LoRA + PT + DPO 3ê°€ì§€ ë¹„êµ)

ì´ ë…¸íŠ¸ë¶ì€ ì´ë¯¸ ì„ íƒì ìœ¼ë¡œ metricsë¥¼ ë¡œë“œí•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**Cell 6**:
```python
methods = ['lora', 'prompt_tuning', 'dpo']
all_metrics = {}

for method in methods:
    metrics = load_metrics(method)
    if metrics:  # âœ… ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ë¡œë“œ
        all_metrics[method] = metrics
```

#### í˜„ì¬ ë¹„êµ ê°€ëŠ¥ (LoRA vs PT)

| Metric | LoRA | Prompt Tuning | Winner |
|--------|------|---------------|--------|
| Trainable Params | 12.16M | 61K | ğŸ† PT (197x fewer) |
| Trainable Ratio | 0.67% | 0.003% | ğŸ† PT |
| Training Time | 8.2 min | 18.8 min | ğŸ† LoRA (2.3x faster) |
| Train Loss | 0.748 | 5.223 | ğŸ† LoRA (7x better) |
| Eval Loss | 0.541 | 2.979 | ğŸ† LoRA (5.5x better) |
| Peak Memory | 5.31 GB | 5.94 GB | ğŸ† LoRA (0.63 GB less) |
| Inference Speed | 7.70 tok/s | 8.44 tok/s | ğŸ† PT (9.6% faster) |
| Model Size | ~50 MB | ~1 MB | ğŸ† PT (50x smaller) |

DPO ì™„ë£Œ í›„ì—ëŠ” 3ê°€ì§€ ë°©ë²• ëª¨ë‘ ë¹„êµ ê°€ëŠ¥í•©ë‹ˆë‹¤!

---

## ğŸ“Š í˜„ì¬ í”„ë¡œì íŠ¸ ìƒíƒœ ìš”ì•½

### âœ… ì™„ë£Œëœ ì‘ì—…

```
Week 1: ë°ì´í„° ìƒì„± (1,500 samples)
Week 2:
  - í’ˆì§ˆ í•„í„°ë§ (1,000 samples)
  - ì„ í˜¸ ë°ì´í„° ìƒì„± ì™„ë£Œ âœ…
Week 3:
  - LoRA SFT ì™„ë£Œ (8.2ë¶„, 0.541 eval loss)
  - Prompt Tuning ì™„ë£Œ (18.8ë¶„, 2.979 eval loss)
```

### â³ ë‹¤ìŒ ì‘ì—… (ëª¨ë‘ ì‹¤í–‰ ê°€ëŠ¥!)

```
Week 3:
  - DPO í•™ìŠµ âœ… ì¤€ë¹„ ì™„ë£Œ (ì„ í˜¸ ë°ì´í„° ìˆìŒ!)
Week 4:
  - Benchmark Evaluation âœ… ì¤€ë¹„ ì™„ë£Œ
  - Agent Evaluation âœ… ì¤€ë¹„ ì™„ë£Œ
  - Comparative Analysis âœ… ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥
```

### ğŸ’° ë¹„ìš© ë¶„ì„

**ì‹¤ì œ ì‚¬ìš© ë¹„ìš©**:
- Notebooks 01-04: 0 units (Free T4)
- Notebook 05 (LoRA): 0.73 units
- Notebook 05b (PT): 1.68 units
- **ì´í•©**: **2.41 units** (100 units ì¤‘ 2.41% ì‚¬ìš©)

**ë‚¨ì€ budgetìœ¼ë¡œ ê°€ëŠ¥í•œ ì‘ì—…**:
- Notebook 07 í‰ê°€: ~2-3 units (Base, SFT, PT ë¹„êµ)
- Notebook 08 í‰ê°€: ~1-2 units (SFT agent í‰ê°€)
- Notebook 09 ë¶„ì„: ~0.5 units (ì‹œê°í™”)
- **í•©ê³„**: ~4-6 units
- **ì „ì²´ í”„ë¡œì íŠ¸ ì˜ˆìƒ ì´ ë¹„ìš©**: ~6-8 units

**ì—¬ìœ  budget**: 92-94 units (ì¶©ë¶„í•¨!)

---

## ğŸ¯ ê¶Œì¥ ì§„í–‰ ë°©í–¥

### Option 1: í˜„ì¬ ê²°ê³¼ë¡œ ì™„ë£Œ (ê°•ë ¥ ê¶Œì¥)

```
âœ… ì¥ì :
- LoRA vs Prompt Tuning ë¹„êµëŠ” ì´ë¯¸ ì™„ë£Œ
- ì¶©ë¶„í•œ ì—°êµ¬ ê°€ì¹˜ (197ë°° íŒŒë¼ë¯¸í„° ì°¨ì´)
- ë¹„ìš© íš¨ìœ¨ì  (ì´ 6-8 units ì˜ˆìƒ)
- ë¹ ë¥¸ ì™„ë£Œ ê°€ëŠ¥ (1-2ì¼)

âŒ ë‹¨ì :
- DPO preference alignment ë¹„êµ ì—†ìŒ
- ì„ í˜¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¯¸ì™„ì„±
```

**ì§„í–‰ ìˆœì„œ**:
1. âœ… Notebook 07 ìˆ˜ì • ë° ì‹¤í–‰ (Base, SFT, PT ë¹„êµ)
2. âœ… Notebook 08 ìˆ˜ì • ë° ì‹¤í–‰ (SFT agent í‰ê°€)
3. âœ… Notebook 09 ì‹¤í–‰ (LoRA vs PT ë¹„êµ ë¶„ì„)
4. âœ… ìµœì¢… ë³´ê³ ì„œ ì‘ì„± (FINAL_REPORT.md ì´ë¯¸ ì™„ë£Œ)

### Option 2: DPOê¹Œì§€ ì™„ë£Œ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

```
âœ… ì¥ì :
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ì„±
- SFT â†’ DPO ë¹„êµ ê°€ëŠ¥
- ì„ í˜¸ ë°ì´í„° ìƒì„± ê²½í—˜

âŒ ë‹¨ì :
- ì¶”ê°€ 10-15ì‹œê°„ ì†Œìš” (ì„ í˜¸ ë°ì´í„° ìƒì„±)
- ì¶”ê°€ 50-65 compute units ì†Œìš”
- ì‹œê°„ ëŒ€ë¹„ ê°€ì¹˜ ë‚®ìŒ (ì´ë¯¸ ì¶©ë¶„í•œ ë¹„êµ ìˆìŒ)
```

---

## ğŸ“ ê° Notebook ìˆ˜ì • ê°€ì´ë“œ

### 06_dpo_training.ipynb

**ì‹¤í–‰ ì—¬ë¶€**: â­ï¸ SKIP ê¶Œì¥

ë§Œì•½ ì‹¤í–‰í•˜ë ¤ë©´:
1. ë¨¼ì € `04_preference_generation_STABLE.ipynb` ì‹¤í–‰ í•„ìš”
2. Cell 8ì— ë°ì´í„° ì¡´ì¬ í™•ì¸ ì½”ë“œ ì¶”ê°€

### 07_benchmark_evaluation.ipynb

**ìˆ˜ì • í•„ìš”**:

```python
# Cell 7: ëª¨ë¸ ê²½ë¡œ ìˆ˜ì •
BASE_MODEL_ID = config['models']['sft_base']
SFT_MODEL_PATH = f"{config['paths']['models_sft']}/final"
PT_MODEL_PATH = f"{config['paths']['models_prompt_tuning']}/final"

# Cell 12: Prompt Tuning ëª¨ë¸ ë¡œë“œ ì¶”ê°€
print("Loading Prompt Tuning model...")
pt_tokenizer = AutoTokenizer.from_pretrained(PT_MODEL_PATH)
pt_base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
pt_model = PeftModel.from_pretrained(pt_base, PT_MODEL_PATH)
pt_model.eval()
print("Prompt Tuning model loaded!")

# Cell 14: DPO ë¶€ë¶„ ì œê±° ë˜ëŠ” ì¡°ê±´ë¶€ ì²˜ë¦¬
# DPO ê´€ë ¨ ì½”ë“œ ì£¼ì„ ì²˜ë¦¬

# Cell 19: í‰ê°€ ë£¨í”„ì— PT ì¶”ê°€
base_resp = generate_response(base_model, base_tokenizer, test['instruction'])
sft_resp = generate_response(sft_model, sft_tokenizer, test['instruction'])
pt_resp = generate_response(pt_model, pt_tokenizer, test['instruction'])  # ì¶”ê°€
# dpo_resp ì œê±°

results.append({
    "instruction": test['instruction'],
    "constraint": test['constraint'],
    "base": base_resp,
    "sft": sft_resp,
    "pt": pt_resp,  # ì¶”ê°€
})
```

### 08_agent_evaluation.ipynb

**ìˆ˜ì • í•„ìš”**:

```python
# Cell 7: ëª¨ë¸ ê²½ë¡œ ë³€ê²½
# DPO ëŒ€ì‹  SFT ì‚¬ìš©
MODEL_PATH = f"{config['paths']['models_sft']}/final"
MODEL_TYPE = "SFT (LoRA)"

print(f"Loading {MODEL_TYPE} model from: {MODEL_PATH}")

# Cell 0 (Markdown): ì„¤ëª… ìˆ˜ì •
# "This notebook evaluates agent capabilities of the DPO model"
# â†’ "This notebook evaluates agent capabilities of the SFT model"
```

### 09_comparative_analysis.ipynb

**ìˆ˜ì • ë¶ˆí•„ìš”**: âœ… ì´ë¯¸ í˜„ì¬ ìƒí™©ì— ë§ê²Œ ì‘ì„±ë¨

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥

1. **Notebook 09 ì‹¤í–‰** (ìˆ˜ì • ë¶ˆí•„ìš”)
   - LoRA vs Prompt Tuning ë¹„êµ ì‹œê°í™”
   - ì˜ˆìƒ ì‹œê°„: 30ë¶„
   - ì˜ˆìƒ ë¹„ìš©: 0.5 units

2. **Notebook 08 ìˆ˜ì • ë° ì‹¤í–‰**
   - SFT ëª¨ë¸ë¡œ agent í‰ê°€
   - ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„
   - ì˜ˆìƒ ë¹„ìš©: 1-2 units

3. **Notebook 07 ìˆ˜ì • ë° ì‹¤í–‰**
   - Base, SFT, PT ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
   - ì˜ˆìƒ ì‹œê°„: 2-3ì‹œê°„
   - ì˜ˆìƒ ë¹„ìš©: 2-3 units

### ìµœì¢… ë¬¸ì„œí™”

4. **FINAL_REPORT.md ê²€í† **
   - ì´ë¯¸ ì‘ì„± ì™„ë£Œ
   - DPO ê´€ë ¨ ë‚´ìš© ì œê±° í•„ìš” ì—¬ë¶€ í™•ì¸

5. **ë°œí‘œ ìë£Œ ì¤€ë¹„**
   - LoRA vs Prompt Tuning ë¹„êµ ê°•ì¡°
   - ë¹„ìš© íš¨ìœ¨ì„± ê°•ì¡° (ì˜ˆìƒ ëŒ€ë¹„ 20ë°° ì ˆê°)

---

**Last Updated**: 2025-12-26
**Review Status**: âœ… Complete
**Next Action**: Notebook 09 ì‹¤í–‰ í›„ 07, 08 ìˆ˜ì • ë° ì‹¤í–‰

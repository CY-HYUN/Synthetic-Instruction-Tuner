# Colab Pro A100 ìµœì í™” í”Œëœ - Synthetic-Instruction-Tuner

## ëª©í‘œ
Google Colab Proì˜ A100 GPU (40GB VRAM)ì— ë§ì¶° notebooksë¥¼ ìµœì í™”í•˜ì—¬:
- í•™ìŠµ ì†ë„ 2-3ë°° í–¥ìƒ
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶• (33-43ì‹œê°„ â†’ 13-20ì‹œê°„)
- 1,500 ìƒ˜í”Œ ìœ ì§€ (ìŠ¤ì¼€ì¼ì—… ì—†ìŒ)
- ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ì‹ ìœ ì§€

## í•µì‹¬ ë³€ê²½ì‚¬í•­

### 1. ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¦ê°€
- **SFT**: 4 â†’ 12 (3ë°°)
- **DPO**: 2 â†’ 8 (4ë°°)
- **Gradient Accumulation**: 4/8 â†’ 2 (ê°ì†Œ)

### 2. ì²´í¬í¬ì¸íŠ¸ ê°„ê²© ì¡°ì •
- **ë°ì´í„° ìƒì„±**: 20 â†’ 100 (5ë°° ê°ì†Œ, ë””ìŠ¤í¬ I/O ìµœì í™”)
- **SFT í•™ìŠµ**: 500 â†’ 200 steps
- **DPO í•™ìŠµ**: 200 â†’ 100 steps

### 3. í•œêµ­ì–´ ì„¤ëª… ì¶”ê°€
- A100 ìµœì í™” ê·¼ê±°
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
- í•™ìŠµ ì‹œê°„ ë¹„êµ (T4 vs A100)

---

## ìˆ˜ì •í•  íŒŒì¼ (ìš°ì„ ìˆœìœ„ ìˆœ)

### âš ï¸ CRITICAL: config.json
**íŒŒì¼**: `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\config.json`

#### ë³€ê²½ 1: ì²´í¬í¬ì¸íŠ¸ ê°„ê²© (Line 11)
```json
"checkpoint_interval": 100,  // 20 â†’ 100
```

#### ë³€ê²½ 2: Training ì„¹ì…˜ ì¬êµ¬ì¡°í™” (Lines 23-52)
**í˜„ì¬ ì¤‘ì²© êµ¬ì¡°ë¥¼ í‰ë©´ êµ¬ì¡°ë¡œ ë³€ê²½:**
```json
"training": {
  "sft_epochs": 3,
  "sft_batch_size": 12,           // 4 â†’ 12 (3ë°°)
  "sft_learning_rate": 2e-4,
  "dpo_epochs": 1,
  "dpo_batch_size": 8,            // 2 â†’ 8 (4ë°°)
  "dpo_learning_rate": 5e-5,
  "dpo_beta": 0.1,
  "gradient_accumulation_steps": 2,  // ê³µí†µ 2
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05
}
```

#### ë³€ê²½ 3: LoRA ì„¹ì…˜ ì‚­ì œ (Lines 54-68)
**ì „ì²´ ì„¹ì…˜ ì œê±°** (training ì„¹ì…˜ì— í†µí•©ë¨)

---

### HIGH: 05_sft_training.ipynb
**íŒŒì¼**: `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\05_sft_training.ipynb`

#### ë³€ê²½ 1: í—¤ë” ì—…ë°ì´íŠ¸ (Cell 0)
```markdown
**Training settings** (Colab Pro A100 ìµœì í™”):
- Batch size: 12 (A100 40GB VRAM í™œìš©, T4 ëŒ€ë¹„ 3ë°° ì¦ê°€)
- Gradient accumulation: 2 (ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ ê°ì†Œ)

**Expected runtime**:
- T4: 6-10 hours
- A100: 2-4 hours (2-3ë°° ë¹ ë¦„)

**A100 ìµœì í™” í¬ì¸íŠ¸**:
- ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ
- 40GB VRAMìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ
```

#### ë³€ê²½ 2: ìµœì í™” ì„¤ëª… ì…€ ì¶”ê°€ (Cell 24 ì´ì „)
**ìƒˆ Markdown ì…€ ì‚½ì…:**
```markdown
## A100 ìµœì í™” ì„¤ì •

- **Batch size: 12** (T4ì˜ 4ì—ì„œ 3ë°° ì¦ê°€)
  - 40GB VRAM í™œìš©í•˜ì—¬ ë” í° ë°°ì¹˜ ì²˜ë¦¬
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~20-25GB / 40GB (ì•ˆì „)

- **Gradient accumulation: 2**
  - Effective batch size: 12 Ã— 2 = 24

- **í•™ìŠµ íš¨ê³¼**: ì „ì²´ í•™ìŠµ ì‹œê°„ 2-3ë°° ë‹¨ì¶•
```

#### ë³€ê²½ 3: Checkpoint ì„¤ì • (Cell 25, ~line 368)
```python
save_steps=200,  # 500 â†’ 200
```

#### ë³€ê²½ 4: Print ë¬¸ ì—…ë°ì´íŠ¸ (Cell 25, ~line 383)
```python
print(f"  Batch size: {training_args.per_device_train_batch_size} (T4: 4 â†’ A100: 12, 3ë°° ì¦ê°€)")
print(f"\nğŸ’¡ A100 40GB VRAM í™œìš© â†’ í•™ìŠµ ì†ë„ 2-3ë°° í–¥ìƒ ì˜ˆìƒ")
```

---

### HIGH: 06_dpo_training.ipynb
**íŒŒì¼**: `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\06_dpo_training.ipynb`

#### ë³€ê²½ 1: í—¤ë” ì—…ë°ì´íŠ¸ (Cell 0)
```markdown
**DPO settings** (Colab Pro A100 ìµœì í™”):
- Batch size: 8 (A100 40GB VRAM í™œìš©, T4 ëŒ€ë¹„ 4ë°° ì¦ê°€)
- Gradient accumulation: 2

**Expected runtime**:
- T4: 4-6 hours
- A100: 1-2 hours (3-4ë°° ë¹ ë¦„)

**A100 ìµœì í™” í¬ì¸íŠ¸**:
- DPOëŠ” ë‘ ëª¨ë¸ ë™ì‹œ ë¡œë“œ â†’ ë§¤ìš° ë©”ëª¨ë¦¬ ì§‘ì•½ì 
- A100 40GBë¡œ ë°°ì¹˜ í¬ê¸° ëŒ€í­ ì¦ê°€ (2â†’8, 4ë°°)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~30-35GB / 40GB
```

#### ë³€ê²½ 2: ìµœì í™” ì„¤ëª… ì…€ ì¶”ê°€ (Cell 20 ì´ì „)
**ìƒˆ Markdown ì…€:**
```markdown
## A100 ìµœì í™” ì„¤ì •

- **Batch size: 8** (T4ì˜ 2ì—ì„œ 4ë°° ì¦ê°€)
  - DPOëŠ” policy + reference model ë™ì‹œ ë¡œë“œ
  - A100 40GBë¡œ ë°°ì¹˜ 8 ê°€ëŠ¥

- **í•™ìŠµ íš¨ê³¼**: ì „ì²´ í•™ìŠµ ì‹œê°„ 3-4ë°° ë‹¨ì¶•
```

#### ë³€ê²½ 3: Gradient Accumulation ìˆ˜ì • (Cell 20, line 278)
```python
gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
# * 2 ì œê±° (A100: ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ ë¶ˆí•„ìš”)
```

#### ë³€ê²½ 4: Checkpoint ì„¤ì • (Cell 20, ~line 293)
```python
save_steps=100,  # 200 â†’ 100
```

#### ë³€ê²½ 5: Print ë¬¸ ì—…ë°ì´íŠ¸ (Cell 20, ~line 303)
```python
print(f"  Batch size: {training_args.per_device_train_batch_size} (T4: 2 â†’ A100: 8, 4ë°° ì¦ê°€)")
print(f"\nğŸ’¡ A100 40GB VRAMìœ¼ë¡œ reference modelê³¼ í•¨ê»˜ í° ë°°ì¹˜ ì‚¬ìš© ê°€ëŠ¥")
```

---

### MEDIUM: 02_magpie_generation.ipynb
**íŒŒì¼**: `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\02_magpie_generation.ipynb`

#### ë³€ê²½ 1: í—¤ë” ì—…ë°ì´íŠ¸ (Cell 0)
```markdown
**Expected runtime**:
- T4: 16-17h â†’ 3ì¼ ë¶„í•  (12ì‹œê°„ ì œí•œ)
- A100: 6-8h â†’ í•œ ë²ˆì— ì™„ë£Œ (ëŸ°íƒ€ì„ ì œí•œ ì—†ìŒ)

**Checkpoint Strategy**:
- T4: 500 samples/day, checkpoint every 20
- A100: 1,500 samples ì—°ì†, checkpoint every 100

**Tip (A100)**: í•œ ì„¸ì…˜ì—ì„œ ì „ì²´ ì™„ë£Œ ê°€ëŠ¥
```

#### ë³€ê²½ 2: Checkpoint ë©”ì‹œì§€ (Cell 17)
```python
if CHECKPOINT_INTERVAL >= 100:
    print(f"   âœ“ A100 ìµœì í™”: ì²´í¬í¬ì¸íŠ¸ ê°„ê²© ì¦ê°€ë¡œ ë””ìŠ¤í¬ I/O ê°ì†Œ")
```

#### ë³€ê²½ 3: Checkpoint ë¡œê¹… ê°œì„  (Cell 20)
```python
if len(generated_data) % CHECKPOINT_INTERVAL == 0:
    save_checkpoint(generated_data, CHECKPOINT_PATH)
    progress_pct = (len(generated_data) / TARGET_SAMPLES) * 100
    print(f"âœ“ Checkpoint: {len(generated_data)}/{TARGET_SAMPLES} ({progress_pct:.1f}%)")
```

---

### MEDIUM: 04_preference_generation.ipynb
**íŒŒì¼**: `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\04_preference_generation.ipynb`

#### ë³€ê²½ 1: í—¤ë” ì—…ë°ì´íŠ¸ (Cell 0)
```markdown
**Expected runtime**:
- T4: 4-6 hours
- A100: 2-3 hours

**Tip (A100)**: í•œ ì„¸ì…˜ì—ì„œ 600 pairs ì™„ë£Œ ê°€ëŠ¥
```

#### ë³€ê²½ 2: Checkpoint ë…¸íŠ¸ ì¶”ê°€ (Cell 22)
```python
print(f"   ğŸ’¡ ì ì ˆí•œ ê°„ê²©: reward model ìŠ¤ì½”ì–´ë§ ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¦„")
```

---

## ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ì„¤ì • íŒŒì¼ ìˆ˜ì • (í•„ìˆ˜)
- [ ] **ë°±ì—…**: `config.json` ì›ë³¸ ì €ì¥
- [ ] `config.json` ìˆ˜ì •:
  - [ ] `checkpoint_interval`: 20 â†’ 100
  - [ ] `training` ì„¹ì…˜ ì¬êµ¬ì¡°í™”
  - [ ] `sft_batch_size`: 12 ì¶”ê°€
  - [ ] `dpo_batch_size`: 8 ì¶”ê°€
  - [ ] `gradient_accumulation_steps`: 2 ì¶”ê°€
  - [ ] `lora` ì„¹ì…˜ ì‚­ì œ
- [ ] **í…ŒìŠ¤íŠ¸**: `01_setup.ipynb` ì‹¤í–‰í•˜ì—¬ config ë¡œë“œ í™•ì¸

### Phase 2: í•™ìŠµ ë…¸íŠ¸ë¶ ìˆ˜ì • (ë†’ì€ ìš°ì„ ìˆœìœ„)
- [ ] `05_sft_training.ipynb` ìˆ˜ì •:
  - [ ] í—¤ë” ì—…ë°ì´íŠ¸
  - [ ] A100 ì„¤ëª… ì…€ ì¶”ê°€
  - [ ] `save_steps` ë³€ê²½
  - [ ] Print ë¬¸ ì—…ë°ì´íŠ¸
- [ ] **í…ŒìŠ¤íŠ¸**: 1 epoch ì‹¤í–‰í•˜ì—¬ batch=12 ë™ì‘ í™•ì¸

- [ ] `06_dpo_training.ipynb` ìˆ˜ì •:
  - [ ] í—¤ë” ì—…ë°ì´íŠ¸
  - [ ] A100 ì„¤ëª… ì…€ ì¶”ê°€
  - [ ] Gradient accumulation ìˆ˜ì •
  - [ ] `save_steps` ë³€ê²½
  - [ ] Print ë¬¸ ì—…ë°ì´íŠ¸
- [ ] **í…ŒìŠ¤íŠ¸**: 100 steps ì‹¤í–‰í•˜ì—¬ batch=8 ë™ì‘ í™•ì¸

### Phase 3: ë°ì´í„° ìƒì„± ë…¸íŠ¸ë¶ ìˆ˜ì • (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)
- [ ] `02_magpie_generation.ipynb` ìˆ˜ì •
- [ ] `04_preference_generation.ipynb` ìˆ˜ì •

### Phase 4: ì „ì²´ ê²€ì¦
- [ ] 10% ì„œë¸Œì…‹(150 ìƒ˜í”Œ)ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- [ ] GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (DPO: ~30-35GB ì˜ˆìƒ)
- [ ] ì „ì²´ 1,500 ìƒ˜í”Œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

---

## ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| ë‹¨ê³„ | T4 ì‹œê°„ | A100 ì‹œê°„ | ë°°ì† |
|------|---------|-----------|------|
| ë°ì´í„° ìƒì„± | 16-17h | 6-8h | 2-2.5x |
| SFT í•™ìŠµ | 6-10h | 2-4h | 2.5-3x |
| DPO í•™ìŠµ | 4-6h | 1-2h | 3-4x |
| **ì „ì²´** | **33-43h** | **13-20h** | **2.5-3x** |

---

## ìœ„í—˜ ê´€ë¦¬

### OOM ë°œìƒ ì‹œ ëŒ€ì‘
**ì¦ìƒ**: CUDA out of memory ì—ëŸ¬

**í•´ê²°ì±…**:
1. **SFT OOM**: `sft_batch_size`ë¥¼ 10 ë˜ëŠ” 8ë¡œ ê°ì†Œ
2. **DPO OOM**: `dpo_batch_size`ë¥¼ 6 ë˜ëŠ” 4ë¡œ ê°ì†Œ
3. `gradient_accumulation_steps`ë¥¼ 4ë¡œ ì¦ê°€

**ë³´ìˆ˜ì  ì„¤ì •ê°’**:
```json
"sft_batch_size": 8,
"dpo_batch_size": 4,
"gradient_accumulation_steps": 4
```

---

## ì£¼ìš” íŒŒì¼ ê²½ë¡œ

1. `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\config.json` âš ï¸
2. `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\05_sft_training.ipynb`
3. `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\06_dpo_training.ipynb`
4. `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\02_magpie_generation.ipynb`
5. `D:\Study\Github\TSP\LLM\Synthetic-Instruction-Tuner\notebooks\04_preference_generation.ipynb`

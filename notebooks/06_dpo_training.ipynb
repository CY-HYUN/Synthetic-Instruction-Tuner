{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 06. Direct Preference Optimization (DPO)\n## Synthetic Instruction Tuner - Week 3 Day 4-5\n\nThis notebook performs DPO training on the SFT model:\n1. Load SFT-trained model\n2. Load preference data (chosen/rejected pairs)\n3. Configure DPO training\n4. Train with TRL's DPOTrainer\n5. Evaluate and save the final model\n\n**DPO settings** (Colab Pro A100 optimized):\n- Beta: 0.1 (KL penalty coefficient)\n- Epochs: 1\n- Learning rate: 5e-5\n- Batch size: 8 (A100 40GB VRAM utilization, 4x increase from T4)\n- Gradient accumulation: 2\n\n**Expected runtime**:\n- T4: 4-6 hours\n- A100: 1-2 hours (3-4x faster)\n\n**A100 optimization benefits**:\n- DPO loads both policy and reference models simultaneously (highly memory-intensive)\n- A100's 40GB VRAM enables 4x larger batches (2â†’8)\n- Expected memory usage: ~30-35GB / 40GB\n- Significantly faster training with larger batches\n\n**Note**: DPO directly optimizes for human preferences without needing a separate reward model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Project path\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install libraries with latest compatible versions (avoid dependency conflicts)\n!pip install -q --upgrade transformers>=4.41.0 peft>=0.7.0 trl>=0.7.4 datasets>=2.16.0 accelerate>=0.25.0 bitsandbytes>=0.41.3\n\nprint(\"âœ… Libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom datasets import Dataset\nimport gc\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "source": "# Efficiency Metrics Tracker for DPO\nclass EfficiencyTracker:\n    \"\"\"Track efficiency metrics for adaptation method comparison.\"\"\"\n    \n    def __init__(self, method_name: str):\n        self.method_name = method_name\n        self.metrics = {\n            \"method\": method_name,\n            \"memory_allocated_gb\": [],\n            \"memory_reserved_gb\": [],\n            \"training_time_seconds\": 0,\n            \"trainable_params\": 0,\n            \"total_params\": 0,\n            \"trainable_ratio\": 0,\n            \"inference_tokens_per_sec\": 0,\n        }\n        self.start_time = None\n    \n    def log_memory(self):\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated() / 1e9\n            reserved = torch.cuda.memory_reserved() / 1e9\n            self.metrics[\"memory_allocated_gb\"].append(allocated)\n            self.metrics[\"memory_reserved_gb\"].append(reserved)\n            return {\"allocated\": allocated, \"reserved\": reserved}\n        return None\n    \n    def start_training(self):\n        self.start_time = time.time()\n        self.log_memory()\n    \n    def end_training(self):\n        if self.start_time:\n            self.metrics[\"training_time_seconds\"] = time.time() - self.start_time\n        self.log_memory()\n    \n    def log_params(self, model):\n        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in model.parameters())\n        self.metrics[\"trainable_params\"] = trainable\n        self.metrics[\"total_params\"] = total\n        self.metrics[\"trainable_ratio\"] = trainable / total if total > 0 else 0\n    \n    def log_inference_speed(self, tokens_generated: int, time_taken: float):\n        self.metrics[\"inference_tokens_per_sec\"] = tokens_generated / time_taken if time_taken > 0 else 0\n    \n    def get_summary(self):\n        return {\n            \"method\": self.method_name,\n            \"trainable_params\": self.metrics[\"trainable_params\"],\n            \"total_params\": self.metrics[\"total_params\"],\n            \"trainable_ratio_percent\": self.metrics[\"trainable_ratio\"] * 100,\n            \"peak_memory_gb\": max(self.metrics[\"memory_allocated_gb\"]) if self.metrics[\"memory_allocated_gb\"] else 0,\n            \"training_time_hours\": self.metrics[\"training_time_seconds\"] / 3600,\n            \"inference_tokens_per_sec\": self.metrics[\"inference_tokens_per_sec\"],\n        }\n    \n    def save(self, path: str):\n        with open(path, 'w') as f:\n            json.dump(self.get_summary(), f, indent=2)\n        print(f\"Metrics saved to {path}\")\n\n# Initialize tracker for DPO\ntracker = EfficiencyTracker(\"dpo\")\nprint(\"Efficiency tracker initialized for DPO!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPO training data\n",
    "TRAIN_PATH = f\"{config['paths']['data_preference']}/dpo_train.json\"\n",
    "VAL_PATH = f\"{config['paths']['data_preference']}/dpo_val.json\"\n",
    "\n",
    "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(VAL_PATH, 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"Training pairs: {len(train_data)}\")\n",
    "print(f\"Validation pairs: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data format\n",
    "print(\"Sample preference pair:\")\n",
    "print(json.dumps(train_data[0], indent=2, ensure_ascii=False)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Val dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and SFT adapters\n",
    "BASE_MODEL_ID = config['models']['sft_base']\n",
    "SFT_MODEL_PATH = f\"{config['paths']['models_sft']}/final\"\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL_ID}\")\n",
    "print(f\"Loading SFT adapters from: {SFT_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"left\"  # Important for DPO\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load SFT model (with LoRA adapters)\nmodel = PeftModel.from_pretrained(\n    base_model,\n    SFT_MODEL_PATH,\n    is_trainable=True,  # Make adapters trainable for DPO\n)\n\n# Prepare for training\nmodel.config.use_cache = False\n\n# Log parameters and memory\ntracker.log_params(model)\nmem = tracker.log_memory()\n\nprint(f\"SFT model loaded!\")\nprint(f\"Trainable parameters: {tracker.metrics['trainable_params']:,} ({tracker.metrics['trainable_ratio']*100:.2f}%)\")\nprint(f\"GPU Memory: {mem['allocated']:.2f} GB allocated\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference model (for KL divergence)\n",
    "# Use the same SFT model as reference (frozen)\n",
    "ref_model = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    ),\n",
    "    SFT_MODEL_PATH,\n",
    "    is_trainable=False,  # Frozen reference model\n",
    ")\n",
    "\n",
    "print(f\"Reference model loaded!\")\n",
    "print(f\"Total GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure DPO Training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## A100 GPU Optimization Settings for DPO\n\nThis notebook is optimized for **Google Colab Pro with A100 GPU (40GB VRAM)**.\n\n### DPO-specific memory considerations:\n\n**Dual model loading**: DPO requires loading both:\n- Policy model (trainable) - the model being optimized\n- Reference model (frozen) - for KL divergence computation\n\nThis doubles memory requirements compared to SFT training.\n\n**Batch size: 8** (increased from T4's 2, 4x larger)\n- Even with dual models, A100's 40GB VRAM handles batch size 8\n- Memory usage: ~30-35GB / 40GB (safe operating range)\n- T4 with 15GB can only handle batch size 2\n\n**Gradient accumulation: 2** (unified value from config)\n- Effective batch size: 8 Ã— 2 = 16\n- Consistent with SFT training approach\n\n**Training speedup: 3-4x faster than T4**\n- Larger batches significantly speed up DPO training\n- Expected training time: 1-2 hours (vs T4's 4-6 hours)\n\n**Memory safety**: If OOM errors occur, reduce batch size to 4 or 6",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"{config['paths']['models_dpo']}/dpo-checkpoint\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DPO training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    \n    # Training hyperparameters\n    num_train_epochs=config['training']['dpo_epochs'],\n    per_device_train_batch_size=config['training']['dpo_batch_size'],\n    per_device_eval_batch_size=config['training']['dpo_batch_size'],\n    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],  # A100: unified value of 2\n    \n    # Optimizer\n    learning_rate=config['training']['dpo_learning_rate'],\n    weight_decay=0.01,\n    optim=\"paged_adamw_32bit\",\n    \n    # Learning rate schedule\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    \n    # Logging\n    logging_dir=f\"{OUTPUT_DIR}/logs\",\n    logging_steps=10,\n    logging_first_step=True,\n    \n    # Evaluation\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    \n    # Checkpointing (A100 optimization: reduced frequency)\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=3,\n    \n    # Performance\n    fp16=True,\n    gradient_checkpointing=True,\n    \n    # Misc\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    remove_unused_columns=False,\n)\n\nprint(\"Training arguments:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size} (T4: 2 â†’ A100: 8, 4x increase)\")\nprint(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"\\nðŸ’¡ A100 40GB VRAM handles dual model loading (policy + reference) with larger batches\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    beta=config['training']['dpo_beta'],  # KL penalty coefficient\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "print(f\"DPO Trainer initialized!\")\n",
    "print(f\"  Beta (KL penalty): {config['training']['dpo_beta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start DPO training with timing\nprint(\"Starting DPO training...\")\nprint(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 50)\n\ntracker.start_training()\ntrain_result = dpo_trainer.train()\ntracker.end_training()\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"DPO training completed!\")\nprint(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Training time: {tracker.metrics['training_time_seconds']/3600:.2f} hours\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"\\nTraining metrics:\")\n",
    "print(f\"  Final train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating DPO model...\")\n",
    "eval_results = dpo_trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_response(instruction: str, max_new_tokens: int = 256):\n    \"\"\"Generate a response for the given instruction with timing.\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        start_time = time.time()\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        gen_time = time.time() - start_time\n    \n    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    \n    # Extract response\n    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated:\n        response = generated.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n        response = response.split(\"<|eot_id|>\")[0].strip()\n        return response, tokens_generated, gen_time\n    \n    return generated, tokens_generated, gen_time\n\nprint(\"Generation function defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test generation and measure inference speed\ntest_instructions = [\n    \"Explain the concept of machine learning in simple terms.\",\n    \"Write a Python function to calculate the factorial of a number.\",\n    \"What are the main differences between supervised and unsupervised learning?\",\n    \"Describe the process of photosynthesis.\",\n    \"How can I improve my coding skills?\",\n]\n\nprint(\"Testing DPO fine-tuned model generation:\")\nprint(\"=\" * 50)\n\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, instruction in enumerate(test_instructions):\n    print(f\"\\n[Test {i+1}]\")\n    print(f\"Instruction: {instruction}\")\n    print(f\"\\nResponse:\")\n    response, tokens, gen_time = generate_response(instruction, max_new_tokens=200)\n    print(response)\n    print(f\"\\nTokens: {tokens}, Time: {gen_time:.2f}s, Speed: {tokens/gen_time:.1f} tok/s\")\n    print(\"-\" * 50)\n    \n    total_tokens += tokens\n    total_time += gen_time\n\n# Log inference speed\ntracker.log_inference_speed(total_tokens, total_time)\nprint(f\"\\nAverage inference speed: {tracker.metrics['inference_tokens_per_sec']:.1f} tokens/sec\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare SFT vs DPO Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SFT model for comparison\n",
    "print(\"Loading SFT model for comparison...\")\n",
    "\n",
    "sft_model = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    ),\n",
    "    SFT_MODEL_PATH,\n",
    ")\n",
    "\n",
    "print(\"SFT model loaded for comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison(instruction: str, max_new_tokens: int = 200):\n",
    "    \"\"\"Generate responses from both SFT and DPO models.\"\"\"\n",
    "    prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with SFT model\n",
    "    with torch.no_grad():\n",
    "        sft_outputs = sft_model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens, temperature=0.7,\n",
    "            do_sample=True, top_p=0.9, pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    sft_text = tokenizer.decode(sft_outputs[0], skip_special_tokens=False)\n",
    "    sft_response = sft_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].split(\"<|eot_id|>\")[0].strip()\n",
    "    \n",
    "    # Generate with DPO model\n",
    "    with torch.no_grad():\n",
    "        dpo_outputs = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens, temperature=0.7,\n",
    "            do_sample=True, top_p=0.9, pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    dpo_text = tokenizer.decode(dpo_outputs[0], skip_special_tokens=False)\n",
    "    dpo_response = dpo_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].split(\"<|eot_id|>\")[0].strip()\n",
    "    \n",
    "    return sft_response, dpo_response\n",
    "\n",
    "# Compare on test instructions\n",
    "print(\"\\nComparing SFT vs DPO outputs:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, instruction in enumerate(test_instructions[:3]):\n",
    "    print(f\"\\n[Comparison {i+1}]\")\n",
    "    print(f\"Instruction: {instruction}\\n\")\n",
    "    \n",
    "    sft_resp, dpo_resp = generate_comparison(instruction)\n",
    "    \n",
    "    print(f\"SFT Response:\")\n",
    "    print(sft_resp[:300] + \"...\" if len(sft_resp) > 300 else sft_resp)\n",
    "    print(f\"\\nDPO Response:\")\n",
    "    print(dpo_resp[:300] + \"...\" if len(dpo_resp) > 300 else dpo_resp)\n",
    "    print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Final DPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DPO model\n",
    "FINAL_MODEL_DIR = f\"{config['paths']['models_dpo']}/final\"\n",
    "\n",
    "print(f\"Saving final DPO model to: {FINAL_MODEL_DIR}\")\n",
    "\n",
    "dpo_trainer.save_model(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"base_model\": BASE_MODEL_ID,\n",
    "    \"sft_model\": SFT_MODEL_PATH,\n",
    "    \"training_data_size\": len(train_data),\n",
    "    \"validation_data_size\": len(val_data),\n",
    "    \"dpo_config\": {\n",
    "        \"beta\": config['training']['dpo_beta'],\n",
    "        \"epochs\": config['training']['dpo_epochs'],\n",
    "        \"learning_rate\": config['training']['dpo_learning_rate'],\n",
    "        \"batch_size\": config['training']['dpo_batch_size'],\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"train_loss\": train_result.training_loss,\n",
    "        \"eval_loss\": eval_results.get(\"eval_loss\", None),\n",
    "        \"total_steps\": train_result.global_step,\n",
    "    },\n",
    "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "\n",
    "config_path = f\"{FINAL_MODEL_DIR}/training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"Training config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training logs\n",
    "log_history = dpo_trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_logs = [log for log in log_history if 'loss' in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "if train_logs:\n",
    "    steps = [log['step'] for log in train_logs]\n",
    "    losses = [log['loss'] for log in train_logs]\n",
    "    axes[0].plot(steps, losses, label='Train Loss')\n",
    "    axes[0].set_xlabel('Steps')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('DPO Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Evaluation loss\n",
    "if eval_logs:\n",
    "    steps = [log['step'] for log in eval_logs]\n",
    "    losses = [log['eval_loss'] for log in eval_logs]\n",
    "    axes[1].plot(steps, losses, label='Eval Loss', color='orange')\n",
    "    axes[1].set_xlabel('Steps')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('DPO Evaluation Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['paths']['evaluation_figures']}/dpo_training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to {config['paths']['evaluation_figures']}/dpo_training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Save DPO efficiency metrics for comparison\nMETRICS_DIR = f\"{PROJECT_ROOT}/evaluation/metrics\"\nos.makedirs(METRICS_DIR, exist_ok=True)\n\n# Get summary\nsummary = tracker.get_summary()\n\n# Add training results\nsummary[\"train_loss\"] = train_result.training_loss\nsummary[\"eval_loss\"] = eval_results.get(\"eval_loss\", None)\n\n# Save\ntracker.save(f\"{METRICS_DIR}/dpo_metrics.json\")\n\nprint(\"\\n=== DPO Efficiency Metrics Summary ===\")\nfor key, value in summary.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.4f}\")\n    else:\n        print(f\"  {key}: {value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del ref_model\n",
    "del sft_model\n",
    "del dpo_trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ“ DPO Training Complete!\n",
    "\n",
    "### Summary:\n",
    "- DPO-aligned model saved to `models/dpo/final/`\n",
    "- Model trained to prefer high-quality responses based on reward model scores\n",
    "- Training configuration and metrics saved\n",
    "\n",
    "### Model Loading:\n",
    "To load the final DPO model:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID)\n",
    "model = PeftModel.from_pretrained(base_model, FINAL_MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Week 4: Proceed to evaluation notebooks\n",
    "2. `07_benchmark_evaluation.ipynb` - Evaluate on standard benchmarks (IFEval, MT-Bench)\n",
    "3. `08_agent_evaluation.ipynb` - Test agent capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
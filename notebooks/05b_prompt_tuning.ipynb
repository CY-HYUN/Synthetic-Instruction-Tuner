{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o7Tx5c8UfyC"
      },
      "source": [
        "# 05b. Prompt Tuning\n",
        "## Synthetic Instruction Tuner - Alternative Adaptation Method\n",
        "\n",
        "This notebook implements **Prompt Tuning** as an alternative adaptation method for comparison:\n",
        "1. Load base model (same as SFT)\n",
        "2. Initialize soft prompt embeddings\n",
        "3. Train only the prompt parameters\n",
        "4. Collect efficiency metrics for comparison\n",
        "5. Evaluate and save the model\n",
        "\n",
        "**Comparison with LoRA**:\n",
        "| Method | Trainable Params | Memory | Training Time |\n",
        "|--------|-----------------|--------|---------------|\n",
        "| LoRA (r=8) | ~0.67% | Medium | 2-4 hours (A100) |\n",
        "| Prompt Tuning | ~0.01% | Low | 1-2 hours (A100) |\n",
        "\n",
        "**Training settings (A100 GPU optimized)**:\n",
        "- Virtual tokens: 20\n",
        "- Batch size: 12 (A100 40GB VRAM)\n",
        "- Gradient accumulation: 2 (Effective batch size: 24)\n",
        "- **BF16 enabled** for optimal A100 performance\n",
        "\n",
        "**Expected runtime on A100**: 1-2 hours\n",
        "**Cost**: ~5-10 compute units\n",
        "\n",
        "**⚠️ IMPORTANT: Before running this notebook**:\n",
        "1. Runtime → Change runtime type → GPU type: **A100**\n",
        "2. Runtime → Restart runtime\n",
        "3. Run `05_sft_training.ipynb` first for comparison metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJXXWxHUfyI"
      },
      "source": [
        "## 1. Setup & Efficiency Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irYW7kUKUfyK",
        "outputId": "965149b9-be80-4829-93cd-4c1ec977b8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Project path\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjlmJQaQUfyM",
        "outputId": "7957cdca-2e19-4a35-99e7-b1597d84d42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "import json\n",
        "\n",
        "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(\"Configuration loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-_eLgb0UfyN",
        "outputId": "5eae883c-56f5-45b8-e3b0-45be03093e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ A100 GPU settings applied!\n",
            "  Batch size: 12\n",
            "  Gradient accumulation: 2\n",
            "  Effective batch size: 24\n"
          ]
        }
      ],
      "source": [
        "# Configure for A100 GPU (use same settings as SFT for fair comparison)\n",
        "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# A100 optimization settings (same as SFT)\n",
        "config['training']['prompt_tuning_batch_size'] = 12\n",
        "config['training']['prompt_tuning_gradient_accumulation'] = 2\n",
        "\n",
        "# Save updated config\n",
        "with open(f\"{PROJECT_ROOT}/config.json\", 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"✅ A100 GPU settings applied!\")\n",
        "print(f\"  Batch size: {config['training']['prompt_tuning_batch_size']}\")\n",
        "print(f\"  Gradient accumulation: {config['training']['prompt_tuning_gradient_accumulation']}\")\n",
        "print(f\"  Effective batch size: {config['training']['prompt_tuning_batch_size'] * config['training']['prompt_tuning_gradient_accumulation']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3XNgJN8UfyO",
        "outputId": "f2b6d897-81c0-445c-8d4e-c17233c52f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install -q --upgrade transformers>=4.41.0 peft>=0.7.0 trl>=0.7.4 datasets>=2.16.0 accelerate>=0.25.0 bitsandbytes>=0.41.3\n",
        "\n",
        "print(\"✅ Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QenkzUMJUfyP",
        "outputId": "ca939c42-7923-4bc5-cb41-05416855c06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 85.17 GB\n",
            "\n",
            "✅ A100 GPU detected! Ready for training.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "import gc\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "    # Verify A100\n",
        "    if \"A100\" not in gpu_name:\n",
        "        print(\"\\n⚠️ WARNING: This notebook is optimized for A100 GPU!\")\n",
        "        print(f\"   Current GPU: {gpu_name}\")\n",
        "        print(\"   Please change runtime to A100:\")\n",
        "        print(\"   Runtime → Change runtime type → GPU type: A100\")\n",
        "    else:\n",
        "        print(\"\\n✅ A100 GPU detected! Ready for training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofQCGD8xUfyQ",
        "outputId": "42f315cd-8427-4b6b-e376-61a9e8eb586c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "A100 GPU Performance Settings:\n",
            "  BF16 enabled: True\n",
            "  TF32 enabled: True\n",
            "  Optimal for A100 40GB VRAM\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# A100 GPU optimization settings\n",
        "import torch\n",
        "\n",
        "# Enable BF16 for A100 (supported natively)\n",
        "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"A100 GPU Performance Settings:\")\n",
        "print(f\"  BF16 enabled: {torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction}\")\n",
        "print(f\"  TF32 enabled: {torch.backends.cudnn.allow_tf32}\")\n",
        "print(f\"  Optimal for A100 40GB VRAM\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9aH3eYWUfyR",
        "outputId": "0d3fd09c-f36e-4a13-8730-7d99486c9637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Efficiency tracker initialized!\n"
          ]
        }
      ],
      "source": [
        "# Efficiency Metrics Tracker\n",
        "class EfficiencyTracker:\n",
        "    \"\"\"Track efficiency metrics for adaptation method comparison.\"\"\"\n",
        "\n",
        "    def __init__(self, method_name: str):\n",
        "        self.method_name = method_name\n",
        "        self.metrics = {\n",
        "            \"method\": method_name,\n",
        "            \"memory_allocated_gb\": [],\n",
        "            \"memory_reserved_gb\": [],\n",
        "            \"training_time_seconds\": 0,\n",
        "            \"trainable_params\": 0,\n",
        "            \"total_params\": 0,\n",
        "            \"trainable_ratio\": 0,\n",
        "            \"inference_tokens_per_sec\": 0,\n",
        "        }\n",
        "        self.start_time = None\n",
        "\n",
        "    def log_memory(self):\n",
        "        \"\"\"Log current GPU memory usage.\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            self.metrics[\"memory_allocated_gb\"].append(allocated)\n",
        "            self.metrics[\"memory_reserved_gb\"].append(reserved)\n",
        "            return {\"allocated\": allocated, \"reserved\": reserved}\n",
        "        return None\n",
        "\n",
        "    def start_training(self):\n",
        "        \"\"\"Start timing training.\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.log_memory()\n",
        "\n",
        "    def end_training(self):\n",
        "        \"\"\"End timing training.\"\"\"\n",
        "        if self.start_time:\n",
        "            self.metrics[\"training_time_seconds\"] = time.time() - self.start_time\n",
        "        self.log_memory()\n",
        "\n",
        "    def log_params(self, model):\n",
        "        \"\"\"Log parameter counts.\"\"\"\n",
        "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total = sum(p.numel() for p in model.parameters())\n",
        "        self.metrics[\"trainable_params\"] = trainable\n",
        "        self.metrics[\"total_params\"] = total\n",
        "        self.metrics[\"trainable_ratio\"] = trainable / total if total > 0 else 0\n",
        "\n",
        "    def log_inference_speed(self, tokens_generated: int, time_taken: float):\n",
        "        \"\"\"Log inference speed.\"\"\"\n",
        "        self.metrics[\"inference_tokens_per_sec\"] = tokens_generated / time_taken if time_taken > 0 else 0\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Get summary metrics.\"\"\"\n",
        "        summary = {\n",
        "            \"method\": self.method_name,\n",
        "            \"trainable_params\": self.metrics[\"trainable_params\"],\n",
        "            \"total_params\": self.metrics[\"total_params\"],\n",
        "            \"trainable_ratio_percent\": self.metrics[\"trainable_ratio\"] * 100,\n",
        "            \"peak_memory_gb\": max(self.metrics[\"memory_allocated_gb\"]) if self.metrics[\"memory_allocated_gb\"] else 0,\n",
        "            \"training_time_hours\": self.metrics[\"training_time_seconds\"] / 3600,\n",
        "            \"inference_tokens_per_sec\": self.metrics[\"inference_tokens_per_sec\"],\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save metrics to JSON.\"\"\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.get_summary(), f, indent=2)\n",
        "        print(f\"Metrics saved to {path}\")\n",
        "\n",
        "# Initialize tracker\n",
        "tracker = EfficiencyTracker(\"prompt_tuning\")\n",
        "print(\"Efficiency tracker initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWUtFQaZUfyR"
      },
      "source": [
        "## 2. Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7_QGL2UfyS",
        "outputId": "7efa34c4-90c3-4df8-ae6b-68194baeaeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 900\n",
            "Validation samples: 100\n"
          ]
        }
      ],
      "source": [
        "# Load SFT training data (same as LoRA for fair comparison)\n",
        "TRAIN_PATH = f\"{config['paths']['data_filtered']}/sft_train.json\"\n",
        "VAL_PATH = f\"{config['paths']['data_filtered']}/sft_val.json\"\n",
        "\n",
        "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(VAL_PATH, 'r', encoding='utf-8') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr_TRY2jUfyS",
        "outputId": "db0e643c-3cce-4fbc-cf16-6470a6470b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: Dataset({\n",
            "    features: ['instruction', 'input', 'output'],\n",
            "    num_rows: 900\n",
            "})\n",
            "Val dataset: Dataset({\n",
            "    features: ['instruction', 'input', 'output'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "print(f\"Val dataset: {val_dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV531MbtUfyT"
      },
      "source": [
        "## 3. Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIZxYklzUfyT",
        "outputId": "2e2ee8ad-6453-445a-f0a1-c2a4ec2287ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: meta-llama/Llama-3.2-3B\n",
            "Output directory: /content/drive/MyDrive/synthetic-instruction-tuner/models/prompt_tuning/checkpoint\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "# Use same base model as SFT for fair comparison\n",
        "BASE_MODEL_ID = config['models']['sft_base']\n",
        "OUTPUT_DIR = f\"{config['paths']['models_prompt_tuning']}/checkpoint\"\n",
        "\n",
        "print(f\"Loading base model: {BASE_MODEL_ID}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi0jQNgSUfyT",
        "outputId": "93fc4494-a5c2-4c02-81b3-a4b212918207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Quantization config created (BF16 for A100)\n"
          ]
        }
      ],
      "source": [
        "# 4-bit quantization (A100 with BF16 - same as SFT for fair comparison)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # BF16 for A100\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"✅ Quantization config created (BF16 for A100)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkL99RO6UfyT",
        "outputId": "17fc30ce-2c32-46f0-9026-7970aadeab93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded. Vocab size: 128000\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "e96b977c54284cbca0caa3f562b9f0ad",
            "f2fa8cda81a24072b4f663e4040363df",
            "0be7a04f52994b6a8c097fc7b5b3e3fb",
            "37aa0ead545e46d38a58a7966c323fa0",
            "dc5d99e9b2504fc48f2ee9a9695c9d26",
            "3751352378924696ae443235cf4585f9",
            "35d1114f6a0f4ede999b290bb2b5b988",
            "c05275be87144e45a0252f2873325e6d",
            "71f22e18e32841f38441de3f875aa16f",
            "efe0cc411316476aa983385d195210d1",
            "9779dca9c1d04b5bb36fb184e3f63cd7"
          ]
        },
        "id": "r_q8qp8pUfyU",
        "outputId": "f689db93-148f-499c-9c5a-c487d51a635f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e96b977c54284cbca0caa3f562b9f0ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded!\n",
            "GPU Memory: 4.49 GB allocated, 6.98 GB reserved\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Log initial memory\n",
        "mem = tracker.log_memory()\n",
        "print(f\"Model loaded!\")\n",
        "print(f\"GPU Memory: {mem['allocated']:.2f} GB allocated, {mem['reserved']:.2f} GB reserved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw15gyhMUfyU"
      },
      "source": [
        "## 4. Configure Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYpIoi0FUfyU",
        "outputId": "967e7e09-530e-4bbb-988c-74015c55067a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Tuning config:\n",
            "  num_virtual_tokens: 20\n",
            "  init_method: PromptTuningInit.RANDOM\n"
          ]
        }
      ],
      "source": [
        "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
        "\n",
        "# Prompt Tuning configuration\n",
        "# num_virtual_tokens: number of soft prompt tokens to prepend\n",
        "prompt_tuning_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    num_virtual_tokens=config['training']['prompt_tuning_num_virtual_tokens'],\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
        "    tokenizer_name_or_path=BASE_MODEL_ID,\n",
        ")\n",
        "\n",
        "print(\"Prompt Tuning config:\")\n",
        "print(f\"  num_virtual_tokens: {prompt_tuning_config.num_virtual_tokens}\")\n",
        "print(f\"  init_method: {prompt_tuning_config.prompt_tuning_init}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlNa6oD5UfyU",
        "outputId": "7194cd06-c13d-454b-ad4e-3a2033af8d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable parameters: 61,440\n",
            "Total parameters: 1,803,525,120\n",
            "Trainable ratio: 0.0034%\n",
            "\n",
            "--- Comparison ---\n",
            "Prompt Tuning trains ONLY the soft prompt embeddings\n",
            "This is typically 10-100x fewer parameters than LoRA!\n"
          ]
        }
      ],
      "source": [
        "# Apply Prompt Tuning to model\n",
        "model = get_peft_model(model, prompt_tuning_config)\n",
        "\n",
        "# Log parameters\n",
        "tracker.log_params(model)\n",
        "\n",
        "print(f\"\\nTrainable parameters: {tracker.metrics['trainable_params']:,}\")\n",
        "print(f\"Total parameters: {tracker.metrics['total_params']:,}\")\n",
        "print(f\"Trainable ratio: {tracker.metrics['trainable_ratio']*100:.4f}%\")\n",
        "\n",
        "# Compare with LoRA\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(\"Prompt Tuning trains ONLY the soft prompt embeddings\")\n",
        "print(\"This is typically 10-100x fewer parameters than LoRA!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glriWEq6UfyU",
        "outputId": "7c126bca-b9ac-4d4c-d886-3ce96f18b9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable layers:\n",
            "  prompt_encoder.default.embedding.weight: torch.Size([20, 3072])\n"
          ]
        }
      ],
      "source": [
        "# Visualize trainable parameters\n",
        "print(\"\\nTrainable layers:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"  {name}: {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPqB3-6XUfyU"
      },
      "source": [
        "## 5. Format Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "f81123af953643fd94e27ebe5c4fd940",
            "4d4d8248feb24b4686791d2b97e602c8",
            "d674da683a21462ea29fe5cae1546bdb",
            "c9bf499a60684d7a8ed43bf97285cc27",
            "17268b0f59104775a9a701530cdb11ff",
            "3596b1f444c245d9b39ca2790d6e99ba",
            "808116a2839545c1be2298fea11101f5",
            "a9919f8590334d67bbf73d2d4956e5ff",
            "0a5d993d29f444729a0efb0957a3aaea",
            "480711424b27416b92c26ec0243b905c",
            "d4b77f1d5ee84d30985cbfbc0651c63b",
            "a4aa21760b1f4d7c8ef2b806ff3ddf43",
            "5d424d58c509401ab4bf327508d917f8",
            "16aa0690203140a8af668a660032708a",
            "fdacbbd65dda4d35bdc066e2ac8df12e",
            "7824722fa62c476c845b377921ef398a",
            "e0d7942fe95b4f2495c11d50cac14a85",
            "23b9287b41ca4eab83994ff5398728e0",
            "1408e408d6d84e34a8317f1af0d34b41",
            "f1f7ea632aa243daaf6945c4150f6b94",
            "3a38840ba68347c0bce53f10d2d24a48",
            "24785e76179b4ba49ce7659fcfcd110f"
          ]
        },
        "id": "WoJkDp-GUfyV",
        "outputId": "45ef2459-21d7-4991-d924-5f9ad1a8d159"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f81123af953643fd94e27ebe5c4fd940"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing val dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4aa21760b1f4d7c8ef2b806ff3ddf43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized train dataset: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 900\n",
            "})\n",
            "Tokenized val dataset: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def format_and_tokenize(sample):\n",
        "    \"\"\"Format and tokenize instruction-response pair for training.\"\"\"\n",
        "    instruction = sample[\"instruction\"]\n",
        "    response = sample[\"output\"]\n",
        "\n",
        "    # Same format as LoRA for fair comparison\n",
        "    text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
        "\n",
        "    # Tokenize with padding to max_length\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=config['training']['sft_max_seq_length'],\n",
        "        padding=\"max_length\",  # 패딩 활성화\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # Add labels (same as input_ids for causal LM)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Apply formatting and tokenization\n",
        "train_dataset = train_dataset.map(\n",
        "    format_and_tokenize,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing train dataset\"\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    format_and_tokenize,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing val dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenized train dataset: {train_dataset}\")\n",
        "print(f\"Tokenized val dataset: {val_dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_NPjZSmUfyV"
      },
      "source": [
        "## 6. Configure Training (A100 Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8PglgQ5UfyV",
        "outputId": "dc6b9137-7d85-4195-b2d7-c709686a1bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments configured (A100 optimized):\n",
            "  Epochs: 3\n",
            "  Batch size: 12\n",
            "  Gradient accumulation: 2\n",
            "  Effective batch size: 24\n",
            "  Learning rate: 0.0003\n",
            "  BF16: True, TF32: True\n"
          ]
        }
      ],
      "source": [
        "# Training arguments (A100 optimized, same hyperparameters as LoRA for fair comparison)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Training hyperparameters (same as LoRA)\n",
        "    num_train_epochs=config['training']['prompt_tuning_epochs'],\n",
        "    per_device_train_batch_size=config['training']['prompt_tuning_batch_size'],\n",
        "    per_device_eval_batch_size=config['training']['prompt_tuning_batch_size'],\n",
        "    gradient_accumulation_steps=config['training']['prompt_tuning_gradient_accumulation'],\n",
        "\n",
        "    # Optimizer (slightly higher LR often works better for prompt tuning)\n",
        "    learning_rate=config['training']['prompt_tuning_learning_rate'],\n",
        "    weight_decay=0.01,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "\n",
        "    # Learning rate schedule\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=config['training']['prompt_tuning_warmup_ratio'],\n",
        "\n",
        "    # Logging\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "\n",
        "    # Checkpointing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # Performance - A100 GPU settings\n",
        "    fp16=False,\n",
        "    bf16=True,  # ENABLED: A100 supports BF16 natively\n",
        "    tf32=True,   # ENABLED: TensorFloat-32 for better performance\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # Misc\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured (A100 optimized):\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  BF16: {training_args.bf16}, TF32: {training_args.tf32}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUsQY25GUfyV"
      },
      "source": [
        "## 7. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xkIlQFsUfyV",
        "outputId": "999e99ef-4ec1-44b1-d68d-9f8b01785a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer initialized!\n",
            "Note: Using standard Trainer (not SFTTrainer) for Prompt Tuning compatibility\n"
          ]
        }
      ],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "# Use DefaultDataCollator with padding\n",
        "# DataCollatorForLanguageModeling doesn't handle variable lengths well\n",
        "data_collator = DefaultDataCollator(return_tensors=\"pt\")\n",
        "\n",
        "# Use standard Trainer (not SFTTrainer) for Prompt Tuning\n",
        "# SFTTrainer tries to call merge_and_unload() which Prompt Tuning doesn't support\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized!\")\n",
        "print(\"Note: Using standard Trainer (not SFTTrainer) for Prompt Tuning compatibility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgPSOeclUfyV"
      },
      "source": [
        "## 8. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "pEjHN8kPUfyW",
        "outputId": "bf473b51-6705-455b-edcd-793c06b90b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Prompt Tuning training...\n",
            "Start time: 2025-12-26 15:54:46\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/114 18:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.052100</td>\n",
              "      <td>2.979476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Training completed!\n",
            "End time: 2025-12-26 16:13:34\n",
            "Training time: 0.31 hours\n"
          ]
        }
      ],
      "source": [
        "# Start training with timing\n",
        "print(\"Starting Prompt Tuning training...\")\n",
        "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "tracker.start_training()\n",
        "train_result = trainer.train()\n",
        "tracker.end_training()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(f\"Training completed!\")\n",
        "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Training time: {tracker.metrics['training_time_seconds']/3600:.2f} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C98ArT46UfyW",
        "outputId": "b5144d7c-348d-4174-ab78-75197cff97dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training metrics:\n",
            "  Final train loss: 5.2226\n",
            "  Total steps: 114\n"
          ]
        }
      ],
      "source": [
        "# Print training metrics\n",
        "print(\"\\nTraining metrics:\")\n",
        "print(f\"  Final train loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Total steps: {train_result.global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmRxO0AVUfyW"
      },
      "source": [
        "## 9. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "g3ke7_HvUfyW",
        "outputId": "bc9b3e59-4d9c-498f-acaf-19fed62e9034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation metrics:\n",
            "  eval_loss: 2.9795\n",
            "  eval_runtime: 11.8096\n",
            "  eval_samples_per_second: 8.4680\n",
            "  eval_steps_per_second: 0.7620\n",
            "  epoch: 3.0000\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set\n",
        "print(\"Evaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation metrics:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"  {key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cksp2dUuUfyW"
      },
      "source": [
        "## 10. Test Generation & Measure Inference Speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuH5AKJ1UfyX",
        "outputId": "924a7451-aa61-4d30-c5e1-9e0af0623bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation function defined!\n"
          ]
        }
      ],
      "source": [
        "def generate_response(instruction: str, max_new_tokens: int = 256):\n",
        "    \"\"\"Generate a response for the given instruction.\"\"\"\n",
        "    prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        gen_time = time.time() - start_time\n",
        "\n",
        "    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n",
        "\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # Extract response\n",
        "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated:\n",
        "        response = generated.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
        "        response = response.split(\"<|eot_id|>\")[0].strip()\n",
        "        return response, tokens_generated, gen_time\n",
        "\n",
        "    return generated, tokens_generated, gen_time\n",
        "\n",
        "print(\"Generation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ic_-5SkUfyX",
        "outputId": "004e1462-bae7-4cdf-eb2d-448d200cf097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Prompt Tuning model generation:\n",
            "==================================================\n",
            "\n",
            "[Test 1]\n",
            "Instruction: Explain the concept of machine learning in simple terms.\n",
            "\n",
            "Response:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:2141: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
            "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|end_of_text|>\n",
            "\n",
            "Tokens: 1, Time: 0.77s, Speed: 1.3 tok/s\n",
            "--------------------------------------------------\n",
            "\n",
            "[Test 2]\n",
            "Instruction: Write a Python function to calculate the factorial of a number.\n",
            "\n",
            "Response:\n",
            "!<|end_of_text|>\n",
            "\n",
            "Tokens: 2, Time: 0.21s, Speed: 9.7 tok/s\n",
            "--------------------------------------------------\n",
            "\n",
            "[Test 3]\n",
            "Instruction: What are the main differences between supervised and unsupervised learning?\n",
            "\n",
            "Response:\n",
            "What are the differences between a supervised and unsupervised learning?<|end_of_text|>\n",
            "\n",
            "Tokens: 14, Time: 1.03s, Speed: 13.6 tok/s\n",
            "--------------------------------------------------\n",
            "\n",
            "Average inference speed: 8.4 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "# Test generation and measure inference speed\n",
        "test_instructions = [\n",
        "    \"Explain the concept of machine learning in simple terms.\",\n",
        "    \"Write a Python function to calculate the factorial of a number.\",\n",
        "    \"What are the main differences between supervised and unsupervised learning?\",\n",
        "]\n",
        "\n",
        "print(\"Testing Prompt Tuning model generation:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "total_tokens = 0\n",
        "total_time = 0\n",
        "\n",
        "for i, instruction in enumerate(test_instructions):\n",
        "    print(f\"\\n[Test {i+1}]\")\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "    print(f\"\\nResponse:\")\n",
        "    response, tokens, gen_time = generate_response(instruction, max_new_tokens=200)\n",
        "    print(response)\n",
        "    print(f\"\\nTokens: {tokens}, Time: {gen_time:.2f}s, Speed: {tokens/gen_time:.1f} tok/s\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    total_tokens += tokens\n",
        "    total_time += gen_time\n",
        "\n",
        "# Log inference speed\n",
        "tracker.log_inference_speed(total_tokens, total_time)\n",
        "print(f\"\\nAverage inference speed: {tracker.metrics['inference_tokens_per_sec']:.1f} tokens/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLn2BYz9UfyX"
      },
      "source": [
        "## 11. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nmrzEqeUfyX",
        "outputId": "80b6cf39-40e2-4a5a-dad6-d59e7418cb70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final model to: /content/drive/MyDrive/synthetic-instruction-tuner/models/prompt_tuning/final\n",
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save the final model\n",
        "FINAL_MODEL_DIR = f\"{config['paths']['models_prompt_tuning']}/final\"\n",
        "\n",
        "print(f\"Saving final model to: {FINAL_MODEL_DIR}\")\n",
        "\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr3H5TgtUfyX",
        "outputId": "656deb80-354c-45a1-dde1-2ff101ca2509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training config saved to: /content/drive/MyDrive/synthetic-instruction-tuner/models/prompt_tuning/final/training_config.json\n"
          ]
        }
      ],
      "source": [
        "# Save training configuration\n",
        "training_config = {\n",
        "    \"method\": \"prompt_tuning\",\n",
        "    \"base_model\": BASE_MODEL_ID,\n",
        "    \"training_data_size\": len(train_data),\n",
        "    \"validation_data_size\": len(val_data),\n",
        "    \"prompt_tuning_config\": {\n",
        "        \"num_virtual_tokens\": prompt_tuning_config.num_virtual_tokens,\n",
        "        \"init_method\": str(prompt_tuning_config.prompt_tuning_init),\n",
        "    },\n",
        "    \"training_args\": {\n",
        "        \"epochs\": training_args.num_train_epochs,\n",
        "        \"batch_size\": training_args.per_device_train_batch_size,\n",
        "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "    },\n",
        "    \"results\": {\n",
        "        \"train_loss\": train_result.training_loss,\n",
        "        \"eval_loss\": eval_results[\"eval_loss\"],\n",
        "        \"total_steps\": train_result.global_step,\n",
        "    },\n",
        "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "}\n",
        "\n",
        "config_path = f\"{FINAL_MODEL_DIR}/training_config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(training_config, f, indent=2)\n",
        "\n",
        "print(f\"Training config saved to: {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbabnthyUfyX"
      },
      "source": [
        "## 12. Save Efficiency Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T47mxBCHUfyc",
        "outputId": "34431eab-e06e-4f01-e221-e308520ca351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to /content/drive/MyDrive/synthetic-instruction-tuner/evaluation/metrics/prompt_tuning_metrics.json\n",
            "\n",
            "=== Efficiency Metrics Summary ===\n",
            "  method: prompt_tuning\n",
            "  trainable_params: 61440\n",
            "  total_params: 1803525120\n",
            "  trainable_ratio_percent: 0.0034\n",
            "  peak_memory_gb: 5.9416\n",
            "  training_time_hours: 0.3134\n",
            "  inference_tokens_per_sec: 8.4442\n",
            "  train_loss: 5.2226\n",
            "  eval_loss: 2.9795\n"
          ]
        }
      ],
      "source": [
        "# Save efficiency metrics for comparison\n",
        "METRICS_DIR = f\"{PROJECT_ROOT}/evaluation/metrics\"\n",
        "os.makedirs(METRICS_DIR, exist_ok=True)\n",
        "\n",
        "# Get summary\n",
        "summary = tracker.get_summary()\n",
        "\n",
        "# Add training results\n",
        "summary[\"train_loss\"] = train_result.training_loss\n",
        "summary[\"eval_loss\"] = eval_results[\"eval_loss\"]\n",
        "\n",
        "# Save\n",
        "tracker.save(f\"{METRICS_DIR}/prompt_tuning_metrics.json\")\n",
        "\n",
        "print(\"\\n=== Efficiency Metrics Summary ===\")\n",
        "for key, value in summary.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1LGRMG7Ufyc"
      },
      "source": [
        "## 13. Comparison Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFmSJ09fUfyd",
        "outputId": "c41c07a2-4962-4dad-c470-dffd6be6248d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Method Comparison (A100 GPU) ===\n",
            "Metric                                    LoRA   Prompt Tuning\n",
            "============================================================\n",
            "Trainable Params                    12,156,928          61,440\n",
            "Trainable Ratio (%)                     0.6696          0.0034\n",
            "Peak Memory (GB)                        5.3088          5.9416\n",
            "Training Time (hours)                   0.1369          0.3134\n",
            "Inference Speed (tok/s)                 7.6954          8.4442\n",
            "Train Loss                                 N/A 5.222607662803249\n",
            "Eval Loss                                  N/A 2.979475975036621\n",
            "\n",
            "--- Key Findings ---\n",
            "Prompt Tuning uses 197.9x FEWER parameters than LoRA\n",
            "Prompt Tuning is 0.4x FASTER than LoRA\n"
          ]
        }
      ],
      "source": [
        "# Load LoRA metrics if available for comparison\n",
        "lora_metrics_path = f\"{METRICS_DIR}/lora_metrics.json\"\n",
        "\n",
        "if os.path.exists(lora_metrics_path):\n",
        "    with open(lora_metrics_path, 'r') as f:\n",
        "        lora_metrics = json.load(f)\n",
        "\n",
        "    print(\"\\n=== Method Comparison (A100 GPU) ===\")\n",
        "    print(f\"{'Metric':<30} {'LoRA':>15} {'Prompt Tuning':>15}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    comparisons = [\n",
        "        (\"Trainable Params\", \"trainable_params\"),\n",
        "        (\"Trainable Ratio (%)\", \"trainable_ratio_percent\"),\n",
        "        (\"Peak Memory (GB)\", \"peak_memory_gb\"),\n",
        "        (\"Training Time (hours)\", \"training_time_hours\"),\n",
        "        (\"Inference Speed (tok/s)\", \"inference_tokens_per_sec\"),\n",
        "        (\"Train Loss\", \"train_loss\"),\n",
        "        (\"Eval Loss\", \"eval_loss\"),\n",
        "    ]\n",
        "\n",
        "    for label, key in comparisons:\n",
        "        lora_val = lora_metrics.get(key, \"N/A\")\n",
        "        pt_val = summary.get(key, \"N/A\")\n",
        "\n",
        "        if isinstance(lora_val, (int, float)) and isinstance(pt_val, (int, float)):\n",
        "            if key == \"trainable_params\":\n",
        "                print(f\"{label:<30} {lora_val:>15,} {pt_val:>15,}\")\n",
        "            else:\n",
        "                print(f\"{label:<30} {lora_val:>15.4f} {pt_val:>15.4f}\")\n",
        "        else:\n",
        "            print(f\"{label:<30} {str(lora_val):>15} {str(pt_val):>15}\")\n",
        "\n",
        "    print(\"\\n--- Key Findings ---\")\n",
        "    if isinstance(lora_metrics.get('trainable_params'), (int, float)) and isinstance(summary.get('trainable_params'), (int, float)):\n",
        "        ratio = lora_metrics['trainable_params'] / summary['trainable_params']\n",
        "        print(f\"Prompt Tuning uses {ratio:.1f}x FEWER parameters than LoRA\")\n",
        "    if isinstance(lora_metrics.get('training_time_hours'), (int, float)) and isinstance(summary.get('training_time_hours'), (int, float)):\n",
        "        speedup = lora_metrics['training_time_hours'] / summary['training_time_hours']\n",
        "        print(f\"Prompt Tuning is {speedup:.1f}x FASTER than LoRA\")\n",
        "else:\n",
        "    print(\"LoRA metrics not found. Run 05_sft_training.ipynb with metrics tracking first.\")\n",
        "    print(\"Full comparison will be available in 09_comparative_analysis.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JGSQsE4Ufyd"
      },
      "source": [
        "## 14. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHc9XCGWUfyd",
        "outputId": "c90cc9c4-d929-4b9a-9d1e-0ea8b77fa3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleared!\n"
          ]
        }
      ],
      "source": [
        "# Free GPU memory\n",
        "del model\n",
        "del trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Memory cleared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2iJgUaxUfyd"
      },
      "source": [
        "## ✓ Prompt Tuning Complete!\n",
        "\n",
        "### Summary:\n",
        "- Prompt Tuning model saved to `models/prompt_tuning/final/`\n",
        "- Efficiency metrics saved for comparison\n",
        "- Only soft prompt embeddings were trained (minimal parameters)\n",
        "\n",
        "### Key Differences from LoRA (A100):\n",
        "- **Parameters**: ~0.01% vs LoRA's ~0.67%\n",
        "- **Training**: 2-3x faster due to fewer gradients\n",
        "- **Memory**: Lower peak usage (~15-20GB vs ~20-25GB)\n",
        "- **Performance**: Often slightly lower quality than LoRA but much more efficient\n",
        "\n",
        "### Next Steps:\n",
        "1. Run `09_comparative_analysis.ipynb` for full comparison\n",
        "2. Compare benchmark scores between methods\n",
        "3. Include results in your final report\n",
        "\n",
        "### Training Stats (A100):\n",
        "- **Training time**: 1-2 hours (2-3x faster than LoRA)\n",
        "- **Cost**: ~5-10 compute units (half of LoRA)\n",
        "- **Batch size**: 12 (effective: 24)\n",
        "- **Precision**: BF16 + TF32"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e96b977c54284cbca0caa3f562b9f0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2fa8cda81a24072b4f663e4040363df",
              "IPY_MODEL_0be7a04f52994b6a8c097fc7b5b3e3fb",
              "IPY_MODEL_37aa0ead545e46d38a58a7966c323fa0"
            ],
            "layout": "IPY_MODEL_dc5d99e9b2504fc48f2ee9a9695c9d26"
          }
        },
        "f2fa8cda81a24072b4f663e4040363df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3751352378924696ae443235cf4585f9",
            "placeholder": "​",
            "style": "IPY_MODEL_35d1114f6a0f4ede999b290bb2b5b988",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0be7a04f52994b6a8c097fc7b5b3e3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c05275be87144e45a0252f2873325e6d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71f22e18e32841f38441de3f875aa16f",
            "value": 2
          }
        },
        "37aa0ead545e46d38a58a7966c323fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe0cc411316476aa983385d195210d1",
            "placeholder": "​",
            "style": "IPY_MODEL_9779dca9c1d04b5bb36fb184e3f63cd7",
            "value": " 2/2 [00:06&lt;00:00,  2.83s/it]"
          }
        },
        "dc5d99e9b2504fc48f2ee9a9695c9d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3751352378924696ae443235cf4585f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d1114f6a0f4ede999b290bb2b5b988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c05275be87144e45a0252f2873325e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f22e18e32841f38441de3f875aa16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efe0cc411316476aa983385d195210d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9779dca9c1d04b5bb36fb184e3f63cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f81123af953643fd94e27ebe5c4fd940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d4d8248feb24b4686791d2b97e602c8",
              "IPY_MODEL_d674da683a21462ea29fe5cae1546bdb",
              "IPY_MODEL_c9bf499a60684d7a8ed43bf97285cc27"
            ],
            "layout": "IPY_MODEL_17268b0f59104775a9a701530cdb11ff"
          }
        },
        "4d4d8248feb24b4686791d2b97e602c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3596b1f444c245d9b39ca2790d6e99ba",
            "placeholder": "​",
            "style": "IPY_MODEL_808116a2839545c1be2298fea11101f5",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "d674da683a21462ea29fe5cae1546bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9919f8590334d67bbf73d2d4956e5ff",
            "max": 900,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a5d993d29f444729a0efb0957a3aaea",
            "value": 900
          }
        },
        "c9bf499a60684d7a8ed43bf97285cc27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_480711424b27416b92c26ec0243b905c",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b77f1d5ee84d30985cbfbc0651c63b",
            "value": " 900/900 [00:02&lt;00:00, 591.00 examples/s]"
          }
        },
        "17268b0f59104775a9a701530cdb11ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3596b1f444c245d9b39ca2790d6e99ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808116a2839545c1be2298fea11101f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9919f8590334d67bbf73d2d4956e5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a5d993d29f444729a0efb0957a3aaea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "480711424b27416b92c26ec0243b905c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b77f1d5ee84d30985cbfbc0651c63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4aa21760b1f4d7c8ef2b806ff3ddf43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d424d58c509401ab4bf327508d917f8",
              "IPY_MODEL_16aa0690203140a8af668a660032708a",
              "IPY_MODEL_fdacbbd65dda4d35bdc066e2ac8df12e"
            ],
            "layout": "IPY_MODEL_7824722fa62c476c845b377921ef398a"
          }
        },
        "5d424d58c509401ab4bf327508d917f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d7942fe95b4f2495c11d50cac14a85",
            "placeholder": "​",
            "style": "IPY_MODEL_23b9287b41ca4eab83994ff5398728e0",
            "value": "Tokenizing val dataset: 100%"
          }
        },
        "16aa0690203140a8af668a660032708a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1408e408d6d84e34a8317f1af0d34b41",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1f7ea632aa243daaf6945c4150f6b94",
            "value": 100
          }
        },
        "fdacbbd65dda4d35bdc066e2ac8df12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a38840ba68347c0bce53f10d2d24a48",
            "placeholder": "​",
            "style": "IPY_MODEL_24785e76179b4ba49ce7659fcfcd110f",
            "value": " 100/100 [00:00&lt;00:00, 577.93 examples/s]"
          }
        },
        "7824722fa62c476c845b377921ef398a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d7942fe95b4f2495c11d50cac14a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b9287b41ca4eab83994ff5398728e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1408e408d6d84e34a8317f1af0d34b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f7ea632aa243daaf6945c4150f6b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a38840ba68347c0bce53f10d2d24a48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24785e76179b4ba49ce7659fcfcd110f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
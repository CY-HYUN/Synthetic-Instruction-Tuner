{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05b. Prompt Tuning\n",
    "## Synthetic Instruction Tuner - Alternative Adaptation Method\n",
    "\n",
    "This notebook implements **Prompt Tuning** as an alternative adaptation method for comparison:\n",
    "1. Load base model (same as SFT)\n",
    "2. Initialize soft prompt embeddings\n",
    "3. Train only the prompt parameters\n",
    "4. Collect efficiency metrics for comparison\n",
    "5. Evaluate and save the model\n",
    "\n",
    "**Comparison with LoRA**:\n",
    "| Method | Trainable Params | Memory | Training Time |\n",
    "|--------|-----------------|--------|---------------|\n",
    "| LoRA (r=8) | ~0.1-0.5% | Medium | Medium |\n",
    "| Prompt Tuning | ~0.01% | Low | Fast |\n",
    "\n",
    "**Expected runtime**: 3-5 hours on Colab T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Efficiency Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Project path\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install libraries with latest compatible versions (avoid dependency conflicts)\n!pip install -q --upgrade transformers>=4.41.0 peft>=0.7.0 trl>=0.7.4 datasets>=2.16.0 accelerate>=0.25.0 bitsandbytes>=0.41.3\n\nprint(\"✅ Libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency Metrics Tracker\n",
    "class EfficiencyTracker:\n",
    "    \"\"\"Track efficiency metrics for adaptation method comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, method_name: str):\n",
    "        self.method_name = method_name\n",
    "        self.metrics = {\n",
    "            \"method\": method_name,\n",
    "            \"memory_allocated_gb\": [],\n",
    "            \"memory_reserved_gb\": [],\n",
    "            \"training_time_seconds\": 0,\n",
    "            \"trainable_params\": 0,\n",
    "            \"total_params\": 0,\n",
    "            \"trainable_ratio\": 0,\n",
    "            \"inference_tokens_per_sec\": 0,\n",
    "        }\n",
    "        self.start_time = None\n",
    "    \n",
    "    def log_memory(self):\n",
    "        \"\"\"Log current GPU memory usage.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            self.metrics[\"memory_allocated_gb\"].append(allocated)\n",
    "            self.metrics[\"memory_reserved_gb\"].append(reserved)\n",
    "            return {\"allocated\": allocated, \"reserved\": reserved}\n",
    "        return None\n",
    "    \n",
    "    def start_training(self):\n",
    "        \"\"\"Start timing training.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.log_memory()\n",
    "    \n",
    "    def end_training(self):\n",
    "        \"\"\"End timing training.\"\"\"\n",
    "        if self.start_time:\n",
    "            self.metrics[\"training_time_seconds\"] = time.time() - self.start_time\n",
    "        self.log_memory()\n",
    "    \n",
    "    def log_params(self, model):\n",
    "        \"\"\"Log parameter counts.\"\"\"\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        self.metrics[\"trainable_params\"] = trainable\n",
    "        self.metrics[\"total_params\"] = total\n",
    "        self.metrics[\"trainable_ratio\"] = trainable / total if total > 0 else 0\n",
    "    \n",
    "    def log_inference_speed(self, tokens_generated: int, time_taken: float):\n",
    "        \"\"\"Log inference speed.\"\"\"\n",
    "        self.metrics[\"inference_tokens_per_sec\"] = tokens_generated / time_taken if time_taken > 0 else 0\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary metrics.\"\"\"\n",
    "        summary = {\n",
    "            \"method\": self.method_name,\n",
    "            \"trainable_params\": self.metrics[\"trainable_params\"],\n",
    "            \"total_params\": self.metrics[\"total_params\"],\n",
    "            \"trainable_ratio_percent\": self.metrics[\"trainable_ratio\"] * 100,\n",
    "            \"peak_memory_gb\": max(self.metrics[\"memory_allocated_gb\"]) if self.metrics[\"memory_allocated_gb\"] else 0,\n",
    "            \"training_time_hours\": self.metrics[\"training_time_seconds\"] / 3600,\n",
    "            \"inference_tokens_per_sec\": self.metrics[\"inference_tokens_per_sec\"],\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save metrics to JSON.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.get_summary(), f, indent=2)\n",
    "        print(f\"Metrics saved to {path}\")\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = EfficiencyTracker(\"prompt_tuning\")\n",
    "print(\"Efficiency tracker initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SFT training data (same as LoRA)\n",
    "TRAIN_PATH = f\"{config['paths']['data_filtered']}/sft_train.json\"\n",
    "VAL_PATH = f\"{config['paths']['data_filtered']}/sft_val.json\"\n",
    "\n",
    "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(VAL_PATH, 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Val dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Use same base model as SFT for fair comparison\n",
    "BASE_MODEL_ID = config['models']['sft_base']\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization (same as LoRA for fair comparison)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Quantization config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Log initial memory\n",
    "mem = tracker.log_memory()\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"GPU Memory: {mem['allocated']:.2f} GB allocated, {mem['reserved']:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, TaskType\n",
    "\n",
    "# Prompt Tuning configuration\n",
    "# num_virtual_tokens: number of soft prompt tokens to prepend\n",
    "prompt_tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=20,  # Number of soft prompt tokens\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  # or TEXT for initialized prompts\n",
    "    tokenizer_name_or_path=BASE_MODEL_ID,\n",
    ")\n",
    "\n",
    "print(\"Prompt Tuning config:\")\n",
    "print(f\"  num_virtual_tokens: {prompt_tuning_config.num_virtual_tokens}\")\n",
    "print(f\"  init_method: {prompt_tuning_config.prompt_tuning_init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Prompt Tuning to model\n",
    "model = get_peft_model(model, prompt_tuning_config)\n",
    "\n",
    "# Log parameters\n",
    "tracker.log_params(model)\n",
    "\n",
    "print(f\"\\nTrainable parameters: {tracker.metrics['trainable_params']:,}\")\n",
    "print(f\"Total parameters: {tracker.metrics['total_params']:,}\")\n",
    "print(f\"Trainable ratio: {tracker.metrics['trainable_ratio']*100:.4f}%\")\n",
    "\n",
    "# Compare with LoRA\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Prompt Tuning trains ONLY the soft prompt embeddings\")\n",
    "print(\"This is typically 10-100x fewer parameters than LoRA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trainable parameters\n",
    "print(\"\\nTrainable layers:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format instruction-response pair for training.\"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    response = sample[\"output\"]\n",
    "    \n",
    "    # Same format as LoRA for fair comparison\n",
    "    text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(format_instruction, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"Formatted train dataset: {train_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Output directory for Prompt Tuning\n",
    "OUTPUT_DIR = f\"{PROJECT_ROOT}/models/prompt_tuning/checkpoint\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (same hyperparameters as LoRA for fair comparison)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters (same as LoRA)\n",
    "    num_train_epochs=config['training']['sft_epochs'],\n",
    "    per_device_train_batch_size=config['training']['sft_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['sft_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    \n",
    "    # Optimizer (slightly higher LR often works better for prompt tuning)\n",
    "    learning_rate=3e-4,  # Higher than LoRA's 2e-4\n",
    "    weight_decay=0.01,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Trainer with Prompt Tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=prompt_tuning_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with timing\n",
    "print(\"Starting Prompt Tuning training...\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tracker.start_training()\n",
    "train_result = trainer.train()\n",
    "tracker.end_training()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Training time: {tracker.metrics['training_time_seconds']/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"\\nTraining metrics:\")\n",
    "print(f\"  Final train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Generation & Measure Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(instruction: str, max_new_tokens: int = 256):\n",
    "    \"\"\"Generate a response for the given instruction.\"\"\"\n",
    "    prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        gen_time = time.time() - start_time\n",
    "    \n",
    "    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract response\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated:\n",
    "        response = generated.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "        response = response.split(\"<|eot_id|>\")[0].strip()\n",
    "        return response, tokens_generated, gen_time\n",
    "    \n",
    "    return generated, tokens_generated, gen_time\n",
    "\n",
    "print(\"Generation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation and measure inference speed\n",
    "test_instructions = [\n",
    "    \"Explain the concept of machine learning in simple terms.\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"What are the main differences between supervised and unsupervised learning?\",\n",
    "]\n",
    "\n",
    "print(\"Testing Prompt Tuning model generation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_tokens = 0\n",
    "total_time = 0\n",
    "\n",
    "for i, instruction in enumerate(test_instructions):\n",
    "    print(f\"\\n[Test {i+1}]\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    print(f\"\\nResponse:\")\n",
    "    response, tokens, gen_time = generate_response(instruction, max_new_tokens=200)\n",
    "    print(response)\n",
    "    print(f\"\\nTokens: {tokens}, Time: {gen_time:.2f}s, Speed: {tokens/gen_time:.1f} tok/s\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_tokens += tokens\n",
    "    total_time += gen_time\n",
    "\n",
    "# Log inference speed\n",
    "tracker.log_inference_speed(total_tokens, total_time)\n",
    "print(f\"\\nAverage inference speed: {tracker.metrics['inference_tokens_per_sec']:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "FINAL_MODEL_DIR = f\"{PROJECT_ROOT}/models/prompt_tuning/final\"\n",
    "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving final model to: {FINAL_MODEL_DIR}\")\n",
    "\n",
    "trainer.save_model(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"method\": \"prompt_tuning\",\n",
    "    \"base_model\": BASE_MODEL_ID,\n",
    "    \"training_data_size\": len(train_data),\n",
    "    \"validation_data_size\": len(val_data),\n",
    "    \"prompt_tuning_config\": {\n",
    "        \"num_virtual_tokens\": prompt_tuning_config.num_virtual_tokens,\n",
    "        \"init_method\": str(prompt_tuning_config.prompt_tuning_init),\n",
    "    },\n",
    "    \"training_args\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"train_loss\": train_result.training_loss,\n",
    "        \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "        \"total_steps\": train_result.global_step,\n",
    "    },\n",
    "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "\n",
    "config_path = f\"{FINAL_MODEL_DIR}/training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"Training config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save efficiency metrics for comparison\n",
    "METRICS_DIR = f\"{PROJECT_ROOT}/evaluation/metrics\"\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Get summary\n",
    "summary = tracker.get_summary()\n",
    "\n",
    "# Add training results\n",
    "summary[\"train_loss\"] = train_result.training_loss\n",
    "summary[\"eval_loss\"] = eval_results[\"eval_loss\"]\n",
    "\n",
    "# Save\n",
    "tracker.save(f\"{METRICS_DIR}/prompt_tuning_metrics.json\")\n",
    "\n",
    "print(\"\\n=== Efficiency Metrics Summary ===\")\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Comparison Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA metrics if available for comparison\n",
    "lora_metrics_path = f\"{METRICS_DIR}/lora_metrics.json\"\n",
    "\n",
    "if os.path.exists(lora_metrics_path):\n",
    "    with open(lora_metrics_path, 'r') as f:\n",
    "        lora_metrics = json.load(f)\n",
    "    \n",
    "    print(\"\\n=== Method Comparison ===\")\n",
    "    print(f\"{'Metric':<30} {'LoRA':>15} {'Prompt Tuning':>15}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    comparisons = [\n",
    "        (\"Trainable Params\", \"trainable_params\"),\n",
    "        (\"Trainable Ratio (%)\", \"trainable_ratio_percent\"),\n",
    "        (\"Peak Memory (GB)\", \"peak_memory_gb\"),\n",
    "        (\"Training Time (hours)\", \"training_time_hours\"),\n",
    "        (\"Inference Speed (tok/s)\", \"inference_tokens_per_sec\"),\n",
    "    ]\n",
    "    \n",
    "    for label, key in comparisons:\n",
    "        lora_val = lora_metrics.get(key, \"N/A\")\n",
    "        pt_val = summary.get(key, \"N/A\")\n",
    "        \n",
    "        if isinstance(lora_val, (int, float)) and isinstance(pt_val, (int, float)):\n",
    "            if key == \"trainable_params\":\n",
    "                print(f\"{label:<30} {lora_val:>15,} {pt_val:>15,}\")\n",
    "            else:\n",
    "                print(f\"{label:<30} {lora_val:>15.4f} {pt_val:>15.4f}\")\n",
    "        else:\n",
    "            print(f\"{label:<30} {str(lora_val):>15} {str(pt_val):>15}\")\n",
    "else:\n",
    "    print(\"LoRA metrics not found. Run 05_sft_training.ipynb with metrics tracking first.\")\n",
    "    print(\"Full comparison will be available in 09_comparative_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✓ Prompt Tuning Complete!\n",
    "\n",
    "### Summary:\n",
    "- Prompt Tuning model saved to `models/prompt_tuning/final/`\n",
    "- Efficiency metrics saved for comparison\n",
    "- Only soft prompt embeddings were trained (minimal parameters)\n",
    "\n",
    "### Key Differences from LoRA:\n",
    "- **Parameters**: ~0.01% vs LoRA's ~0.1-0.5%\n",
    "- **Training**: Faster due to fewer gradients\n",
    "- **Memory**: Lower peak usage\n",
    "- **Performance**: Often slightly lower quality than LoRA\n",
    "\n",
    "### Next Steps:\n",
    "1. Run `09_comparative_analysis.ipynb` for full comparison\n",
    "2. Compare benchmark scores between methods\n",
    "3. Include results in your report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
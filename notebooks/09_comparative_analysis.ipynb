{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09. Comparative Analysis\n",
        "## Synthetic Instruction Tuner - Method Comparison\n",
        "\n",
        "This notebook provides comprehensive comparison of different adaptation methods:\n",
        "1. Load efficiency metrics from all methods\n",
        "2. Load benchmark evaluation results\n",
        "3. Compare performance vs efficiency trade-offs\n",
        "4. Generate visualizations and summary tables\n",
        "5. Export results for report\n",
        "\n",
        "**Methods Compared**:\n",
        "- LoRA (r=8, alpha=16)\n",
        "- Prompt Tuning (20 virtual tokens)\n",
        "- DPO (preference alignment)\n",
        "- Zero-shot Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Project path\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "import json\n",
        "\n",
        "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(\"Configuration loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "METRICS_DIR = f\"{PROJECT_ROOT}/evaluation/metrics\"\n",
        "FIGURES_DIR = f\"{PROJECT_ROOT}/evaluation/figures\"\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Libraries loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Efficiency Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metrics from all methods\n",
        "def load_metrics(method_name: str) -> dict:\n",
        "    \"\"\"Load metrics for a specific method.\"\"\"\n",
        "    path = f\"{METRICS_DIR}/{method_name}_metrics.json\"\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    else:\n",
        "        print(f\"Warning: {path} not found\")\n",
        "        return None\n",
        "\n",
        "# Load all available metrics\n",
        "methods = ['lora', 'prompt_tuning', 'dpo']\n",
        "all_metrics = {}\n",
        "\n",
        "for method in methods:\n",
        "    metrics = load_metrics(method)\n",
        "    if metrics:\n",
        "        all_metrics[method] = metrics\n",
        "        print(f\"Loaded metrics for: {method}\")\n",
        "\n",
        "print(f\"\\nTotal methods loaded: {len(all_metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "if all_metrics:\n",
        "    df_metrics = pd.DataFrame(all_metrics).T\n",
        "    df_metrics.index.name = 'Method'\n",
        "    \n",
        "    print(\"=== Efficiency Metrics Comparison ===\")\n",
        "    display(df_metrics)\n",
        "else:\n",
        "    print(\"No metrics available. Run training notebooks first.\")\n",
        "    # Create sample data for demonstration\n",
        "    sample_data = {\n",
        "        'lora': {\n",
        "            'method': 'lora_r8',\n",
        "            'trainable_params': 4194304,\n",
        "            'total_params': 3000000000,\n",
        "            'trainable_ratio_percent': 0.14,\n",
        "            'peak_memory_gb': 8.5,\n",
        "            'training_time_hours': 6.5,\n",
        "            'inference_tokens_per_sec': 25.0,\n",
        "            'train_loss': 1.45,\n",
        "            'eval_loss': 1.52\n",
        "        },\n",
        "        'prompt_tuning': {\n",
        "            'method': 'prompt_tuning',\n",
        "            'trainable_params': 81920,\n",
        "            'total_params': 3000000000,\n",
        "            'trainable_ratio_percent': 0.003,\n",
        "            'peak_memory_gb': 6.2,\n",
        "            'training_time_hours': 3.5,\n",
        "            'inference_tokens_per_sec': 28.0,\n",
        "            'train_loss': 1.85,\n",
        "            'eval_loss': 1.92\n",
        "        },\n",
        "        'dpo': {\n",
        "            'method': 'dpo',\n",
        "            'trainable_params': 4194304,\n",
        "            'total_params': 3000000000,\n",
        "            'trainable_ratio_percent': 0.14,\n",
        "            'peak_memory_gb': 12.5,\n",
        "            'training_time_hours': 4.0,\n",
        "            'inference_tokens_per_sec': 25.0,\n",
        "            'train_loss': 0.65,\n",
        "            'eval_loss': 0.72\n",
        "        }\n",
        "    }\n",
        "    all_metrics = sample_data\n",
        "    df_metrics = pd.DataFrame(sample_data).T\n",
        "    print(\"Using sample data for demonstration:\")\n",
        "    display(df_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Benchmark Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load benchmark results if available\n",
        "benchmark_path = f\"{METRICS_DIR}/benchmark_results.json\"\n",
        "\n",
        "if os.path.exists(benchmark_path):\n",
        "    with open(benchmark_path, 'r') as f:\n",
        "        benchmark_results = json.load(f)\n",
        "    print(\"Benchmark results loaded!\")\n",
        "else:\n",
        "    # Sample benchmark data\n",
        "    benchmark_results = {\n",
        "        'zero_shot': {\n",
        "            'mmlu': 42.5,\n",
        "            'hellaswag': 55.2,\n",
        "            'arc_easy': 58.3,\n",
        "            'truthfulqa': 35.1\n",
        "        },\n",
        "        'lora': {\n",
        "            'mmlu': 48.2,\n",
        "            'hellaswag': 62.5,\n",
        "            'arc_easy': 65.8,\n",
        "            'truthfulqa': 42.3\n",
        "        },\n",
        "        'prompt_tuning': {\n",
        "            'mmlu': 45.1,\n",
        "            'hellaswag': 58.8,\n",
        "            'arc_easy': 61.2,\n",
        "            'truthfulqa': 38.5\n",
        "        },\n",
        "        'dpo': {\n",
        "            'mmlu': 49.5,\n",
        "            'hellaswag': 64.2,\n",
        "            'arc_easy': 67.5,\n",
        "            'truthfulqa': 48.9\n",
        "        }\n",
        "    }\n",
        "    print(\"Using sample benchmark data for demonstration\")\n",
        "\n",
        "df_benchmarks = pd.DataFrame(benchmark_results).T\n",
        "df_benchmarks.index.name = 'Method'\n",
        "\n",
        "print(\"\\n=== Benchmark Results ===\")\n",
        "display(df_benchmarks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Efficiency Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "methods_list = list(all_metrics.keys())\n",
        "colors = sns.color_palette(\"husl\", len(methods_list))\n",
        "\n",
        "# 1. Trainable Parameters\n",
        "ax = axes[0, 0]\n",
        "params = [all_metrics[m]['trainable_params'] for m in methods_list]\n",
        "bars = ax.bar(methods_list, params, color=colors)\n",
        "ax.set_ylabel('Parameters')\n",
        "ax.set_title('Trainable Parameters')\n",
        "ax.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
        "for bar, val in zip(bars, params):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{val:,.0f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 2. Trainable Ratio\n",
        "ax = axes[0, 1]\n",
        "ratios = [all_metrics[m]['trainable_ratio_percent'] for m in methods_list]\n",
        "bars = ax.bar(methods_list, ratios, color=colors)\n",
        "ax.set_ylabel('Percentage (%)')\n",
        "ax.set_title('Trainable Parameter Ratio')\n",
        "for bar, val in zip(bars, ratios):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{val:.3f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 3. Peak Memory\n",
        "ax = axes[0, 2]\n",
        "memory = [all_metrics[m]['peak_memory_gb'] for m in methods_list]\n",
        "bars = ax.bar(methods_list, memory, color=colors)\n",
        "ax.set_ylabel('Memory (GB)')\n",
        "ax.set_title('Peak GPU Memory Usage')\n",
        "ax.axhline(y=15, color='r', linestyle='--', label='T4 Limit (15GB)')\n",
        "ax.legend()\n",
        "for bar, val in zip(bars, memory):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Training Time\n",
        "ax = axes[1, 0]\n",
        "time_hrs = [all_metrics[m]['training_time_hours'] for m in methods_list]\n",
        "bars = ax.bar(methods_list, time_hrs, color=colors)\n",
        "ax.set_ylabel('Hours')\n",
        "ax.set_title('Training Time')\n",
        "for bar, val in zip(bars, time_hrs):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{val:.1f}h', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 5. Inference Speed\n",
        "ax = axes[1, 1]\n",
        "speed = [all_metrics[m]['inference_tokens_per_sec'] for m in methods_list]\n",
        "bars = ax.bar(methods_list, speed, color=colors)\n",
        "ax.set_ylabel('Tokens/sec')\n",
        "ax.set_title('Inference Speed')\n",
        "for bar, val in zip(bars, speed):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 6. Eval Loss\n",
        "ax = axes[1, 2]\n",
        "losses = [all_metrics[m].get('eval_loss', 0) for m in methods_list]\n",
        "bars = ax.bar(methods_list, losses, color=colors)\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Evaluation Loss')\n",
        "for bar, val in zip(bars, losses):\n",
        "    if val:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "                f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.suptitle('Adaptation Method Efficiency Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/efficiency_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved to {FIGURES_DIR}/efficiency_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Benchmark Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark comparison radar chart\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Bar chart comparison\n",
        "ax = axes[0]\n",
        "df_benchmarks.plot(kind='bar', ax=ax, width=0.8)\n",
        "ax.set_xlabel('Method')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Benchmark Performance by Method')\n",
        "ax.legend(title='Benchmark', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Average performance\n",
        "ax = axes[1]\n",
        "avg_scores = df_benchmarks.mean(axis=1)\n",
        "bars = ax.bar(avg_scores.index, avg_scores.values, color=colors[:len(avg_scores)])\n",
        "ax.set_ylabel('Average Accuracy (%)')\n",
        "ax.set_title('Average Benchmark Performance')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "for bar, val in zip(bars, avg_scores.values):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/benchmark_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved to {FIGURES_DIR}/benchmark_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance vs Efficiency Trade-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trade-off analysis\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter: Performance vs Training Time\n",
        "ax = axes[0]\n",
        "for i, method in enumerate(methods_list):\n",
        "    if method in df_benchmarks.index:\n",
        "        x = all_metrics[method]['training_time_hours']\n",
        "        y = df_benchmarks.loc[method].mean()\n",
        "        ax.scatter(x, y, s=200, c=[colors[i]], label=method, edgecolors='black', linewidth=2)\n",
        "        ax.annotate(method, (x, y), textcoords=\"offset points\", \n",
        "                    xytext=(0, 10), ha='center', fontsize=10)\n",
        "\n",
        "ax.set_xlabel('Training Time (hours)')\n",
        "ax.set_ylabel('Average Benchmark Score (%)')\n",
        "ax.set_title('Performance vs Training Time')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Scatter: Performance vs Memory\n",
        "ax = axes[1]\n",
        "for i, method in enumerate(methods_list):\n",
        "    if method in df_benchmarks.index:\n",
        "        x = all_metrics[method]['peak_memory_gb']\n",
        "        y = df_benchmarks.loc[method].mean()\n",
        "        ax.scatter(x, y, s=200, c=[colors[i]], label=method, edgecolors='black', linewidth=2)\n",
        "        ax.annotate(method, (x, y), textcoords=\"offset points\", \n",
        "                    xytext=(0, 10), ha='center', fontsize=10)\n",
        "\n",
        "ax.set_xlabel('Peak Memory (GB)')\n",
        "ax.set_ylabel('Average Benchmark Score (%)')\n",
        "ax.set_title('Performance vs Memory Usage')\n",
        "ax.axvline(x=15, color='r', linestyle='--', label='T4 Limit', alpha=0.7)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/tradeoff_analysis.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved to {FIGURES_DIR}/tradeoff_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary Table for Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive summary table\n",
        "summary_data = []\n",
        "\n",
        "for method in methods_list:\n",
        "    row = {\n",
        "        'Method': method.upper(),\n",
        "        'Trainable Params': f\"{all_metrics[method]['trainable_params']:,}\",\n",
        "        'Param Ratio (%)': f\"{all_metrics[method]['trainable_ratio_percent']:.3f}\",\n",
        "        'Memory (GB)': f\"{all_metrics[method]['peak_memory_gb']:.1f}\",\n",
        "        'Train Time (h)': f\"{all_metrics[method]['training_time_hours']:.1f}\",\n",
        "        'Inference (tok/s)': f\"{all_metrics[method]['inference_tokens_per_sec']:.1f}\",\n",
        "    }\n",
        "    \n",
        "    # Add benchmark scores if available\n",
        "    if method in df_benchmarks.index:\n",
        "        row['Avg Benchmark'] = f\"{df_benchmarks.loc[method].mean():.1f}%\"\n",
        "    else:\n",
        "        row['Avg Benchmark'] = 'N/A'\n",
        "    \n",
        "    summary_data.append(row)\n",
        "\n",
        "# Add zero-shot baseline\n",
        "if 'zero_shot' in df_benchmarks.index:\n",
        "    summary_data.insert(0, {\n",
        "        'Method': 'ZERO-SHOT',\n",
        "        'Trainable Params': '0',\n",
        "        'Param Ratio (%)': '0.000',\n",
        "        'Memory (GB)': 'N/A',\n",
        "        'Train Time (h)': '0.0',\n",
        "        'Inference (tok/s)': 'N/A',\n",
        "        'Avg Benchmark': f\"{df_benchmarks.loc['zero_shot'].mean():.1f}%\"\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE COMPARISON SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "display(df_summary)\n",
        "\n",
        "# Save to CSV\n",
        "df_summary.to_csv(f\"{METRICS_DIR}/comparison_summary.csv\", index=False)\n",
        "print(f\"\\nSaved to {METRICS_DIR}/comparison_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate key findings\n",
        "print(\"=\" * 60)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Find best methods for each metric\n",
        "findings = []\n",
        "\n",
        "# Best performance\n",
        "if 'dpo' in df_benchmarks.index:\n",
        "    best_perf = df_benchmarks.mean(axis=1).idxmax()\n",
        "    findings.append(f\"1. Best Performance: {best_perf.upper()} ({df_benchmarks.loc[best_perf].mean():.1f}% avg)\")\n",
        "\n",
        "# Most efficient (params)\n",
        "min_params_method = min(all_metrics.keys(), key=lambda m: all_metrics[m]['trainable_params'])\n",
        "findings.append(f\"2. Most Parameter-Efficient: {min_params_method.upper()} ({all_metrics[min_params_method]['trainable_params']:,} params)\")\n",
        "\n",
        "# Fastest training\n",
        "fastest_method = min(all_metrics.keys(), key=lambda m: all_metrics[m]['training_time_hours'])\n",
        "findings.append(f\"3. Fastest Training: {fastest_method.upper()} ({all_metrics[fastest_method]['training_time_hours']:.1f} hours)\")\n",
        "\n",
        "# Lowest memory\n",
        "min_mem_method = min(all_metrics.keys(), key=lambda m: all_metrics[m]['peak_memory_gb'])\n",
        "findings.append(f\"4. Lowest Memory: {min_mem_method.upper()} ({all_metrics[min_mem_method]['peak_memory_gb']:.1f} GB)\")\n",
        "\n",
        "for finding in findings:\n",
        "    print(finding)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RECOMMENDATIONS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "- For best quality: Use LoRA + DPO pipeline\n",
        "- For limited resources: Use Prompt Tuning (minimal parameters)\n",
        "- For quick experiments: Start with Prompt Tuning, then upgrade to LoRA\n",
        "- For production: LoRA provides best quality/efficiency balance\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export for Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export all results\n",
        "report_data = {\n",
        "    \"efficiency_metrics\": all_metrics,\n",
        "    \"benchmark_results\": benchmark_results,\n",
        "    \"key_findings\": findings,\n",
        "    \"summary_table\": summary_data\n",
        "}\n",
        "\n",
        "with open(f\"{METRICS_DIR}/full_comparison_report.json\", 'w') as f:\n",
        "    json.dump(report_data, f, indent=2)\n",
        "\n",
        "print(f\"Full report saved to {METRICS_DIR}/full_comparison_report.json\")\n",
        "\n",
        "# List all generated files\n",
        "print(\"\\n=== Generated Files ===\")\n",
        "for f in os.listdir(FIGURES_DIR):\n",
        "    print(f\"  - {FIGURES_DIR}/{f}\")\n",
        "for f in os.listdir(METRICS_DIR):\n",
        "    if f.endswith('.json') or f.endswith('.csv'):\n",
        "        print(f\"  - {METRICS_DIR}/{f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ“ Comparative Analysis Complete!\n",
        "\n",
        "### Generated Outputs:\n",
        "1. **Efficiency Comparison Chart**: `evaluation/figures/efficiency_comparison.png`\n",
        "2. **Benchmark Comparison Chart**: `evaluation/figures/benchmark_comparison.png`\n",
        "3. **Trade-off Analysis**: `evaluation/figures/tradeoff_analysis.png`\n",
        "4. **Summary CSV**: `evaluation/metrics/comparison_summary.csv`\n",
        "5. **Full Report JSON**: `evaluation/metrics/full_comparison_report.json`\n",
        "\n",
        "### Key Metrics Compared:\n",
        "- Trainable Parameters\n",
        "- Memory Usage\n",
        "- Training Time\n",
        "- Inference Speed\n",
        "- Benchmark Performance\n",
        "\n",
        "### Next Steps:\n",
        "1. Use these visualizations in your Written Report\n",
        "2. Include key findings in your Presentation\n",
        "3. Discuss trade-offs in Comparative Analysis section"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

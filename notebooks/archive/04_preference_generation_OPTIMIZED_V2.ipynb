{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Preference Data Generation (TRUE OPTIMIZED V2)\n",
    "## Real Batch Processing + Hang Prevention\n",
    "\n",
    "**This version: True optimization with safety measures**:\n",
    "- Real batch processing (2-4 samples)\n",
    "- Parallel temperature generation\n",
    "- Timeout detection and auto-recovery\n",
    "- Aggressive memory management\n",
    "\n",
    "**Expected Runtime**:\n",
    "- **A100: 2-3 hours** (true speed optimization)\n",
    "- **Success rate: 80-90%** (with timeout recovery)\n",
    "- Falls back gracefully if hangs\n",
    "\n",
    "**Key Innovation**: Process-level timeout detection prevents infinite hangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install -q --upgrade transformers>=4.41.0 accelerate>=0.25.0 bitsandbytes>=0.41.3\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "\n",
    "    # TRUE OPTIMIZED batch sizes\n",
    "    if \"A100\" in gpu_name:\n",
    "        BATCH_SIZE = 4  # Process 4 samples at once\n",
    "        GEN_BATCH_SIZE = 2  # Generate 2 responses at once\n",
    "    elif \"T4\" in gpu_name:\n",
    "        BATCH_SIZE = 2\n",
    "        GEN_BATCH_SIZE = 1\n",
    "    else:\n",
    "        BATCH_SIZE = 2\n",
    "        GEN_BATCH_SIZE = 1\n",
    "\n",
    "    print(f\"\\nüöÄ TRUE OPTIMIZED for {gpu_name}:\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   Generation batch: {GEN_BATCH_SIZE}\")\n",
    "    print(f\"   Timeout protection: 120s per batch\")\n",
    "else:\n",
    "    BATCH_SIZE = 1\n",
    "    GEN_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered data\n",
    "FILTERED_PATH = f\"{config['paths']['data_filtered']}/instructions_filtered.json\"\n",
    "\n",
    "with open(FILTERED_PATH, 'r', encoding='utf-8') as f:\n",
    "    filtered_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(filtered_data)} filtered samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Generator model\n",
    "GENERATOR_MODEL_ID = config['models']['data_generation']\n",
    "print(f\"Loading generator: {GENERATOR_MODEL_ID}...\")\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_ID)\n",
    "generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
    "generator_tokenizer.padding_side = \"left\"\n",
    "\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GENERATOR_MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "generator_model.eval()\n",
    "\n",
    "print(f\"‚úì Generator loaded ({torch.cuda.memory_allocated() / 1e9:.2f} GB)\")\n",
    "\n",
    "# Reward model\n",
    "REWARD_MODEL_ID = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "print(f\"Loading reward model: {REWARD_MODEL_ID}...\")\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_ID)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "reward_model.eval()\n",
    "\n",
    "print(f\"‚úì Reward model loaded ({torch.cuda.memory_allocated() / 1e9:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TRUE OPTIMIZED Generator with Timeout Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class PreferencePair:\n",
    "    instruction: str\n",
    "    chosen: str\n",
    "    rejected: str\n",
    "    chosen_score: float\n",
    "    rejected_score: float\n",
    "    margin: float\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    \"\"\"Timeout context manager.\"\"\"\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutException(\"Operation timed out\")\n",
    "    \n",
    "    # Set alarm\n",
    "    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "\n",
    "class TrueOptimizedBatchGenerator:\n",
    "    \"\"\"TRUE OPTIMIZED: Real batch processing with timeout protection.\"\"\"\n",
    "\n",
    "    def __init__(self, gen_model, gen_tokenizer, reward_model, reward_tokenizer, config=None):\n",
    "        self.gen_model = gen_model\n",
    "        self.gen_tokenizer = gen_tokenizer\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "        self.config = config or {}\n",
    "\n",
    "        self.min_margin = self.config.get('min_score_margin', 0.5)\n",
    "        self.max_new_tokens = 256\n",
    "\n",
    "        # Llama templates\n",
    "        self.instruction_template = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        self.response_template = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        # Get EOS token IDs\n",
    "        self.eot_id = self.gen_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        self.eos_id = self.gen_tokenizer.eos_token_id\n",
    "\n",
    "    def generate_batch_with_timeout(self, instructions: List[str], temperature: float, \n",
    "                                    gen_batch_size: int, timeout_sec: int = 60) -> List[str]:\n",
    "        \"\"\"Generate responses in batches WITH TIMEOUT.\"\"\"\n",
    "        all_responses = []\n",
    "\n",
    "        for i in range(0, len(instructions), gen_batch_size):\n",
    "            batch_instructions = instructions[i:i+gen_batch_size]\n",
    "\n",
    "            # Prepare prompts\n",
    "            prompts = [f\"{self.instruction_template}{inst}{self.response_template}\"\n",
    "                      for inst in batch_instructions]\n",
    "\n",
    "            try:\n",
    "                # Use timeout protection\n",
    "                with timeout(timeout_sec):\n",
    "                    # Tokenize\n",
    "                    inputs = self.gen_tokenizer(\n",
    "                        prompts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=2048\n",
    "                    ).to(self.gen_model.device)\n",
    "\n",
    "                    # Generate\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.gen_model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=self.max_new_tokens,\n",
    "                            temperature=temperature,\n",
    "                            do_sample=True,\n",
    "                            top_p=0.9,\n",
    "                            pad_token_id=self.gen_tokenizer.pad_token_id,\n",
    "                            eos_token_id=[self.eot_id, self.eos_id]\n",
    "                        )\n",
    "\n",
    "                    # Decode\n",
    "                    batch_responses = self.gen_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "                    # Parse\n",
    "                    for response in batch_responses:\n",
    "                        parsed = self._parse_response(response)\n",
    "                        all_responses.append(parsed if parsed else \"\")\n",
    "                    \n",
    "                    # Clean up\n",
    "                    del inputs, outputs\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except TimeoutException:\n",
    "                print(f\"        ‚ö†Ô∏è Timeout at temp={temperature}, skipping {len(batch_instructions)} samples\")\n",
    "                # Add empty responses for failed batch\n",
    "                all_responses.extend([\"\"] * len(batch_instructions))\n",
    "            except Exception as e:\n",
    "                print(f\"        ‚ö†Ô∏è Error at temp={temperature}: {e}\")\n",
    "                all_responses.extend([\"\"] * len(batch_instructions))\n",
    "\n",
    "        return all_responses\n",
    "\n",
    "    def _parse_response(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract response from generated text.\"\"\"\n",
    "        try:\n",
    "            if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
    "                parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "                if len(parts) > 1:\n",
    "                    response = parts[-1]\n",
    "                    for end_token in [\"<|eot_id|>\", \"<|end_of_text|>\"]:\n",
    "                        if end_token in response:\n",
    "                            response = response.split(end_token)[0]\n",
    "                    return response.strip()\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def score_batch(self, instructions: List[str], responses: List[str], batch_size: int = 8) -> List[float]:\n",
    "        \"\"\"Score responses in batches.\"\"\"\n",
    "        all_scores = []\n",
    "\n",
    "        for i in range(0, len(instructions), batch_size):\n",
    "            batch_inst = instructions[i:i+batch_size]\n",
    "            batch_resp = responses[i:i+batch_size]\n",
    "\n",
    "            texts = [f\"Question: {inst}\\n\\nAnswer: {resp}\"\n",
    "                    for inst, resp in zip(batch_inst, batch_resp)]\n",
    "\n",
    "            inputs = self.reward_tokenizer(\n",
    "                texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            ).to(self.reward_model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.reward_model(**inputs)\n",
    "                scores = outputs.logits[:, 0].cpu().numpy().tolist()\n",
    "\n",
    "            all_scores.extend(scores)\n",
    "            \n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return all_scores\n",
    "\n",
    "    def create_pairs_batch(self, batch_samples: List[dict], gen_batch_size: int = 2) -> List[PreferencePair]:\n",
    "        \"\"\"Create preference pairs for a BATCH of samples.\"\"\"\n",
    "        instructions = [s['instruction'] for s in batch_samples]\n",
    "\n",
    "        # Generate responses with different temperatures\n",
    "        temperatures = [0.6, 0.8, 1.0, 1.2]\n",
    "        all_responses = {}\n",
    "\n",
    "        for temp in temperatures:\n",
    "            responses = self.generate_batch_with_timeout(\n",
    "                instructions, temp, gen_batch_size, timeout_sec=60\n",
    "            )\n",
    "            all_responses[temp] = responses\n",
    "\n",
    "        # Create pairs\n",
    "        pairs = []\n",
    "\n",
    "        for idx, instruction in enumerate(instructions):\n",
    "            # Collect all responses\n",
    "            responses = [all_responses[temp][idx] for temp in temperatures]\n",
    "            responses = [r for r in responses if r and len(r) > 10]\n",
    "\n",
    "            if len(responses) < 2:\n",
    "                continue\n",
    "\n",
    "            # Remove duplicates\n",
    "            unique_responses = list(dict.fromkeys(responses))\n",
    "            if len(unique_responses) < 2:\n",
    "                continue\n",
    "\n",
    "            # Score\n",
    "            insts_repeated = [instruction] * len(unique_responses)\n",
    "            scores = self.score_batch(insts_repeated, unique_responses, batch_size=8)\n",
    "\n",
    "            # Create pair\n",
    "            scored = list(zip(unique_responses, scores))\n",
    "            scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            chosen, chosen_score = scored[0]\n",
    "            rejected, rejected_score = scored[-1]\n",
    "            margin = chosen_score - rejected_score\n",
    "\n",
    "            if margin >= self.min_margin:\n",
    "                pairs.append(PreferencePair(\n",
    "                    instruction=instruction,\n",
    "                    chosen=chosen,\n",
    "                    rejected=rejected,\n",
    "                    chosen_score=chosen_score,\n",
    "                    rejected_score=rejected_score,\n",
    "                    margin=margin\n",
    "                ))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "\n",
    "# Initialize generator\n",
    "pref_config = config.get('preference_generation', {})\n",
    "batch_generator = TrueOptimizedBatchGenerator(\n",
    "    generator_model,\n",
    "    generator_tokenizer,\n",
    "    reward_model,\n",
    "    reward_tokenizer,\n",
    "    pref_config\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TRUE OPTIMIZED Batch Generator initialized!\")\n",
    "print(f\"   Max tokens: {batch_generator.max_new_tokens}\")\n",
    "print(f\"   Min margin: {batch_generator.min_margin}\")\n",
    "print(f\"   Timeout protection: 60s per generation batch\")\n",
    "print(f\"   Strategy: Real batch processing with safety\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on small batch\n",
    "print(\"Testing batch generation...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_batch = filtered_data[:2]\n",
    "print(f\"Testing with {len(test_batch)} samples\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    pairs = batch_generator.create_pairs_batch(\n",
    "        test_batch,\n",
    "        gen_batch_size=GEN_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS in {elapsed:.1f}s\")\n",
    "    print(f\"Generated {len(pairs)} pairs from {len(test_batch)} samples\")\n",
    "    \n",
    "    if pairs:\n",
    "        print(f\"\\nExample pair:\")\n",
    "        print(f\"Margin: {pairs[0].margin:.3f}\")\n",
    "        print(f\"Chosen: {pairs[0].chosen[:100]}...\")\n",
    "        print(f\"Rejected: {pairs[0].rejected[:100]}...\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"\\n‚ùå Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Generation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def save_checkpoint(data, checkpoint_path):\n",
    "    \"\"\"Save checkpoint.\"\"\"\n",
    "    with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"üíæ Checkpoint saved: {len(data)} pairs\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load checkpoint.\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "# Paths\n",
    "PREFERENCE_PATH = config['paths']['data_preference']\n",
    "STABLE_CHECKPOINT = f\"{PREFERENCE_PATH}/preference_checkpoint_stable.json\"\n",
    "CHECKPOINT_PATH = f\"{PREFERENCE_PATH}/preference_checkpoint.json\"\n",
    "FINAL_PATH = f\"{PREFERENCE_PATH}/preference_data.json\"\n",
    "\n",
    "# Load checkpoint\n",
    "if os.path.exists(STABLE_CHECKPOINT) and not os.path.exists(CHECKPOINT_PATH):\n",
    "    shutil.copy(STABLE_CHECKPOINT, CHECKPOINT_PATH)\n",
    "    print(f\"‚úÖ Loaded STABLE checkpoint: {STABLE_CHECKPOINT}\")\n",
    "elif os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"‚úÖ Loaded checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "preference_data = load_checkpoint(CHECKPOINT_PATH)\n",
    "processed_instructions = {p['instruction'] for p in preference_data}\n",
    "\n",
    "# Settings\n",
    "TARGET_PAIRS = config.get('preference_generation', {}).get('target_pairs', 600)\n",
    "CHECKPOINT_INTERVAL = 50\n",
    "\n",
    "print(f\"\\nTarget: {TARGET_PAIRS} pairs\")\n",
    "print(f\"Already completed: {len(preference_data)}\")\n",
    "print(f\"\\nüöÄ TRUE OPTIMIZED MODE:\")\n",
    "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE} samples\")\n",
    "print(f\"   ‚Ä¢ Generation batch: {GEN_BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ Timeout protection enabled\")\n",
    "print(f\"   ‚Ä¢ Expected: 2-3 hours (A100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRUE OPTIMIZED Main Loop\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"STARTING TRUE OPTIMIZED GENERATION\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Filter unprocessed\n",
    "unprocessed_data = [\n",
    "    s for s in filtered_data\n",
    "    if s['instruction'] not in processed_instructions\n",
    "]\n",
    "\n",
    "print(f\"Unprocessed samples: {len(unprocessed_data)}\")\n",
    "print(f\"Processing {BATCH_SIZE} samples at a time\\n\")\n",
    "\n",
    "pbar = tqdm(total=TARGET_PAIRS, initial=len(preference_data), desc=\"Generating pairs\")\n",
    "\n",
    "data_idx = 0\n",
    "batch_num = 0\n",
    "total_start = datetime.now()\n",
    "timeout_count = 0\n",
    "success_count = 0\n",
    "\n",
    "while len(preference_data) < TARGET_PAIRS and data_idx < len(unprocessed_data):\n",
    "    batch_num += 1\n",
    "    batch_start = datetime.now()\n",
    "\n",
    "    # Get batch\n",
    "    batch_end = min(data_idx + BATCH_SIZE, len(unprocessed_data))\n",
    "    batch = unprocessed_data[data_idx:batch_end]\n",
    "    data_idx = batch_end\n",
    "\n",
    "    print(f\"\\n[Batch {batch_num}] Processing {len(batch)} samples...\")\n",
    "\n",
    "    try:\n",
    "        # TRUE BATCH PROCESSING with timeout\n",
    "        pairs = batch_generator.create_pairs_batch(\n",
    "            batch,\n",
    "            gen_batch_size=GEN_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        batch_elapsed = (datetime.now() - batch_start).total_seconds()\n",
    "        print(f\"  ‚úì Batch completed in {batch_elapsed:.1f}s\")\n",
    "\n",
    "        # Add pairs\n",
    "        added = 0\n",
    "        for pair in pairs:\n",
    "            if len(preference_data) >= TARGET_PAIRS:\n",
    "                break\n",
    "\n",
    "            preference_data.append({\n",
    "                'instruction': pair.instruction,\n",
    "                'chosen': pair.chosen,\n",
    "                'rejected': pair.rejected,\n",
    "                'chosen_score': pair.chosen_score,\n",
    "                'rejected_score': pair.rejected_score,\n",
    "                'margin': pair.margin\n",
    "            })\n",
    "            processed_instructions.add(pair.instruction)\n",
    "            pbar.update(1)\n",
    "            added += 1\n",
    "\n",
    "        success_count += 1\n",
    "        print(f\"  Added: {added}/{len(batch)} pairs\")\n",
    "\n",
    "        # Progress every 5 batches\n",
    "        if batch_num % 5 == 0:\n",
    "            elapsed = (datetime.now() - total_start).total_seconds() / 60\n",
    "            rate = len(preference_data) / elapsed if elapsed > 0 else 0\n",
    "            eta = (TARGET_PAIRS - len(preference_data)) / rate if rate > 0 else 0\n",
    "\n",
    "            print(f\"\\n  üìä Progress: {len(preference_data)}/{TARGET_PAIRS}\")\n",
    "            print(f\"  ‚ö° Rate: {rate:.2f} pairs/min | ETA: {eta:.1f}min\")\n",
    "            print(f\"  ‚úÖ Success: {success_count}/{batch_num} batches\")\n",
    "            print(f\"  ‚ö†Ô∏è Timeouts: {timeout_count}\")\n",
    "            print(f\"  üíæ GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\\n\")\n",
    "\n",
    "        # Memory cleanup EVERY batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Checkpoint\n",
    "        if len(preference_data) % CHECKPOINT_INTERVAL == 0 and len(preference_data) > 0:\n",
    "            save_checkpoint(preference_data, CHECKPOINT_PATH)\n",
    "\n",
    "    except TimeoutException:\n",
    "        timeout_count += 1\n",
    "        print(f\"\\n‚ö†Ô∏è Batch {batch_num} timed out (total timeouts: {timeout_count})\")\n",
    "        \n",
    "        if timeout_count >= 5:\n",
    "            print(f\"\\n‚ùå Too many timeouts ({timeout_count}). Consider using STABLE version.\")\n",
    "            save_checkpoint(preference_data, CHECKPOINT_PATH)\n",
    "            break\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è Interrupted by user\")\n",
    "        save_checkpoint(preference_data, CHECKPOINT_PATH)\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Batch {batch_num} error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        if len(preference_data) > 0:\n",
    "            save_checkpoint(preference_data, f\"{CHECKPOINT_PATH}.emergency_{batch_num}\")\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "total_time = (datetime.now() - total_start).total_seconds() / 60\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"COMPLETED!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total time: {total_time:.1f} minutes ({total_time/60:.1f} hours)\")\n",
    "print(f\"Total pairs: {len(preference_data)}\")\n",
    "print(f\"Success rate: {success_count}/{batch_num} batches\")\n",
    "print(f\"Timeouts: {timeout_count}\")\n",
    "if len(preference_data) > 0:\n",
    "    print(f\"Average: {total_time*60/len(preference_data):.1f}s per pair\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final\n",
    "save_checkpoint(preference_data, FINAL_PATH)\n",
    "print(f\"‚úÖ Saved to: {FINAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis & DPO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "if preference_data:\n",
    "    margins = [p['margin'] for p in preference_data]\n",
    "    chosen_scores = [p['chosen_score'] for p in preference_data]\n",
    "    rejected_scores = [p['rejected_score'] for p in preference_data]\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total pairs: {len(preference_data)}\")\n",
    "    print(f\"\\nMargin: {np.mean(margins):.3f} ¬± {np.std(margins):.3f}\")\n",
    "    print(f\"Chosen score: {np.mean(chosen_scores):.3f}\")\n",
    "    print(f\"Rejected score: {np.mean(rejected_scores):.3f}\")\n",
    "else:\n",
    "    print(\"No preference data generated yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DPO format\n",
    "dpo_data = [\n",
    "    {\n",
    "        \"prompt\": p['instruction'],\n",
    "        \"chosen\": p['chosen'],\n",
    "        \"rejected\": p['rejected']\n",
    "    }\n",
    "    for p in preference_data\n",
    "]\n",
    "\n",
    "DPO_PATH = f\"{PREFERENCE_PATH}/dpo_data.json\"\n",
    "with open(DPO_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dpo_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ DPO data saved: {DPO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(dpo_data, test_size=0.1, random_state=42)\n",
    "\n",
    "with open(f\"{PREFERENCE_PATH}/dpo_train.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f\"{PREFERENCE_PATH}/dpo_val.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Train: {len(train_data)} pairs\")\n",
    "print(f\"Val: {len(val_data)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del generator_model, generator_tokenizer\n",
    "del reward_model, reward_tokenizer\n",
    "del batch_generator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Complete!\n",
    "\n",
    "### TRUE OPTIMIZED VERSION:\n",
    "- **Real batch processing**: 4 samples at once (A100)\n",
    "- **Timeout protection**: 60s per generation batch\n",
    "- **Auto-recovery**: Skips timed-out batches\n",
    "- **Expected runtime**: 2-3 hours (A100)\n",
    "\n",
    "### Performance:\n",
    "- **Speed**: 3-4x faster than STABLE (if successful)\n",
    "- **Success rate**: 80-90% (with timeout handling)\n",
    "- **Fallback**: Use STABLE if >5 timeouts\n",
    "\n",
    "### Innovation:\n",
    "- Process-level timeout detection prevents infinite hangs\n",
    "- Graceful degradation on timeout\n",
    "- Real batch efficiency with safety\n",
    "\n",
    "### Next Steps:\n",
    "1. `05_sft_training.ipynb`\n",
    "2. `06_dpo_training.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

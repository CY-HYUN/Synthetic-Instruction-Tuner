{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Supervised Fine-Tuning (SFT)\n",
    "## Synthetic Instruction Tuner - Week 3 Day 1-3\n",
    "\n",
    "This notebook performs supervised fine-tuning on the filtered instruction dataset:\n",
    "1. Load base model (Llama-3.2-3B/Mistral-7B/Qwen2.5-3B)\n",
    "2. Prepare SFT training data\n",
    "3. Configure LoRA for parameter-efficient fine-tuning\n",
    "4. Train with TRL's SFTTrainer\n",
    "5. Evaluate and save the fine-tuned model\n",
    "\n",
    "**Training settings**:\n",
    "- LoRA: r=8, alpha=16\n",
    "- Epochs: 3\n",
    "- Learning rate: 2e-4\n",
    "- Batch size: 4 (with gradient accumulation)\n",
    "\n",
    "**Expected runtime**: 6-10 hours on Colab T4\n",
    "\n",
    "**Tip**: Monitor training loss and save checkpoints regularly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Project path\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install libraries with latest compatible versions (avoid dependency conflicts)\n!pip install -q --upgrade transformers>=4.41.0 peft>=0.7.0 trl>=0.7.4 datasets>=2.16.0 accelerate>=0.25.0 bitsandbytes>=0.41.3\n\nprint(\"✅ Libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom datasets import Dataset\nimport gc\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "source": "# Efficiency Metrics Tracker for Method Comparison\nclass EfficiencyTracker:\n    \"\"\"Track efficiency metrics for adaptation method comparison.\"\"\"\n    \n    def __init__(self, method_name: str):\n        self.method_name = method_name\n        self.metrics = {\n            \"method\": method_name,\n            \"memory_allocated_gb\": [],\n            \"memory_reserved_gb\": [],\n            \"training_time_seconds\": 0,\n            \"trainable_params\": 0,\n            \"total_params\": 0,\n            \"trainable_ratio\": 0,\n            \"inference_tokens_per_sec\": 0,\n        }\n        self.start_time = None\n    \n    def log_memory(self):\n        \"\"\"Log current GPU memory usage.\"\"\"\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated() / 1e9\n            reserved = torch.cuda.memory_reserved() / 1e9\n            self.metrics[\"memory_allocated_gb\"].append(allocated)\n            self.metrics[\"memory_reserved_gb\"].append(reserved)\n            return {\"allocated\": allocated, \"reserved\": reserved}\n        return None\n    \n    def start_training(self):\n        \"\"\"Start timing training.\"\"\"\n        self.start_time = time.time()\n        self.log_memory()\n    \n    def end_training(self):\n        \"\"\"End timing training.\"\"\"\n        if self.start_time:\n            self.metrics[\"training_time_seconds\"] = time.time() - self.start_time\n        self.log_memory()\n    \n    def log_params(self, model):\n        \"\"\"Log parameter counts.\"\"\"\n        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in model.parameters())\n        self.metrics[\"trainable_params\"] = trainable\n        self.metrics[\"total_params\"] = total\n        self.metrics[\"trainable_ratio\"] = trainable / total if total > 0 else 0\n    \n    def log_inference_speed(self, tokens_generated: int, time_taken: float):\n        \"\"\"Log inference speed.\"\"\"\n        self.metrics[\"inference_tokens_per_sec\"] = tokens_generated / time_taken if time_taken > 0 else 0\n    \n    def get_summary(self):\n        \"\"\"Get summary metrics.\"\"\"\n        summary = {\n            \"method\": self.method_name,\n            \"trainable_params\": self.metrics[\"trainable_params\"],\n            \"total_params\": self.metrics[\"total_params\"],\n            \"trainable_ratio_percent\": self.metrics[\"trainable_ratio\"] * 100,\n            \"peak_memory_gb\": max(self.metrics[\"memory_allocated_gb\"]) if self.metrics[\"memory_allocated_gb\"] else 0,\n            \"training_time_hours\": self.metrics[\"training_time_seconds\"] / 3600,\n            \"inference_tokens_per_sec\": self.metrics[\"inference_tokens_per_sec\"],\n        }\n        return summary\n    \n    def save(self, path: str):\n        \"\"\"Save metrics to JSON.\"\"\"\n        with open(path, 'w') as f:\n            json.dump(self.get_summary(), f, indent=2)\n        print(f\"Metrics saved to {path}\")\n\n# Initialize tracker for LoRA\ntracker = EfficiencyTracker(\"lora_r8\")\nprint(\"Efficiency tracker initialized for LoRA!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SFT training data\n",
    "TRAIN_PATH = f\"{config['paths']['data_filtered']}/sft_train.json\"\n",
    "VAL_PATH = f\"{config['paths']['data_filtered']}/sft_val.json\"\n",
    "\n",
    "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(VAL_PATH, 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data format\n",
    "print(\"Sample training data:\")\n",
    "print(json.dumps(train_data[0], indent=2, ensure_ascii=False)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Val dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Choose your base model\n",
    "# Options: meta-llama/Llama-3.2-3B, mistralai/Mistral-7B-v0.1, Qwen/Qwen2.5-3B\n",
    "BASE_MODEL_ID = config['models']['sft_base']\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Quantization config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # Important for training\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Enable gradient checkpointing for memory efficiency\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Log memory after model load\nmem = tracker.log_memory()\nprint(f\"Model loaded!\")\nprint(f\"GPU Memory: {mem['allocated']:.2f} GB allocated, {mem['reserved']:.2f} GB reserved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=config['training']['lora_r'],\n",
    "    lora_alpha=config['training']['lora_alpha'],\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=config['training']['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"LoRA config:\")\n",
    "print(f\"  r: {lora_config.r}\")\n",
    "print(f\"  alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  target_modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply LoRA to model\nmodel = get_peft_model(model, lora_config)\n\n# Log parameters for efficiency comparison\ntracker.log_params(model)\n\n# Print trainable parameters\nprint(f\"\\nTrainable parameters: {tracker.metrics['trainable_params']:,} ({tracker.metrics['trainable_ratio']*100:.2f}%)\")\nprint(f\"Total parameters: {tracker.metrics['total_params']:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"\n",
    "    Format instruction-response pair into a prompt for training.\n",
    "    Uses Llama 3 chat template format.\n",
    "    \"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    response = sample[\"output\"]\n",
    "    \n",
    "    # Llama 3 format\n",
    "    text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Test formatting\n",
    "sample_formatted = format_instruction(train_data[0])\n",
    "print(\"Sample formatted text:\")\n",
    "print(sample_formatted[\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply formatting to datasets\n",
    "train_dataset = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(format_instruction, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"Formatted train dataset: {train_dataset}\")\n",
    "print(f\"Formatted val dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"{config['paths']['models_sft']}/sft-checkpoint\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=config['training']['sft_epochs'],\n",
    "    per_device_train_batch_size=config['training']['sft_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['sft_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=config['training']['sft_learning_rate'],\n",
    "    weight_decay=0.01,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Total steps: ~{len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"SFT Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start training with timing\nprint(\"Starting SFT training...\")\nprint(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 50)\n\ntracker.start_training()\ntrain_result = trainer.train()\ntracker.end_training()\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Training completed!\")\nprint(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Training time: {tracker.metrics['training_time_seconds']/3600:.2f} hours\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"\\nTraining metrics:\")\n",
    "print(f\"  Final train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_response(instruction: str, max_new_tokens: int = 256):\n    \"\"\"Generate a response for the given instruction.\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        start_time = time.time()\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        gen_time = time.time() - start_time\n    \n    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]\n    \n    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    \n    # Extract response\n    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated:\n        response = generated.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n        response = response.split(\"<|eot_id|>\")[0].strip()\n        return response, tokens_generated, gen_time\n    \n    return generated, tokens_generated, gen_time\n\nprint(\"Generation function defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test generation and measure inference speed\ntest_instructions = [\n    \"Explain the concept of machine learning in simple terms.\",\n    \"Write a Python function to calculate the factorial of a number.\",\n    \"What are the main differences between supervised and unsupervised learning?\",\n]\n\nprint(\"Testing fine-tuned model generation:\")\nprint(\"=\" * 50)\n\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, instruction in enumerate(test_instructions):\n    print(f\"\\n[Test {i+1}]\")\n    print(f\"Instruction: {instruction}\")\n    print(f\"\\nResponse:\")\n    response, tokens, gen_time = generate_response(instruction, max_new_tokens=200)\n    print(response)\n    print(f\"\\nTokens: {tokens}, Time: {gen_time:.2f}s, Speed: {tokens/gen_time:.1f} tok/s\")\n    print(\"-\" * 50)\n    \n    total_tokens += tokens\n    total_time += gen_time\n\n# Log inference speed\ntracker.log_inference_speed(total_tokens, total_time)\nprint(f\"\\nAverage inference speed: {tracker.metrics['inference_tokens_per_sec']:.1f} tokens/sec\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "FINAL_MODEL_DIR = f\"{config['paths']['models_sft']}/final\"\n",
    "\n",
    "print(f\"Saving final model to: {FINAL_MODEL_DIR}\")\n",
    "\n",
    "trainer.save_model(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"base_model\": BASE_MODEL_ID,\n",
    "    \"training_data_size\": len(train_data),\n",
    "    \"validation_data_size\": len(val_data),\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"alpha\": lora_config.lora_alpha,\n",
    "        \"dropout\": lora_config.lora_dropout,\n",
    "        \"target_modules\": lora_config.target_modules,\n",
    "    },\n",
    "    \"training_args\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"train_loss\": train_result.training_loss,\n",
    "        \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "        \"total_steps\": train_result.global_step,\n",
    "    },\n",
    "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "\n",
    "config_path = f\"{FINAL_MODEL_DIR}/training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"Training config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load training logs\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract train and eval losses\n",
    "train_logs = [log for log in log_history if 'loss' in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "if train_logs:\n",
    "    steps = [log['step'] for log in train_logs]\n",
    "    losses = [log['loss'] for log in train_logs]\n",
    "    axes[0].plot(steps, losses, label='Train Loss')\n",
    "    axes[0].set_xlabel('Steps')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Evaluation loss\n",
    "if eval_logs:\n",
    "    steps = [log['step'] for log in eval_logs]\n",
    "    losses = [log['eval_loss'] for log in eval_logs]\n",
    "    axes[1].plot(steps, losses, label='Eval Loss', color='orange')\n",
    "    axes[1].set_xlabel('Steps')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Evaluation Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['paths']['evaluation_figures']}/sft_training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to {config['paths']['evaluation_figures']}/sft_training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Save efficiency metrics for comparison\nMETRICS_DIR = f\"{PROJECT_ROOT}/evaluation/metrics\"\nos.makedirs(METRICS_DIR, exist_ok=True)\n\n# Get summary\nsummary = tracker.get_summary()\n\n# Add training results\nsummary[\"train_loss\"] = train_result.training_loss\nsummary[\"eval_loss\"] = eval_results[\"eval_loss\"]\n\n# Save\ntracker.save(f\"{METRICS_DIR}/lora_metrics.json\")\n\nprint(\"\\n=== LoRA Efficiency Metrics Summary ===\")\nfor key, value in summary.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.4f}\")\n    else:\n        print(f\"  {key}: {value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✓ SFT Training Complete!\n",
    "\n",
    "### Summary:\n",
    "- Fine-tuned model saved to `models/sft/final/`\n",
    "- LoRA adapters trained with parameter-efficient fine-tuning\n",
    "- Training configuration and metrics saved\n",
    "\n",
    "### Model Loading:\n",
    "To load the fine-tuned model later:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID)\n",
    "model = PeftModel.from_pretrained(base_model, FINAL_MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Proceed to `06_dpo_training.ipynb` for DPO alignment\n",
    "2. This will further improve the model using preference data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om035uiSLcpO"
      },
      "source": [
        "# 01. Environment Setup\n",
        "## Synthetic Instruction Tuner - Week 1 Day 1-2\n",
        "\n",
        "This notebook sets up the environment for the entire pipeline:\n",
        "1. GPU verification\n",
        "2. Library installation\n",
        "3. Hugging Face authentication\n",
        "4. Model loading test\n",
        "5. Project structure setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHDvIV-MLcpf"
      },
      "source": [
        "## 1. GPU Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7OgPpHkLcpj",
        "outputId": "d6d1ed9d-aba9-45aa-f1cb-e77bc866ec88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 16 18:53:04 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5-gaNZLLcpo",
        "outputId": "7587364e-c868-4b85-ecbf-25c47e68eeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03s9wOztLcpp"
      },
      "source": [
        "## 2. Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_zS4zgXeLcpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0378b28-a65f-431b-f0af-1ccf931c0ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "All libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install core libraries with latest compatible versions\n",
        "!pip install -q --upgrade transformers>=4.41.0\n",
        "!pip install -q --upgrade peft>=0.7.0\n",
        "!pip install -q --upgrade trl>=0.7.4\n",
        "!pip install -q --upgrade datasets>=2.16.0\n",
        "!pip install -q --upgrade accelerate>=0.25.0\n",
        "!pip install -q --upgrade bitsandbytes>=0.41.3\n",
        "!pip install -q sentencepiece protobuf\n",
        "\n",
        "# Install evaluation library\n",
        "!pip install -q lm-eval\n",
        "\n",
        "# Install utilities\n",
        "!pip install -q jsonlines huggingface_hub\n",
        "\n",
        "print(\"All libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installations and check versions\n",
        "import transformers\n",
        "import peft\n",
        "import trl\n",
        "import datasets\n",
        "import accelerate\n",
        "\n",
        "print(\"Installed versions:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"peft: {peft.__version__}\")\n",
        "print(f\"trl: {trl.__version__}\")\n",
        "print(f\"datasets: {datasets.__version__}\")\n",
        "print(f\"accelerate: {accelerate.__version__}\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\n✅ All libraries installed and verified!\")"
      ],
      "metadata": {
        "id": "lsWPj8ZDc08I",
        "outputId": "bff6df35-56ed-45bc-a79e-74dd9831dc21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed versions:\n",
            "==================================================\n",
            "transformers: 4.57.3\n",
            "peft: 0.18.0\n",
            "trl: 0.26.1\n",
            "datasets: 4.4.1\n",
            "accelerate: 1.12.0\n",
            "==================================================\n",
            "\n",
            "✅ All libraries installed and verified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-KcjuCALcps",
        "outputId": "1721c649-ee28-4a76-e5dd-40692a0e66c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers: 4.57.3\n",
            "peft: 0.18.0\n",
            "trl: 0.26.1\n",
            "datasets: 4.4.1\n",
            "accelerate: 1.12.0\n"
          ]
        }
      ],
      "source": [
        "# Verify installations\n",
        "import transformers\n",
        "import peft\n",
        "import trl\n",
        "import datasets\n",
        "import accelerate\n",
        "\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"peft: {peft.__version__}\")\n",
        "print(f\"trl: {trl.__version__}\")\n",
        "print(f\"datasets: {datasets.__version__}\")\n",
        "print(f\"accelerate: {accelerate.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlBNBQHtLcpv"
      },
      "source": [
        "## 3. Google Drive Mount (for Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4LRvUVgLcpw",
        "outputId": "94da3189-73a0-4c0d-ddb8-68458116a0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk9B4q3DLcpy",
        "outputId": "3efe2950-dc72-4d0f-c8d1-ca54d0ea924b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/data/raw\n",
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/data/filtered\n",
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/data/preference\n",
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/models/sft\n",
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/models/dpo\n",
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/evaluation/results\n",
            "Created: /content/drive/MyDrive/synthetic-instruction-tuner/evaluation/figures\n",
            "\n",
            "Project structure ready!\n"
          ]
        }
      ],
      "source": [
        "# Set project paths\n",
        "import os\n",
        "\n",
        "# Change this to your Google Drive path\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\"\n",
        "\n",
        "# Create project directories if they don't exist\n",
        "directories = [\n",
        "    f\"{PROJECT_ROOT}/data/raw\",\n",
        "    f\"{PROJECT_ROOT}/data/filtered\",\n",
        "    f\"{PROJECT_ROOT}/data/preference\",\n",
        "    f\"{PROJECT_ROOT}/models/sft\",\n",
        "    f\"{PROJECT_ROOT}/models/dpo\",\n",
        "    f\"{PROJECT_ROOT}/evaluation/results\",\n",
        "    f\"{PROJECT_ROOT}/evaluation/figures\",\n",
        "]\n",
        "\n",
        "for dir_path in directories:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    print(f\"Created: {dir_path}\")\n",
        "\n",
        "print(\"\\nProject structure ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPRtTJO6Lcp3"
      },
      "source": [
        "## 4. Hugging Face Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHUyIKSdLcp5",
        "outputId": "b0d548b8-9873-4e50-eaf1-4a71225fa13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as: Changyong3\n",
            "Email: N/A\n"
          ]
        }
      ],
      "source": [
        "# Verify login\n",
        "from huggingface_hub import whoami\n",
        "\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"Logged in as: {user_info['name']}\")\n",
        "    print(f\"Email: {user_info.get('email', 'N/A')}\")\n",
        "except Exception as e:\n",
        "    print(f\"Login failed: {e}\")\n",
        "    print(\"Please run the login cell again with a valid token.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPvuFzxcLcp7",
        "outputId": "735810fd-337c-42af-c1ee-30a4d76d5d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta-llama/Llama-3.1-8B-Instruct...\n",
            "This may take a few minutes on first download.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Model ID for data generation\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# 4-bit quantization config for memory efficiency\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "print(\"This may take a few minutes on first download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCS3C3RdLcp8",
        "outputId": "4b023cd7-bfb1-40ea-a2d0-006f735117ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded. Vocab size: 128000\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d3040bc38827431686d2d7d75a36039e",
            "7b06709745324ef88219675a155526e3",
            "0eafcc08df27472fbf566a5be681fc84",
            "c657956a4f724ecf883a1797517da9a5",
            "4f32c8f399ae496fa0af046ede9b39f8",
            "a9b30a4b70854e98b62b7f9d69bd620e",
            "d45eccaf5bbe41019d3fd36697512b0e",
            "db3835a5cc01425aba1723780b5e1a52",
            "b79b96cab56c487fa1b658d80de23d8f",
            "801a6cdf55b945038c7404a57a325cab",
            "54982c10b9e54f838f7b37823c2a52a1"
          ]
        },
        "id": "j1-nzU_3Lcp9",
        "outputId": "d46f5590-182c-495d-cfec-914344be45d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3040bc38827431686d2d7d75a36039e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "Model dtype: torch.float16\n",
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n",
        "print(f\"Device: {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tBAiLTULcp9",
        "outputId": "f49e0a48-24b0-457e-afe2-f848ad2b816c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory Allocated: 9.31 GB\n",
            "GPU Memory Reserved: 12.69 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"GPU Memory Reserved: {reserved:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0HK4bqsLcp-"
      },
      "source": [
        "## 6. Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HollHuHHLcp-",
        "outputId": "c31febcb-5003-4433-c6d0-3e9622be1632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input format:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is machine learning?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Simple inference test\n",
        "test_prompt = \"What is machine learning?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": test_prompt}\n",
        "]\n",
        "\n",
        "# Format with chat template\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(\"Input format:\")\n",
        "print(input_text)\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziXsg4_oLcp-",
        "outputId": "343079d1-c3bc-4871-9548-ac67d263261e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "user\n",
            "\n",
            "What is machine learning?assistant\n",
            "\n",
            "Machine learning is a subset of artificial intelligence (AI) that involves the use of algorithms and statistical models to enable computers to learn from data, make decisions, and improve their performance on a task without being explicitly programmed. In other words, machine learning allows computers to learn from experience and improve their performance over time.\n",
            "\n",
            "Machine learning involves training algorithms on large datasets to identify patterns, relationships, and trends, and to make predictions or decisions based on that analysis. The goal of machine learning is to enable computers to learn from data and make accurate predictions or decisions, rather than being explicitly programmed to perform a specific task.\n",
            "\n",
            "There are several types of machine learning, including:\n",
            "\n",
            "1. **Supervised learning**: This type of machine learning involves training algorithms on labeled data, where the correct output is already known. The algorithm learns to map inputs to outputs based on the labeled data.\n",
            "2. **Unsupervised learning**: This type of machine learning involves training algorithms on unlabeled data, where the goal is to identify patterns or structure in the data.\n",
            "3. **Reinforcement learning**: This type of machine learning involves training algorithms through trial and error, where the algorithm learns to take actions to maximize a reward or minimize a penalty.\n",
            "4. **Deep learning**: This type of machine learning involves the\n"
          ]
        }
      ],
      "source": [
        "# Generate response\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXnfBHYzLcp_"
      },
      "source": [
        "## 7. Test Magpie-style Generation\n",
        "\n",
        "The Magpie method uses only the user template prefix to trigger instruction generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_K5h2SMLcp_",
        "outputId": "c631a45b-8ebc-4fa0-fdb2-8a8491cdfca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magpie template:\n",
            "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n'\n"
          ]
        }
      ],
      "source": [
        "# Magpie-style template (only the prefix, no actual prompt)\n",
        "# This triggers the model to generate an instruction\n",
        "magpie_template = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "\n",
        "print(\"Magpie template:\")\n",
        "print(repr(magpie_template))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa40UyUsLcqA",
        "outputId": "2d4d2a8d-5f9c-420f-bdd8-0f5e55479812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magpie-generated instruction:\n",
            "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "## Step 1: Understand the problem\n",
            "The problem asks us to find the sum of the squares of all the numbers from 1 to 100.\n",
            "\n",
            "## Step 2: Recall the formula for the sum of squares of the first n natural numbers\n",
            "The sum of squares of the first n natural numbers is given by the formula: 1^2 + 2^2 + 3^2 +... + n^2 = n * (n + 1) * (2n + 1) / 6.\n",
            "\n",
            "## Step 3: Apply the formula to find the sum of squares of numbers from 1 to 100\n",
            "Using the formula, we can substitute n = 100 to get the sum of squares of numbers from 1 to 100.\n",
            "\n",
            "## Step 4: Calculate the sum\n",
            "Substituting n = 100 in the formula, we get: sum = 100 * (100 + 1) * (2 * 100 + 1) / 6 = 100 * 101 * 201 / 6.\n",
            "\n",
            "## Step 5: Perform the arithmetic\n",
            "Performing the arithmetic, we get: sum = 100 * 101 * 201 = 2020200, then dividing by 6 gives\n"
          ]
        }
      ],
      "source": [
        "# Generate instruction using Magpie method\n",
        "inputs = tokenizer(magpie_template, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.9,  # Higher temperature for diversity\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "print(\"Magpie-generated instruction:\")\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zidrQYTsLcqA",
        "outputId": "7b0b515f-5836-404f-941f-82c19d4349e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed instruction:\n",
            "## Step 1: Understand the problem\n",
            "The problem asks us to find the sum of the squares of all the numbers from 1 to 100.\n",
            "\n",
            "## Step 2: Recall the formula for the sum of squares of the first n natural numbers\n",
            "The sum of squares of the first n natural numbers is given by the formula: 1^2 + 2^2 + 3^2 +... + n^2 = n * (n + 1) * (2n + 1) / 6.\n",
            "\n",
            "## Step 3: Apply the formula to find the sum of squares of numbers from 1 to 100\n",
            "Using the formula, we can substitute n = 100 to get the sum of squares of numbers from 1 to 100.\n",
            "\n",
            "## Step 4: Calculate the sum\n",
            "Substituting n = 100 in the formula, we get: sum = 100 * (100 + 1) * (2 * 100 + 1) / 6 = 100 * 101 * 201 / 6.\n",
            "\n",
            "## Step 5: Perform the arithmetic\n",
            "Performing the arithmetic, we get: sum = 100 * 101 * 201 = 2020200, then dividing by 6 gives\n"
          ]
        }
      ],
      "source": [
        "# Parse the generated instruction\n",
        "def parse_magpie_output(output_text):\n",
        "    \"\"\"Extract instruction from Magpie-style generation.\"\"\"\n",
        "    # Remove the template prefix\n",
        "    if \"<|start_header_id|>user<|end_header_id|>\" in output_text:\n",
        "        parts = output_text.split(\"<|start_header_id|>user<|end_header_id|>\")\n",
        "        if len(parts) > 1:\n",
        "            instruction_part = parts[1]\n",
        "            # Extract until end of turn or assistant header\n",
        "            if \"<|eot_id|>\" in instruction_part:\n",
        "                instruction = instruction_part.split(\"<|eot_id|>\")[0].strip()\n",
        "            elif \"<|start_header_id|>assistant\" in instruction_part:\n",
        "                instruction = instruction_part.split(\"<|start_header_id|>assistant\")[0].strip()\n",
        "            else:\n",
        "                instruction = instruction_part.strip()\n",
        "            return instruction\n",
        "    return None\n",
        "\n",
        "instruction = parse_magpie_output(generated)\n",
        "print(\"Parsed instruction:\")\n",
        "print(instruction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pk79zy6LcqB"
      },
      "source": [
        "## 8. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jini_BWLcqB",
        "outputId": "8c7212d8-9700-4012-e746-a0e98b9c2bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleared!\n",
            "GPU Memory Allocated: 0.01 GB\n"
          ]
        }
      ],
      "source": [
        "# Free GPU memory\n",
        "import gc\n",
        "\n",
        "del model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Memory cleared!\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI5zJnEVLcqB"
      },
      "source": [
        "## 9. Configuration File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmBSwjlILcqC",
        "outputId": "6fce34f6-f997-46ea-9495-e1aad1088575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved to: /content/drive/MyDrive/synthetic-instruction-tuner/config.json\n"
          ]
        }
      ],
      "source": [
        "# Save project configuration\n",
        "import json\n",
        "\n",
        "config = {\n",
        "    \"project_name\": \"synthetic-instruction-tuner\",\n",
        "    \"version\": \"0.1.0\",\n",
        "\n",
        "    # Paths\n",
        "    \"paths\": {\n",
        "        \"project_root\": PROJECT_ROOT,\n",
        "        \"data_raw\": f\"{PROJECT_ROOT}/data/raw\",\n",
        "        \"data_filtered\": f\"{PROJECT_ROOT}/data/filtered\",\n",
        "        \"data_preference\": f\"{PROJECT_ROOT}/data/preference\",\n",
        "        \"models_sft\": f\"{PROJECT_ROOT}/models/sft\",\n",
        "        \"models_dpo\": f\"{PROJECT_ROOT}/models/dpo\",\n",
        "        \"evaluation_results\": f\"{PROJECT_ROOT}/evaluation/results\",\n",
        "        \"evaluation_figures\": f\"{PROJECT_ROOT}/evaluation/figures\"\n",
        "    },\n",
        "\n",
        "    # Models\n",
        "    \"models\": {\n",
        "        \"data_generation\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"fine_tuning_targets\": [\n",
        "            \"meta-llama/Llama-3.2-3B\",\n",
        "            \"mistralai/Mistral-7B-v0.1\",\n",
        "            \"Qwen/Qwen2.5-3B\"\n",
        "        ],\n",
        "        \"reward_model\": \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
        "    },\n",
        "\n",
        "    # Data generation settings (optimized for Colab free tier)\n",
        "    \"data_generation\": {\n",
        "        \"target_raw_samples\": 1500,\n",
        "        \"target_filtered_samples\": 1000,\n",
        "        \"checkpoint_interval\": 20,\n",
        "        \"temperature\": 0.9,\n",
        "        \"max_new_tokens\": 512\n",
        "    },\n",
        "\n",
        "    # Quality filtering settings\n",
        "    \"filtering\": {\n",
        "        \"min_words\": 20,\n",
        "        \"max_words\": 500,\n",
        "        \"jaccard_threshold\": 0.8,\n",
        "        \"refusal_keywords\": [\n",
        "            \"I'm an AI\", \"I cannot\", \"I don't have\",\n",
        "            \"As an AI\", \"I'm not able\", \"I apologize\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # LoRA settings\n",
        "    \"lora\": {\n",
        "        \"r\": 8,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        \"lora_dropout\": 0.05\n",
        "    },\n",
        "\n",
        "    # SFT settings\n",
        "    \"sft\": {\n",
        "        \"num_epochs\": 3,\n",
        "        \"batch_size\": 4,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"warmup_ratio\": 0.03,\n",
        "        \"save_steps\": 500\n",
        "    },\n",
        "\n",
        "    # DPO settings\n",
        "    \"dpo\": {\n",
        "        \"beta\": 0.1,\n",
        "        \"num_epochs\": 1,\n",
        "        \"batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 8,\n",
        "        \"learning_rate\": 5e-5\n",
        "    },\n",
        "\n",
        "    # Evaluation benchmarks\n",
        "    \"evaluation\": {\n",
        "        \"benchmarks\": [\"ifeval\", \"mmlu\", \"truthfulqa\"],\n",
        "        \"batch_size\": 4\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save config\n",
        "config_path = f\"{PROJECT_ROOT}/config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"Configuration saved to: {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oAssvCKLcqF"
      },
      "source": [
        "## ✅ Setup Complete!\n",
        "\n",
        "### Checklist:\n",
        "- [x] GPU verified (T4 16GB)\n",
        "- [x] Libraries installed\n",
        "- [x] Google Drive mounted\n",
        "- [x] Hugging Face authenticated\n",
        "- [x] Model loading tested\n",
        "- [x] Magpie-style generation tested\n",
        "- [x] Configuration saved\n",
        "\n",
        "### Next Steps:\n",
        "1. Proceed to `02_magpie_generation.ipynb` for data generation\n",
        "2. Run across 3 days for ~16-17 hours total to generate 1,500 samples (optimized for Colab free tier)\n",
        "3. Follow the checkpoint-based generation strategy to handle Colab's 12h runtime limit"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3040bc38827431686d2d7d75a36039e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b06709745324ef88219675a155526e3",
              "IPY_MODEL_0eafcc08df27472fbf566a5be681fc84",
              "IPY_MODEL_c657956a4f724ecf883a1797517da9a5"
            ],
            "layout": "IPY_MODEL_4f32c8f399ae496fa0af046ede9b39f8"
          }
        },
        "7b06709745324ef88219675a155526e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9b30a4b70854e98b62b7f9d69bd620e",
            "placeholder": "​",
            "style": "IPY_MODEL_d45eccaf5bbe41019d3fd36697512b0e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0eafcc08df27472fbf566a5be681fc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db3835a5cc01425aba1723780b5e1a52",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b79b96cab56c487fa1b658d80de23d8f",
            "value": 4
          }
        },
        "c657956a4f724ecf883a1797517da9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801a6cdf55b945038c7404a57a325cab",
            "placeholder": "​",
            "style": "IPY_MODEL_54982c10b9e54f838f7b37823c2a52a1",
            "value": " 4/4 [01:34&lt;00:00, 19.08s/it]"
          }
        },
        "4f32c8f399ae496fa0af046ede9b39f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b30a4b70854e98b62b7f9d69bd620e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45eccaf5bbe41019d3fd36697512b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db3835a5cc01425aba1723780b5e1a52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79b96cab56c487fa1b658d80de23d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "801a6cdf55b945038c7404a57a325cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54982c10b9e54f838f7b37823c2a52a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
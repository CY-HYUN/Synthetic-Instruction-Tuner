{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Environment Setup\n",
    "## Synthetic Instruction Tuner - Week 1 Day 1-2\n",
    "\n",
    "This notebook sets up the environment for the entire pipeline:\n",
    "1. GPU verification\n",
    "2. Library installation\n",
    "3. Hugging Face authentication\n",
    "4. Model loading test\n",
    "5. Project structure setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "!pip install -q transformers==4.36.0\n",
    "!pip install -q peft==0.7.0\n",
    "!pip install -q trl==0.7.4\n",
    "!pip install -q datasets==2.16.0\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q bitsandbytes==0.41.3\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q protobuf\n",
    "\n",
    "# Install evaluation library\n",
    "!pip install -q lm-eval==0.4.0\n",
    "\n",
    "# Install utilities\n",
    "!pip install -q jsonlines\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "import accelerate\n",
    "\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"trl: {trl.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "print(f\"accelerate: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Google Drive Mount (for Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project paths\n",
    "import os\n",
    "\n",
    "# Change this to your Google Drive path\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\"\n",
    "\n",
    "# Create project directories if they don't exist\n",
    "directories = [\n",
    "    f\"{PROJECT_ROOT}/data/raw\",\n",
    "    f\"{PROJECT_ROOT}/data/filtered\",\n",
    "    f\"{PROJECT_ROOT}/data/preference\",\n",
    "    f\"{PROJECT_ROOT}/models/sft\",\n",
    "    f\"{PROJECT_ROOT}/models/dpo\",\n",
    "    f\"{PROJECT_ROOT}/evaluation/results\",\n",
    "    f\"{PROJECT_ROOT}/evaluation/figures\",\n",
    "]\n",
    "\n",
    "for dir_path in directories:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"Created: {dir_path}\")\n",
    "\n",
    "print(\"\\nProject structure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Login to Hugging Face\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "# Make sure you have accepted Llama 3.1 and 3.2 licenses\n",
    "\n",
    "login()  # This will prompt for your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify login\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "    print(f\"Email: {user_info.get('email', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Login failed: {e}\")\n",
    "    print(\"Please run the login cell again with a valid token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Model Loading (Llama-3.1-8B-Instruct)\n",
    "\n",
    "**Important**: Before running this, make sure you have:\n",
    "1. Accepted the Llama 3.1 license at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "2. Logged in to Hugging Face (cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Model ID for data generation\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 4-bit quantization config for memory efficiency\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "print(\"This may take a few minutes on first download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference test\n",
    "test_prompt = \"What is machine learning?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Input format:\")\n",
    "print(input_text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Magpie-style Generation\n",
    "\n",
    "The Magpie method uses only the user template prefix to trigger instruction generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magpie-style template (only the prefix, no actual prompt)\n",
    "# This triggers the model to generate an instruction\n",
    "magpie_template = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "\n",
    "print(\"Magpie template:\")\n",
    "print(repr(magpie_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate instruction using Magpie method\n",
    "inputs = tokenizer(magpie_template, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.9,  # Higher temperature for diversity\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"Magpie-generated instruction:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the generated instruction\n",
    "def parse_magpie_output(output_text):\n",
    "    \"\"\"Extract instruction from Magpie-style generation.\"\"\"\n",
    "    # Remove the template prefix\n",
    "    if \"<|start_header_id|>user<|end_header_id|>\" in output_text:\n",
    "        parts = output_text.split(\"<|start_header_id|>user<|end_header_id|>\")\n",
    "        if len(parts) > 1:\n",
    "            instruction_part = parts[1]\n",
    "            # Extract until end of turn or assistant header\n",
    "            if \"<|eot_id|>\" in instruction_part:\n",
    "                instruction = instruction_part.split(\"<|eot_id|>\")[0].strip()\n",
    "            elif \"<|start_header_id|>assistant\" in instruction_part:\n",
    "                instruction = instruction_part.split(\"<|start_header_id|>assistant\")[0].strip()\n",
    "            else:\n",
    "                instruction = instruction_part.strip()\n",
    "            return instruction\n",
    "    return None\n",
    "\n",
    "instruction = parse_magpie_output(generated)\n",
    "print(\"Parsed instruction:\")\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "import gc\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save project configuration\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    \"project_name\": \"synthetic-instruction-tuner\",\n",
    "    \"version\": \"0.1.0\",\n",
    "    \n",
    "    # Paths\n",
    "    \"paths\": {\n",
    "        \"project_root\": PROJECT_ROOT,\n",
    "        \"data_raw\": f\"{PROJECT_ROOT}/data/raw\",\n",
    "        \"data_filtered\": f\"{PROJECT_ROOT}/data/filtered\",\n",
    "        \"data_preference\": f\"{PROJECT_ROOT}/data/preference\",\n",
    "        \"models_sft\": f\"{PROJECT_ROOT}/models/sft\",\n",
    "        \"models_dpo\": f\"{PROJECT_ROOT}/models/dpo\",\n",
    "        \"evaluation_results\": f\"{PROJECT_ROOT}/evaluation/results\",\n",
    "        \"evaluation_figures\": f\"{PROJECT_ROOT}/evaluation/figures\"\n",
    "    },\n",
    "    \n",
    "    # Models\n",
    "    \"models\": {\n",
    "        \"data_generation\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"fine_tuning_targets\": [\n",
    "            \"meta-llama/Llama-3.2-3B\",\n",
    "            \"mistralai/Mistral-7B-v0.1\",\n",
    "            \"Qwen/Qwen2.5-3B\"\n",
    "        ],\n",
    "        \"reward_model\": \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "    },\n",
    "    \n",
    "    # Data generation settings\n",
    "    \"data_generation\": {\n",
    "        \"target_raw_samples\": 15000,\n",
    "        \"target_filtered_samples\": 10000,\n",
    "        \"checkpoint_interval\": 1000,\n",
    "        \"temperature\": 0.9,\n",
    "        \"max_new_tokens\": 512\n",
    "    },\n",
    "    \n",
    "    # Quality filtering settings\n",
    "    \"filtering\": {\n",
    "        \"min_words\": 20,\n",
    "        \"max_words\": 500,\n",
    "        \"jaccard_threshold\": 0.8,\n",
    "        \"refusal_keywords\": [\n",
    "            \"I'm an AI\", \"I cannot\", \"I don't have\",\n",
    "            \"As an AI\", \"I'm not able\", \"I apologize\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # LoRA settings\n",
    "    \"lora\": {\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"lora_dropout\": 0.05\n",
    "    },\n",
    "    \n",
    "    # SFT settings\n",
    "    \"sft\": {\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"save_steps\": 500\n",
    "    },\n",
    "    \n",
    "    # DPO settings\n",
    "    \"dpo\": {\n",
    "        \"beta\": 0.1,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"learning_rate\": 5e-5\n",
    "    },\n",
    "    \n",
    "    # Evaluation benchmarks\n",
    "    \"evaluation\": {\n",
    "        \"benchmarks\": [\"ifeval\", \"mmlu\", \"truthfulqa\"],\n",
    "        \"batch_size\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = f\"{PROJECT_ROOT}/config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Setup Complete!\n",
    "\n",
    "### Checklist:\n",
    "- [x] GPU verified (T4 16GB)\n",
    "- [x] Libraries installed\n",
    "- [x] Google Drive mounted\n",
    "- [x] Hugging Face authenticated\n",
    "- [x] Model loading tested\n",
    "- [x] Magpie-style generation tested\n",
    "- [x] Configuration saved\n",
    "\n",
    "### Next Steps:\n",
    "1. Proceed to `02_magpie_generation.ipynb` for data generation\n",
    "2. Run overnight for ~8-10 hours to generate 15,000 samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

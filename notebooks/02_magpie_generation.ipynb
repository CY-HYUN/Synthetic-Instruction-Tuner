{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Magpie Data Generation\n",
    "## Synthetic Instruction Tuner - Week 1 Day 3-5\n",
    "\n",
    "This notebook generates synthetic instruction-response pairs using the Magpie method:\n",
    "1. Load Llama-3.1-8B-Instruct model\n",
    "2. Generate instructions using template-only prompts\n",
    "3. Generate responses for each instruction\n",
    "4. Save checkpoints periodically\n",
    "\n",
    "**Expected runtime**: 8-10 hours for 15,000 samples\n",
    "\n",
    "**Tip**: Run this overnight and use checkpoints to resume if disconnected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Project path\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Target samples: {config['data_generation']['target_raw_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if needed\n",
    "!pip install -q transformers==4.36.0 accelerate bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face login\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = config['models']['data_generation']\n",
    "\n",
    "# 4-bit quantization for memory efficiency\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Magpie Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagpieGenerator:\n",
    "    \"\"\"Generate synthetic instructions and responses using Magpie method.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        \n",
    "        # Magpie template (Llama 3.1 format)\n",
    "        self.instruction_template = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        self.response_template = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \n",
    "        # Generation settings\n",
    "        self.temperature = config['data_generation']['temperature']\n",
    "        self.max_new_tokens = config['data_generation']['max_new_tokens']\n",
    "    \n",
    "    def generate_instruction(self):\n",
    "        \"\"\"Generate a single instruction using Magpie method.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            self.instruction_template, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=self.temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=[\n",
    "                    self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "                    self.tokenizer.eos_token_id\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        instruction = self._parse_instruction(generated)\n",
    "        return instruction\n",
    "    \n",
    "    def generate_response(self, instruction):\n",
    "        \"\"\"Generate a response for the given instruction.\"\"\"\n",
    "        # Format as conversation\n",
    "        prompt = f\"{self.instruction_template}{instruction}{self.response_template}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=0.7,  # Lower temperature for more focused responses\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=[\n",
    "                    self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "                    self.tokenizer.eos_token_id\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        response = self._parse_response(generated)\n",
    "        return response\n",
    "    \n",
    "    def _parse_instruction(self, text):\n",
    "        \"\"\"Extract instruction from generated text.\"\"\"\n",
    "        try:\n",
    "            # Find content after user header\n",
    "            if \"<|start_header_id|>user<|end_header_id|>\" in text:\n",
    "                parts = text.split(\"<|start_header_id|>user<|end_header_id|>\")\n",
    "                if len(parts) > 1:\n",
    "                    instruction = parts[1]\n",
    "                    # Remove end tokens\n",
    "                    for end_token in [\"<|eot_id|>\", \"<|start_header_id|>assistant\"]:\n",
    "                        if end_token in instruction:\n",
    "                            instruction = instruction.split(end_token)[0]\n",
    "                    return instruction.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _parse_response(self, text):\n",
    "        \"\"\"Extract response from generated text.\"\"\"\n",
    "        try:\n",
    "            # Find content after assistant header\n",
    "            if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
    "                parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "                if len(parts) > 1:\n",
    "                    response = parts[-1]  # Get the last assistant response\n",
    "                    # Remove end tokens\n",
    "                    for end_token in [\"<|eot_id|>\", \"<|end_of_text|>\"]:\n",
    "                        if end_token in response:\n",
    "                            response = response.split(end_token)[0]\n",
    "                    return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def generate_pair(self):\n",
    "        \"\"\"Generate a single instruction-response pair.\"\"\"\n",
    "        instruction = self.generate_instruction()\n",
    "        if instruction and len(instruction) > 10:\n",
    "            response = self.generate_response(instruction)\n",
    "            if response and len(response) > 10:\n",
    "                return {\n",
    "                    \"instruction\": instruction,\n",
    "                    \"response\": response\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# Initialize generator\n",
    "generator = MagpieGenerator(model, tokenizer, config)\n",
    "print(\"Generator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single generation\n",
    "print(\"Testing single generation...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_pair = generator.generate_pair()\n",
    "if test_pair:\n",
    "    print(f\"Instruction:\\n{test_pair['instruction']}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(f\"Response:\\n{test_pair['response']}\")\n",
    "else:\n",
    "    print(\"Generation failed. Check model and templates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple generations\n",
    "print(\"Testing 5 generations...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(5):\n",
    "    pair = generator.generate_pair()\n",
    "    if pair:\n",
    "        print(f\"\\n[{i+1}] Instruction: {pair['instruction'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"[{i+1}] Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Generation with Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, checkpoint_path):\n",
    "    \"\"\"Save data to checkpoint file.\"\"\"\n",
    "    with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Checkpoint saved: {len(data)} samples\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load data from checkpoint file.\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = config['paths']['data_raw']\n",
    "CHECKPOINT_PATH = f\"{DATA_PATH}/instructions_checkpoint.json\"\n",
    "FINAL_PATH = f\"{DATA_PATH}/instructions_raw.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation settings\n",
    "TARGET_SAMPLES = config['data_generation']['target_raw_samples']\n",
    "CHECKPOINT_INTERVAL = config['data_generation']['checkpoint_interval']\n",
    "\n",
    "print(f\"Target samples: {TARGET_SAMPLES}\")\n",
    "print(f\"Checkpoint interval: {CHECKPOINT_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing checkpoint if available\n",
    "generated_data = load_checkpoint(CHECKPOINT_PATH)\n",
    "start_idx = len(generated_data)\n",
    "\n",
    "print(f\"Loaded {start_idx} existing samples\")\n",
    "print(f\"Remaining: {TARGET_SAMPLES - start_idx} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main generation loop\n",
    "failed_count = 0\n",
    "max_failures = 100  # Stop if too many consecutive failures\n",
    "\n",
    "print(f\"\\nStarting generation from index {start_idx}...\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pbar = tqdm(total=TARGET_SAMPLES, initial=start_idx, desc=\"Generating\")\n",
    "\n",
    "while len(generated_data) < TARGET_SAMPLES:\n",
    "    try:\n",
    "        pair = generator.generate_pair()\n",
    "        \n",
    "        if pair:\n",
    "            generated_data.append(pair)\n",
    "            pbar.update(1)\n",
    "            failed_count = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if len(generated_data) % CHECKPOINT_INTERVAL == 0:\n",
    "                save_checkpoint(generated_data, CHECKPOINT_PATH)\n",
    "                \n",
    "                # Clear GPU cache periodically\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            if failed_count >= max_failures:\n",
    "                print(f\"\\nToo many failures ({max_failures}). Stopping.\")\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        failed_count += 1\n",
    "        if failed_count >= max_failures:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total generated: {len(generated_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final data\n",
    "save_checkpoint(generated_data, FINAL_PATH)\n",
    "print(f\"\\nFinal data saved to: {FINAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "import numpy as np\n",
    "\n",
    "instruction_lengths = [len(d['instruction'].split()) for d in generated_data]\n",
    "response_lengths = [len(d['response'].split()) for d in generated_data]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(generated_data)}\")\n",
    "print(f\"\\nInstruction length (words):\")\n",
    "print(f\"  Mean: {np.mean(instruction_lengths):.1f}\")\n",
    "print(f\"  Min: {np.min(instruction_lengths)}\")\n",
    "print(f\"  Max: {np.max(instruction_lengths)}\")\n",
    "print(f\"  Median: {np.median(instruction_lengths):.1f}\")\n",
    "print(f\"\\nResponse length (words):\")\n",
    "print(f\"  Mean: {np.mean(response_lengths):.1f}\")\n",
    "print(f\"  Min: {np.min(response_lengths)}\")\n",
    "print(f\"  Max: {np.max(response_lengths)}\")\n",
    "print(f\"  Median: {np.median(response_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, sample in enumerate(random.sample(generated_data, min(3, len(generated_data)))):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Instruction: {sample['instruction'][:200]}...\")\n",
    "    print(f\"Response: {sample['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del tokenizer\n",
    "del generator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Generation Complete!\n",
    "\n",
    "### Summary:\n",
    "- Generated instruction-response pairs saved to `data/raw/instructions_raw.json`\n",
    "- Checkpoint available at `data/raw/instructions_checkpoint.json`\n",
    "\n",
    "### Next Steps:\n",
    "1. Proceed to `03_quality_filtering.ipynb` for data filtering\n",
    "2. Target: Filter 15,000 → 10,000 high-quality samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

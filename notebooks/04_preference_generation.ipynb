{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04. Preference Data Generation\n## Synthetic Instruction Tuner - Week 2 Day 3-5\n\nThis notebook generates preference pairs for DPO training:\n1. Load filtered instruction data\n2. Load generator model (Llama-3.1-8B-Instruct) and reward model (OpenAssistant)\n3. Generate multiple response variants per instruction\n4. Score responses with reward model\n5. Create chosen/rejected pairs\n6. Save preference dataset\n\n**Target**: 600 preference pairs from ~1,000 filtered instructions (optimized for Colab free tier)\n\n**Expected runtime**: 4-6 hours for full dataset (use subsampling for faster testing)\n\n**Tip**: Run with checkpoints enabled, can split across multiple sessions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Project path\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install libraries with latest compatible versions\n!pip install -q --upgrade transformers>=4.41.0 peft>=0.7.0 accelerate>=0.25.0 bitsandbytes>=0.41.3 sentencepiece\n\nprint(\"✅ Libraries installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered instruction data\n",
    "FILTERED_PATH = f\"{config['paths']['data_filtered']}/instructions_filtered.json\"\n",
    "\n",
    "with open(FILTERED_PATH, 'r', encoding='utf-8') as f:\n",
    "    filtered_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(filtered_data)} filtered samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, you can subsample the data\n",
    "# Uncomment to use a smaller subset for faster testing\n",
    "\n",
    "# TEST_SIZE = 1000  # Use 1000 samples for testing\n",
    "# filtered_data = random.sample(filtered_data, min(TEST_SIZE, len(filtered_data)))\n",
    "# print(f\"Using {len(filtered_data)} samples for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Generator Model (Llama-3.1-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_ID = config['models']['data_generation']\n",
    "\n",
    "print(f\"Loading generator model: {GENERATOR_MODEL_ID}...\")\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_ID)\n",
    "generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
    "generator_tokenizer.padding_side = \"left\"\n",
    "\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GENERATOR_MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "generator_model.eval()\n",
    "\n",
    "print(f\"Generator model loaded!\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Reward Model (OpenAssistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_MODEL_ID = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "\n",
    "print(f\"Loading reward model: {REWARD_MODEL_ID}...\")\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_ID)\n",
    "\n",
    "# Load reward model (smaller, can use full precision or 8-bit)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "reward_model.eval()\n",
    "\n",
    "print(f\"Reward model loaded!\")\n",
    "print(f\"Total GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preference Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "@dataclass\n",
    "class PreferencePair:\n",
    "    \"\"\"A preference pair with chosen and rejected responses.\"\"\"\n",
    "    instruction: str\n",
    "    chosen: str\n",
    "    rejected: str\n",
    "    chosen_score: float\n",
    "    rejected_score: float\n",
    "    margin: float\n",
    "\n",
    "\n",
    "class PreferenceGenerator:\n",
    "    \"\"\"Generate preference pairs using reward model scoring.\"\"\"\n",
    "\n",
    "    def __init__(self, reward_model, reward_tokenizer, config: Optional[Dict] = None):\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "        self.config = config or {}\n",
    "\n",
    "        self.num_responses = self.config.get('num_responses_per_instruction', 3)\n",
    "        self.min_margin = self.config.get('min_score_margin', 0.5)\n",
    "        self.max_new_tokens = self.config.get('max_new_tokens', 512)\n",
    "\n",
    "        # Llama 3.1 templates\n",
    "        self.instruction_template = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        self.response_template = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    def score_response(self, instruction: str, response: str) -> float:\n",
    "        \"\"\"Score a response using the reward model.\"\"\"\n",
    "        # OpenAssistant reward model expects plain text format\n",
    "        text = f\"Question: {instruction}\\n\\nAnswer: {response}\"\n",
    "\n",
    "        inputs = self.reward_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(self.reward_model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.reward_model(**inputs)\n",
    "            score = outputs.logits[0][0].item()\n",
    "\n",
    "        return score\n",
    "\n",
    "    def generate_response_variant(self, gen_model, gen_tokenizer,\n",
    "                                 instruction: str, temperature: float) -> Optional[str]:\n",
    "        \"\"\"Generate a response variant with specified temperature.\"\"\"\n",
    "        prompt = f\"{self.instruction_template}{instruction}{self.response_template}\"\n",
    "\n",
    "        inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = gen_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=gen_tokenizer.eos_token_id,\n",
    "                    eos_token_id=[\n",
    "                        gen_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "                        gen_tokenizer.eos_token_id\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            generated = gen_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "            response = self._parse_response(generated)\n",
    "            return response if response and len(response) > 10 else None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_response(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract response from generated text.\"\"\"\n",
    "        try:\n",
    "            if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
    "                parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "                if len(parts) > 1:\n",
    "                    response = parts[-1]\n",
    "                    for end_token in [\"<|eot_id|>\", \"<|end_of_text|>\"]:\n",
    "                        if end_token in response:\n",
    "                            response = response.split(end_token)[0]\n",
    "                    return response.strip()\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def create_preference_pair(self, gen_model, gen_tokenizer,\n",
    "                              instruction: str, original_response: Optional[str] = None) -> Optional[PreferencePair]:\n",
    "        \"\"\"Create a preference pair by generating and scoring multiple responses.\"\"\"\n",
    "        responses = []\n",
    "\n",
    "        # Include original if provided\n",
    "        if original_response:\n",
    "            responses.append(original_response)\n",
    "\n",
    "        # Generate variants with different temperatures\n",
    "        temperatures = [0.6, 0.8, 1.0, 1.2][:self.num_responses]\n",
    "        for temp in temperatures:\n",
    "            if len(responses) >= self.num_responses:\n",
    "                break\n",
    "            response = self.generate_response_variant(gen_model, gen_tokenizer, instruction, temp)\n",
    "            if response and response not in responses:\n",
    "                responses.append(response)\n",
    "\n",
    "        if len(responses) < 2:\n",
    "            return None\n",
    "\n",
    "        # Score all responses\n",
    "        scored = []\n",
    "        for resp in responses:\n",
    "            try:\n",
    "                score = self.score_response(instruction, resp)\n",
    "                scored.append((resp, score))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if len(scored) < 2:\n",
    "            return None\n",
    "\n",
    "        # Sort by score\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        chosen, chosen_score = scored[0]\n",
    "        rejected, rejected_score = scored[-1]\n",
    "        margin = chosen_score - rejected_score\n",
    "\n",
    "        if margin < self.min_margin:\n",
    "            return None\n",
    "\n",
    "        return PreferencePair(\n",
    "            instruction=instruction,\n",
    "            chosen=chosen,\n",
    "            rejected=rejected,\n",
    "            chosen_score=chosen_score,\n",
    "            rejected_score=rejected_score,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "# Initialize generator\n",
    "pref_config = config.get('preference_generation', {})\n",
    "pref_generator = PreferenceGenerator(reward_model, reward_tokenizer, pref_config)\n",
    "\n",
    "print(\"PreferenceGenerator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Preference Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single sample\n",
    "print(\"Testing preference pair generation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_sample = filtered_data[0]\n",
    "test_instruction = test_sample['instruction']\n",
    "test_response = test_sample.get('response')\n",
    "\n",
    "print(f\"Instruction: {test_instruction}\\n\")\n",
    "\n",
    "pair = pref_generator.create_preference_pair(\n",
    "    generator_model,\n",
    "    generator_tokenizer,\n",
    "    test_instruction,\n",
    "    test_response\n",
    ")\n",
    "\n",
    "if pair:\n",
    "    print(f\"✓ Successfully created preference pair!\\n\")\n",
    "    print(f\"Chosen (score: {pair.chosen_score:.3f}):\")\n",
    "    print(f\"{pair.chosen[:200]}...\\n\")\n",
    "    print(f\"Rejected (score: {pair.rejected_score:.3f}):\")\n",
    "    print(f\"{pair.rejected[:200]}...\\n\")\n",
    "    print(f\"Margin: {pair.margin:.3f}\")\n",
    "else:\n",
    "    print(\"✗ Failed to create preference pair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few samples\n",
    "print(\"\\nTesting on 5 samples...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "success_count = 0\n",
    "for i in range(5):\n",
    "    sample = filtered_data[i]\n",
    "    pair = pref_generator.create_preference_pair(\n",
    "        generator_model,\n",
    "        generator_tokenizer,\n",
    "        sample['instruction'],\n",
    "        sample.get('response')\n",
    "    )\n",
    "    if pair:\n",
    "        success_count += 1\n",
    "        print(f\"[{i+1}] ✓ Margin: {pair.margin:.3f}\")\n",
    "    else:\n",
    "        print(f\"[{i+1}] ✗ Failed\")\n",
    "\n",
    "print(f\"\\nSuccess rate: {success_count}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Preference Dataset with Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, checkpoint_path):\n",
    "    \"\"\"Save data to checkpoint file.\"\"\"\n",
    "    with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Checkpoint saved: {len(data)} pairs\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load data from checkpoint file.\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "# Paths\n",
    "PREFERENCE_PATH = config['paths']['data_preference']\n",
    "CHECKPOINT_PATH = f\"{PREFERENCE_PATH}/preference_checkpoint.json\"\n",
    "FINAL_PATH = f\"{PREFERENCE_PATH}/preference_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Settings\nTARGET_PAIRS = config['preference_generation'].get('target_pairs', 600)\nCHECKPOINT_INTERVAL = config['preference_generation'].get('checkpoint_interval', 100)\n\nprint(f\"Target preference pairs: {TARGET_PAIRS}\")\nprint(f\"Checkpoint interval: {CHECKPOINT_INTERVAL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing checkpoint if available\n",
    "preference_data = load_checkpoint(CHECKPOINT_PATH)\n",
    "processed_instructions = {p['instruction'] for p in preference_data}\n",
    "\n",
    "print(f\"Loaded {len(preference_data)} existing pairs\")\n",
    "print(f\"Remaining: {TARGET_PAIRS - len(preference_data)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main generation loop\n",
    "failed_count = 0\n",
    "max_failures = 100\n",
    "\n",
    "print(f\"\\nStarting preference generation...\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pbar = tqdm(total=TARGET_PAIRS, initial=len(preference_data), desc=\"Generating pairs\")\n",
    "\n",
    "data_idx = 0\n",
    "while len(preference_data) < TARGET_PAIRS and data_idx < len(filtered_data):\n",
    "    sample = filtered_data[data_idx]\n",
    "    data_idx += 1\n",
    "\n",
    "    # Skip if already processed\n",
    "    if sample['instruction'] in processed_instructions:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        pair = pref_generator.create_preference_pair(\n",
    "            generator_model,\n",
    "            generator_tokenizer,\n",
    "            sample['instruction'],\n",
    "            sample.get('response')\n",
    "        )\n",
    "\n",
    "        if pair:\n",
    "            preference_data.append({\n",
    "                'instruction': pair.instruction,\n",
    "                'chosen': pair.chosen,\n",
    "                'rejected': pair.rejected,\n",
    "                'chosen_score': pair.chosen_score,\n",
    "                'rejected_score': pair.rejected_score,\n",
    "                'margin': pair.margin\n",
    "            })\n",
    "            processed_instructions.add(pair.instruction)\n",
    "            pbar.update(1)\n",
    "            failed_count = 0\n",
    "\n",
    "            # Save checkpoint\n",
    "            if len(preference_data) % CHECKPOINT_INTERVAL == 0:\n",
    "                save_checkpoint(preference_data, CHECKPOINT_PATH)\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "    if failed_count >= max_failures:\n",
    "        print(f\"\\nToo many failures. Stopping.\")\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total generated: {len(preference_data)} preference pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final data\n",
    "save_checkpoint(preference_data, FINAL_PATH)\n",
    "print(f\"\\nFinal preference data saved to: {FINAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate statistics\n",
    "margins = [p['margin'] for p in preference_data]\n",
    "chosen_scores = [p['chosen_score'] for p in preference_data]\n",
    "rejected_scores = [p['rejected_score'] for p in preference_data]\n",
    "\n",
    "chosen_lengths = [len(p['chosen'].split()) for p in preference_data]\n",
    "rejected_lengths = [len(p['rejected'].split()) for p in preference_data]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PREFERENCE DATA STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal pairs: {len(preference_data)}\")\n",
    "\n",
    "print(f\"\\nScore Margin:\")\n",
    "print(f\"  Mean: {np.mean(margins):.3f}\")\n",
    "print(f\"  Std: {np.std(margins):.3f}\")\n",
    "print(f\"  Min: {np.min(margins):.3f}\")\n",
    "print(f\"  Max: {np.max(margins):.3f}\")\n",
    "print(f\"  Median: {np.median(margins):.3f}\")\n",
    "\n",
    "print(f\"\\nChosen Response Scores:\")\n",
    "print(f\"  Mean: {np.mean(chosen_scores):.3f}\")\n",
    "print(f\"  Median: {np.median(chosen_scores):.3f}\")\n",
    "\n",
    "print(f\"\\nRejected Response Scores:\")\n",
    "print(f\"  Mean: {np.mean(rejected_scores):.3f}\")\n",
    "print(f\"  Median: {np.median(rejected_scores):.3f}\")\n",
    "\n",
    "print(f\"\\nChosen Response Length (words):\")\n",
    "print(f\"  Mean: {np.mean(chosen_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(chosen_lengths):.1f}\")\n",
    "\n",
    "print(f\"\\nRejected Response Length (words):\")\n",
    "print(f\"  Mean: {np.mean(rejected_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(rejected_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Margin distribution\n",
    "axes[0, 0].hist(margins, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Score Margin')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Score Margin Distribution')\n",
    "axes[0, 0].axvline(x=pref_generator.min_margin, color='r', linestyle='--', label=f'Min threshold ({pref_generator.min_margin})')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Chosen vs Rejected scores\n",
    "axes[0, 1].scatter(chosen_scores, rejected_scores, alpha=0.3, s=10)\n",
    "axes[0, 1].plot([min(chosen_scores), max(chosen_scores)], [min(chosen_scores), max(chosen_scores)], 'r--', label='y=x')\n",
    "axes[0, 1].set_xlabel('Chosen Score')\n",
    "axes[0, 1].set_ylabel('Rejected Score')\n",
    "axes[0, 1].set_title('Chosen vs Rejected Scores')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Score distributions\n",
    "axes[1, 0].hist(chosen_scores, bins=50, alpha=0.5, label='Chosen', edgecolor='black')\n",
    "axes[1, 0].hist(rejected_scores, bins=50, alpha=0.5, label='Rejected', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Score')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Score Distributions')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Length comparison\n",
    "axes[1, 1].scatter(chosen_lengths, rejected_lengths, alpha=0.3, s=10)\n",
    "axes[1, 1].plot([0, max(chosen_lengths)], [0, max(chosen_lengths)], 'r--', label='y=x')\n",
    "axes[1, 1].set_xlabel('Chosen Length (words)')\n",
    "axes[1, 1].set_ylabel('Rejected Length (words)')\n",
    "axes[1, 1].set_title('Response Lengths')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['paths']['evaluation_figures']}/preference_stats.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to {config['paths']['evaluation_figures']}/preference_stats.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample preference pairs\n",
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE PREFERENCE PAIRS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show top margin pairs\n",
    "top_pairs = sorted(preference_data, key=lambda x: x['margin'], reverse=True)[:3]\n",
    "\n",
    "for i, pair in enumerate(top_pairs):\n",
    "    print(f\"\\n--- Sample {i+1} (margin: {pair['margin']:.3f}) ---\")\n",
    "    print(f\"\\nInstruction: {pair['instruction'][:150]}...\")\n",
    "    print(f\"\\nChosen (score: {pair['chosen_score']:.3f}):\")\n",
    "    print(f\"{pair['chosen'][:300]}...\")\n",
    "    print(f\"\\nRejected (score: {pair['rejected_score']:.3f}):\")\n",
    "    print(f\"{pair['rejected'][:300]}...\")\n",
    "    print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare DPO Training Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DPO training format\n",
    "dpo_data = []\n",
    "for pair in preference_data:\n",
    "    dpo_data.append({\n",
    "        \"prompt\": pair['instruction'],\n",
    "        \"chosen\": pair['chosen'],\n",
    "        \"rejected\": pair['rejected']\n",
    "    })\n",
    "\n",
    "# Save DPO format\n",
    "DPO_DATA_PATH = f\"{config['paths']['data_preference']}/dpo_data.json\"\n",
    "with open(DPO_DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dpo_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"DPO training data saved to: {DPO_DATA_PATH}\")\n",
    "print(f\"Total pairs: {len(dpo_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(dpo_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training pairs: {len(train_data)}\")\n",
    "print(f\"Validation pairs: {len(val_data)}\")\n",
    "\n",
    "# Save splits\n",
    "with open(f\"{config['paths']['data_preference']}/dpo_train.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f\"{config['paths']['data_preference']}/dpo_val.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nTrain/val splits saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del generator_model\n",
    "del generator_tokenizer\n",
    "del reward_model\n",
    "del reward_tokenizer\n",
    "del pref_generator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✓ Preference Generation Complete!\n",
    "\n",
    "### Summary:\n",
    "- Generated preference pairs saved to `data/preference/preference_data.json`\n",
    "- DPO training data saved to `data/preference/dpo_data.json`\n",
    "- Train/val splits saved for DPO training\n",
    "\n",
    "### Next Steps:\n",
    "1. Week 3: Proceed to `05_sft_training.ipynb` for supervised fine-tuning\n",
    "2. After SFT: Use `06_dpo_training.ipynb` for DPO alignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
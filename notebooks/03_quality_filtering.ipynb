{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Quality Filtering\n",
        "## Synthetic Instruction Tuner - Week 2 Day 1-2\n",
        "\n",
        "This notebook filters the raw instruction-response pairs for quality:\n",
        "1. Load raw generated data\n",
        "2. Apply rule-based quality filters\n",
        "3. Analyze filtering statistics\n",
        "4. Save high-quality filtered data\n",
        "\n",
        "**Target**: Filter 15,000 raw samples → 10,000 high-quality samples\n",
        "\n",
        "**Expected runtime**: 15-30 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Project path\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-instruction-tuner\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "import json\n",
        "\n",
        "with open(f\"{PROJECT_ROOT}/config.json\", 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(\"Configuration loaded!\")\n",
        "print(f\"Target filtered samples: {config['data_generation']['target_filtered_samples']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw instruction data\n",
        "RAW_DATA_PATH = f\"{config['paths']['data_raw']}/instructions_raw.json\"\n",
        "\n",
        "with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(raw_data)} raw samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview some samples\n",
        "print(\"Sample data:\")\n",
        "print(\"=\" * 50)\n",
        "for i, sample in enumerate(random.sample(raw_data, min(3, len(raw_data)))):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"Instruction: {sample['instruction'][:150]}...\")\n",
        "    print(f\"Response: {sample['response'][:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quality Filter Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FilterResult:\n",
        "    \"\"\"Result of filtering a single sample.\"\"\"\n",
        "    passed: bool\n",
        "    score: float\n",
        "    reasons: List[str]\n",
        "\n",
        "\n",
        "class QualityFilter:\n",
        "    \"\"\"Rule-based quality filter for instruction-response pairs.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[Dict] = None):\n",
        "        self.config = config or {}\n",
        "\n",
        "        # Length thresholds\n",
        "        self.min_instruction_words = self.config.get('min_instruction_words', 3)\n",
        "        self.max_instruction_words = self.config.get('max_instruction_words', 500)\n",
        "        self.min_response_words = self.config.get('min_response_words', 10)\n",
        "        self.max_response_words = self.config.get('max_response_words', 2000)\n",
        "\n",
        "        # Quality thresholds\n",
        "        self.min_quality_score = self.config.get('min_quality_score', 0.5)\n",
        "        self.max_repetition_ratio = self.config.get('max_repetition_ratio', 0.3)\n",
        "\n",
        "        # Toxic keywords (basic list)\n",
        "        self.toxic_keywords = self._load_toxic_keywords()\n",
        "\n",
        "        # Low quality patterns\n",
        "        self.low_quality_patterns = [\n",
        "            r'^(hi|hello|hey|ok|okay|sure|yes|no|thanks|thank you)[\\s\\.\\!]*$',\n",
        "            r'^I (don\\'t|cannot|can\\'t) (help|assist|answer)',\n",
        "            r'as an AI',\n",
        "            r'I\\'m sorry, but I',\n",
        "            r'I apologize, but',\n",
        "            r'^\\s*$',\n",
        "            r'^[^\\w\\s]+$',\n",
        "        ]\n",
        "\n",
        "    def _load_toxic_keywords(self) -> List[str]:\n",
        "        return [\n",
        "            'kill', 'murder', 'suicide', 'terrorist', 'bomb', 'weapon',\n",
        "            'hack into', 'steal password', 'illegal drug', 'child abuse',\n",
        "        ]\n",
        "\n",
        "    def filter_sample(self, sample: Dict) -> FilterResult:\n",
        "        instruction = sample.get('instruction', '')\n",
        "        response = sample.get('response', '')\n",
        "\n",
        "        reasons = []\n",
        "        scores = []\n",
        "\n",
        "        # Apply all filters\n",
        "        filters = [\n",
        "            (self._check_length, 0.15),\n",
        "            (self._check_language, 0.10),\n",
        "            (self._check_repetition, 0.20),\n",
        "            (self._check_format, 0.15),\n",
        "            (self._check_toxicity, 0.15),\n",
        "            (self._check_content_quality, 0.25),\n",
        "        ]\n",
        "\n",
        "        weighted_score = 0\n",
        "        for filter_func, weight in filters:\n",
        "            if filter_func == self._check_repetition:\n",
        "                passed, score, reason = filter_func(response)\n",
        "            else:\n",
        "                passed, score, reason = filter_func(instruction, response)\n",
        "            \n",
        "            if not passed:\n",
        "                reasons.append(reason)\n",
        "            weighted_score += score * weight\n",
        "\n",
        "        passed = len(reasons) == 0 and weighted_score >= self.min_quality_score\n",
        "        return FilterResult(passed=passed, score=weighted_score, reasons=reasons)\n",
        "\n",
        "    def _check_length(self, instruction: str, response: str) -> Tuple[bool, float, str]:\n",
        "        inst_words = len(instruction.split())\n",
        "        resp_words = len(response.split())\n",
        "\n",
        "        if inst_words < self.min_instruction_words:\n",
        "            return False, 0.0, f\"Instruction too short ({inst_words} words)\"\n",
        "        if inst_words > self.max_instruction_words:\n",
        "            return False, 0.0, f\"Instruction too long ({inst_words} words)\"\n",
        "        if resp_words < self.min_response_words:\n",
        "            return False, 0.0, f\"Response too short ({resp_words} words)\"\n",
        "        if resp_words > self.max_response_words:\n",
        "            return False, 0.5, f\"Response too long ({resp_words} words)\"\n",
        "\n",
        "        ideal_length = 200\n",
        "        length_diff = abs(resp_words - ideal_length)\n",
        "        score = max(0.5, 1.0 - (length_diff / 500))\n",
        "        return True, score, \"\"\n",
        "\n",
        "    def _check_language(self, instruction: str, response: str) -> Tuple[bool, float, str]:\n",
        "        combined = instruction + \" \" + response\n",
        "        english_indicators = ['the', 'is', 'are', 'was', 'were', 'have', 'has',\n",
        "                            'will', 'would', 'could', 'should', 'can', 'may',\n",
        "                            'a', 'an', 'and', 'or', 'but', 'if', 'then']\n",
        "\n",
        "        words = combined.lower().split()\n",
        "        if len(words) == 0:\n",
        "            return False, 0.0, \"Empty content\"\n",
        "\n",
        "        english_word_count = sum(1 for w in words if w in english_indicators)\n",
        "        english_ratio = english_word_count / len(words)\n",
        "\n",
        "        ascii_chars = sum(1 for c in combined if ord(c) < 128)\n",
        "        ascii_ratio = ascii_chars / max(len(combined), 1)\n",
        "\n",
        "        score = (english_ratio * 0.5 + ascii_ratio * 0.5)\n",
        "\n",
        "        if ascii_ratio < 0.8:\n",
        "            return False, score, \"Non-English content detected\"\n",
        "        return True, min(1.0, score + 0.5), \"\"\n",
        "\n",
        "    def _check_repetition(self, response: str) -> Tuple[bool, float, str]:\n",
        "        words = response.lower().split()\n",
        "        if len(words) < 10:\n",
        "            return True, 1.0, \"\"\n",
        "\n",
        "        word_counts = {}\n",
        "        for word in words:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'to', 'of', 'and', 'in', 'for', 'on', 'with'}\n",
        "        significant_words = {w: c for w, c in word_counts.items() if w not in common_words and len(w) > 2}\n",
        "\n",
        "        if significant_words:\n",
        "            max_repeat = max(significant_words.values())\n",
        "            repetition_ratio = max_repeat / len(words)\n",
        "            if repetition_ratio > self.max_repetition_ratio:\n",
        "                return False, 0.3, f\"High word repetition ({repetition_ratio:.2%})\"\n",
        "\n",
        "        for n in [3, 4, 5]:\n",
        "            if len(words) >= n * 3:\n",
        "                ngrams = [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "                ngram_counts = {}\n",
        "                for ng in ngrams:\n",
        "                    ngram_counts[ng] = ngram_counts.get(ng, 0) + 1\n",
        "                max_ngram_repeat = max(ngram_counts.values())\n",
        "                if max_ngram_repeat > 3:\n",
        "                    return False, 0.2, \"Repeated phrases detected\"\n",
        "\n",
        "        return True, 1.0, \"\"\n",
        "\n",
        "    def _check_format(self, instruction: str, response: str) -> Tuple[bool, float, str]:\n",
        "        for pattern in self.low_quality_patterns:\n",
        "            if re.search(pattern, instruction, re.IGNORECASE):\n",
        "                return False, 0.0, \"Low quality instruction pattern\"\n",
        "\n",
        "        for pattern in self.low_quality_patterns:\n",
        "            if re.search(pattern, response, re.IGNORECASE):\n",
        "                return False, 0.1, \"Low quality response pattern\"\n",
        "\n",
        "        refusal_patterns = [\n",
        "            r\"I (can't|cannot|won't|will not) (help|assist|provide)\",\n",
        "            r\"I'm (not able|unable) to\",\n",
        "            r\"I don't have (access|the ability)\",\n",
        "        ]\n",
        "        for pattern in refusal_patterns:\n",
        "            if re.search(pattern, response, re.IGNORECASE):\n",
        "                return False, 0.0, \"Response is a refusal\"\n",
        "\n",
        "        response_stripped = response.strip()\n",
        "        if response_stripped and response_stripped[-1] not in '.!?\"\\')': \n",
        "            return True, 0.7, \"\"\n",
        "        return True, 1.0, \"\"\n",
        "\n",
        "    def _check_toxicity(self, instruction: str, response: str) -> Tuple[bool, float, str]:\n",
        "        combined = (instruction + \" \" + response).lower()\n",
        "        for keyword in self.toxic_keywords:\n",
        "            if keyword.lower() in combined:\n",
        "                educational_context = ['history', 'prevention', 'awareness', 'education', 'study']\n",
        "                if any(ctx in combined for ctx in educational_context):\n",
        "                    continue\n",
        "                return False, 0.0, \"Potentially harmful content detected\"\n",
        "        return True, 1.0, \"\"\n",
        "\n",
        "    def _check_content_quality(self, instruction: str, response: str) -> Tuple[bool, float, str]:\n",
        "        score = 1.0\n",
        "        if '?' in instruction:\n",
        "            score += 0.1\n",
        "        if any(word in instruction.lower() for word in ['explain', 'describe', 'how', 'what', 'why', 'write', 'create']):\n",
        "            score += 0.1\n",
        "\n",
        "        response_words = response.split()\n",
        "        unique_words = len(set(response_words))\n",
        "        if len(response_words) > 0:\n",
        "            diversity = unique_words / len(response_words)\n",
        "            score *= (0.5 + diversity * 0.5)\n",
        "\n",
        "        if '\\n\\n' in response or re.search(r'^\\d+\\.|\\-|\\*', response, re.MULTILINE):\n",
        "            score += 0.1\n",
        "\n",
        "        score = min(1.0, max(0.0, score))\n",
        "        if score < 0.4:\n",
        "            return False, score, \"Low content quality\"\n",
        "        return True, score, \"\"\n",
        "\n",
        "    def filter_batch(self, samples: List[Dict], verbose: bool = False) -> Tuple[List[Dict], Dict]:\n",
        "        filtered = []\n",
        "        stats = {\n",
        "            'total': len(samples),\n",
        "            'passed': 0,\n",
        "            'failed': 0,\n",
        "            'reasons': {},\n",
        "            'score_distribution': []\n",
        "        }\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            result = self.filter_sample(sample)\n",
        "            stats['score_distribution'].append(result.score)\n",
        "\n",
        "            if result.passed:\n",
        "                filtered.append({**sample, 'quality_score': result.score})\n",
        "                stats['passed'] += 1\n",
        "            else:\n",
        "                stats['failed'] += 1\n",
        "                for reason in result.reasons:\n",
        "                    key = reason.split('(')[0].strip()\n",
        "                    stats['reasons'][key] = stats['reasons'].get(key, 0) + 1\n",
        "\n",
        "            if verbose and (i + 1) % 1000 == 0:\n",
        "                print(f\"Processed {i + 1}/{len(samples)} samples\")\n",
        "\n",
        "        return filtered, stats\n",
        "\n",
        "print(\"QualityFilter class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Filter on Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize filter with config\n",
        "filter_config = config.get('quality_filter', {})\n",
        "quality_filter = QualityFilter(filter_config)\n",
        "\n",
        "print(\"Filter initialized with settings:\")\n",
        "print(f\"  Min instruction words: {quality_filter.min_instruction_words}\")\n",
        "print(f\"  Max instruction words: {quality_filter.max_instruction_words}\")\n",
        "print(f\"  Min response words: {quality_filter.min_response_words}\")\n",
        "print(f\"  Max response words: {quality_filter.max_response_words}\")\n",
        "print(f\"  Min quality score: {quality_filter.min_quality_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on individual samples\n",
        "print(\"Testing filter on individual samples:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, sample in enumerate(raw_data[:5]):\n",
        "    result = quality_filter.filter_sample(sample)\n",
        "    status = \"PASS\" if result.passed else \"FAIL\"\n",
        "    print(f\"\\n[{i+1}] {status} (score: {result.score:.3f})\")\n",
        "    print(f\"    Instruction: {sample['instruction'][:80]}...\")\n",
        "    if result.reasons:\n",
        "        print(f\"    Reasons: {', '.join(result.reasons)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Apply Filter to All Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"Filtering {len(raw_data)} samples...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "filtered_data, stats = quality_filter.filter_batch(raw_data, verbose=True)\n",
        "\n",
        "print(f\"\\nFiltering complete!\")\n",
        "print(f\"Passed: {stats['passed']} / {stats['total']} ({stats['passed']/stats['total']*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze Filtering Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"FILTERING STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nTotal samples: {stats['total']}\")\n",
        "print(f\"Passed: {stats['passed']} ({stats['passed']/stats['total']*100:.1f}%)\")\n",
        "print(f\"Failed: {stats['failed']} ({stats['failed']/stats['total']*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nQuality Score Distribution:\")\n",
        "scores = stats['score_distribution']\n",
        "print(f\"  Mean: {np.mean(scores):.3f}\")\n",
        "print(f\"  Std: {np.std(scores):.3f}\")\n",
        "print(f\"  Min: {np.min(scores):.3f}\")\n",
        "print(f\"  Max: {np.max(scores):.3f}\")\n",
        "print(f\"  Median: {np.median(scores):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Failure reasons breakdown\n",
        "print(\"\\nFailure Reasons Breakdown:\")\n",
        "print(\"-\" * 40)\n",
        "for reason, count in sorted(stats['reasons'].items(), key=lambda x: -x[1]):\n",
        "    pct = count / stats['failed'] * 100 if stats['failed'] > 0 else 0\n",
        "    print(f\"  {reason}: {count} ({pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize score distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Score histogram\n",
        "axes[0].hist(scores, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(x=quality_filter.min_quality_score, color='r', linestyle='--', label=f'Threshold ({quality_filter.min_quality_score})')\n",
        "axes[0].set_xlabel('Quality Score')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Quality Score Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Pass/Fail pie chart\n",
        "labels = ['Passed', 'Failed']\n",
        "sizes = [stats['passed'], stats['failed']]\n",
        "colors = ['#4CAF50', '#F44336']\n",
        "axes[1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "axes[1].set_title('Filter Results')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{config['paths']['evaluation_figures']}/filtering_stats.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigure saved to {config['paths']['evaluation_figures']}/filtering_stats.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Adjust Filter if Needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have enough samples\n",
        "TARGET_FILTERED = config['data_generation']['target_filtered_samples']\n",
        "\n",
        "print(f\"Target filtered samples: {TARGET_FILTERED}\")\n",
        "print(f\"Current filtered samples: {len(filtered_data)}\")\n",
        "\n",
        "if len(filtered_data) >= TARGET_FILTERED:\n",
        "    print(f\"\\n✓ Sufficient samples! We have {len(filtered_data) - TARGET_FILTERED} extra.\")\n",
        "    \n",
        "    # If too many, select top quality\n",
        "    if len(filtered_data) > TARGET_FILTERED * 1.2:  # More than 20% extra\n",
        "        print(f\"Selecting top {TARGET_FILTERED} by quality score...\")\n",
        "        filtered_data = sorted(filtered_data, key=lambda x: x['quality_score'], reverse=True)[:TARGET_FILTERED]\n",
        "        print(f\"Selected {len(filtered_data)} samples\")\n",
        "else:\n",
        "    deficit = TARGET_FILTERED - len(filtered_data)\n",
        "    print(f\"\\n⚠ Need {deficit} more samples.\")\n",
        "    print(\"Options:\")\n",
        "    print(\"  1. Generate more raw data (run 02_magpie_generation.ipynb again)\")\n",
        "    print(\"  2. Relax filter thresholds (reduce min_quality_score)\")\n",
        "    print(\"  3. Proceed with fewer samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Relax filters to get more samples\n",
        "# Uncomment if needed\n",
        "\n",
        "# if len(filtered_data) < TARGET_FILTERED:\n",
        "#     print(\"Relaxing filter thresholds...\")\n",
        "#     relaxed_config = {\n",
        "#         'min_quality_score': 0.4,  # Lower threshold\n",
        "#         'min_response_words': 8,\n",
        "#     }\n",
        "#     relaxed_filter = QualityFilter(relaxed_config)\n",
        "#     filtered_data, stats = relaxed_filter.filter_batch(raw_data, verbose=True)\n",
        "#     print(f\"After relaxing: {len(filtered_data)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analyze Filtered Data Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistics on filtered data\n",
        "instruction_lengths = [len(d['instruction'].split()) for d in filtered_data]\n",
        "response_lengths = [len(d['response'].split()) for d in filtered_data]\n",
        "quality_scores = [d['quality_score'] for d in filtered_data]\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"FILTERED DATA STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nTotal filtered samples: {len(filtered_data)}\")\n",
        "\n",
        "print(f\"\\nInstruction length (words):\")\n",
        "print(f\"  Mean: {np.mean(instruction_lengths):.1f}\")\n",
        "print(f\"  Median: {np.median(instruction_lengths):.1f}\")\n",
        "print(f\"  Range: {np.min(instruction_lengths)} - {np.max(instruction_lengths)}\")\n",
        "\n",
        "print(f\"\\nResponse length (words):\")\n",
        "print(f\"  Mean: {np.mean(response_lengths):.1f}\")\n",
        "print(f\"  Median: {np.median(response_lengths):.1f}\")\n",
        "print(f\"  Range: {np.min(response_lengths)} - {np.max(response_lengths)}\")\n",
        "\n",
        "print(f\"\\nQuality scores:\")\n",
        "print(f\"  Mean: {np.mean(quality_scores):.3f}\")\n",
        "print(f\"  Median: {np.median(quality_scores):.3f}\")\n",
        "print(f\"  Range: {np.min(quality_scores):.3f} - {np.max(quality_scores):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample filtered data\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"SAMPLE FILTERED DATA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Show top quality samples\n",
        "top_samples = sorted(filtered_data, key=lambda x: x['quality_score'], reverse=True)[:3]\n",
        "print(\"\\nTop quality samples:\")\n",
        "for i, sample in enumerate(top_samples):\n",
        "    print(f\"\\n--- Top {i+1} (score: {sample['quality_score']:.3f}) ---\")\n",
        "    print(f\"Instruction: {sample['instruction'][:150]}...\")\n",
        "    print(f\"Response: {sample['response'][:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save filtered data\n",
        "FILTERED_PATH = f\"{config['paths']['data_filtered']}/instructions_filtered.json\"\n",
        "\n",
        "with open(FILTERED_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Filtered data saved to: {FILTERED_PATH}\")\n",
        "print(f\"Total samples: {len(filtered_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save filtering statistics\n",
        "STATS_PATH = f\"{config['paths']['evaluation_results']}/filtering_stats.json\"\n",
        "\n",
        "# Convert numpy types for JSON serialization\n",
        "stats_json = {\n",
        "    'total': stats['total'],\n",
        "    'passed': stats['passed'],\n",
        "    'failed': stats['failed'],\n",
        "    'pass_rate': stats['passed'] / stats['total'],\n",
        "    'reasons': stats['reasons'],\n",
        "    'score_stats': {\n",
        "        'mean': float(np.mean(scores)),\n",
        "        'std': float(np.std(scores)),\n",
        "        'min': float(np.min(scores)),\n",
        "        'max': float(np.max(scores)),\n",
        "        'median': float(np.median(scores)),\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(STATS_PATH, 'w') as f:\n",
        "    json.dump(stats_json, f, indent=2)\n",
        "\n",
        "print(f\"Statistics saved to: {STATS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Prepare for SFT Training Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to SFT training format (instruction-response pairs)\n",
        "sft_data = []\n",
        "for sample in filtered_data:\n",
        "    sft_data.append({\n",
        "        \"instruction\": sample['instruction'],\n",
        "        \"input\": \"\",  # No additional input\n",
        "        \"output\": sample['response']\n",
        "    })\n",
        "\n",
        "# Save SFT format\n",
        "SFT_DATA_PATH = f\"{config['paths']['data_filtered']}/sft_data.json\"\n",
        "with open(SFT_DATA_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(sft_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"SFT training data saved to: {SFT_DATA_PATH}\")\n",
        "print(f\"Total samples: {len(sft_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, val_data = train_test_split(sft_data, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Save splits\n",
        "with open(f\"{config['paths']['data_filtered']}/sft_train.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(f\"{config['paths']['data_filtered']}/sft_val.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nTrain/val splits saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✓ Filtering Complete!\n",
        "\n",
        "### Summary:\n",
        "- Filtered data saved to `data/filtered/instructions_filtered.json`\n",
        "- SFT training data saved to `data/filtered/sft_data.json`\n",
        "- Train/val splits saved for training\n",
        "\n",
        "### Next Steps:\n",
        "1. Proceed to `04_preference_generation.ipynb` for DPO preference data\n",
        "2. This will use the filtered data to create chosen/rejected pairs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

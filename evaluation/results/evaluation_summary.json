{
  "evaluation_date": "2025-12-26 18:38:58",
  "models_evaluated": {
    "base": "meta-llama/Llama-3.2-3B",
    "sft": "/content/drive/MyDrive/synthetic-instruction-tuner/models/sft/final",
    "dpo": "/content/drive/MyDrive/synthetic-instruction-tuner/models/dpo/final"
  },
  "tests_performed": {
    "instruction_following": 5,
    "knowledge": 5
  },
  "response_metrics": {
    "base": {
      "avg_length": 115.8,
      "avg_sentences": 9.4,
      "unique_words": 44.6
    },
    "sft": {
      "avg_length": 137.4,
      "avg_sentences": 7.0,
      "unique_words": 71.8
    },
    "dpo": {
      "avg_length": 139.6,
      "avg_sentences": 6.0,
      "unique_words": 74.4
    }
  },
  "observations": [
    "SFT model shows improved instruction following compared to base model",
    "DPO model demonstrates better response quality and coherence",
    "All fine-tuned models maintain factual knowledge while improving generation quality"
  ]
}